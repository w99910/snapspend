// ignore_for_file: always_specify_types
// ignore_for_file: camel_case_types
// ignore_for_file: constant_identifier_names
// ignore_for_file: non_constant_identifier_names
// ignore_for_file: public_member_api_docs
// ignore_for_file: unused_element
// ignore_for_file: unused_field

// AUTO GENERATED FILE, DO NOT EDIT.
//
// Generated by `package:ffigen`.
// ignore_for_file: type=lint, unused_import
import 'dart:ffi' as ffi;

/// FFI bindings for llama.cpp
class LlamaBindings {
  /// Holds the symbol lookup function.
  final ffi.Pointer<T> Function<T extends ffi.NativeType>(String symbolName)
  _lookup;

  /// The symbols are looked up in [dynamicLibrary].
  LlamaBindings(ffi.DynamicLibrary dynamicLibrary)
    : _lookup = dynamicLibrary.lookup;

  /// The symbols are looked up with [lookup].
  LlamaBindings.fromLookup(
    ffi.Pointer<T> Function<T extends ffi.NativeType>(String symbolName) lookup,
  ) : _lookup = lookup;

  /// Set the abort callback (passing null will restore original abort functionality: printing a message to stdout)
  /// Returns the old callback for chaining
  ggml_abort_callback_t ggml_set_abort_callback(
    ggml_abort_callback_t callback,
  ) {
    return _ggml_set_abort_callback(callback);
  }

  late final _ggml_set_abort_callbackPtr =
      _lookup<
        ffi.NativeFunction<
          ggml_abort_callback_t Function(ggml_abort_callback_t)
        >
      >('ggml_set_abort_callback');
  late final _ggml_set_abort_callback = _ggml_set_abort_callbackPtr
      .asFunction<ggml_abort_callback_t Function(ggml_abort_callback_t)>();

  void ggml_abort(
    ffi.Pointer<ffi.Char> file,
    int line,
    ffi.Pointer<ffi.Char> fmt,
  ) {
    return _ggml_abort(file, line, fmt);
  }

  late final _ggml_abortPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Void Function(
            ffi.Pointer<ffi.Char>,
            ffi.Int,
            ffi.Pointer<ffi.Char>,
          )
        >
      >('ggml_abort');
  late final _ggml_abort = _ggml_abortPtr
      .asFunction<
        void Function(ffi.Pointer<ffi.Char>, int, ffi.Pointer<ffi.Char>)
      >();

  /// get ggml_status name string
  ffi.Pointer<ffi.Char> ggml_status_to_string(ggml_status status) {
    return _ggml_status_to_string(status.value);
  }

  late final _ggml_status_to_stringPtr =
      _lookup<ffi.NativeFunction<ffi.Pointer<ffi.Char> Function(ffi.Int)>>(
        'ggml_status_to_string',
      );
  late final _ggml_status_to_string = _ggml_status_to_stringPtr
      .asFunction<ffi.Pointer<ffi.Char> Function(int)>();

  double ggml_fp16_to_fp32(int arg0) {
    return _ggml_fp16_to_fp32(arg0);
  }

  late final _ggml_fp16_to_fp32Ptr =
      _lookup<ffi.NativeFunction<ffi.Float Function(ggml_fp16_t)>>(
        'ggml_fp16_to_fp32',
      );
  late final _ggml_fp16_to_fp32 = _ggml_fp16_to_fp32Ptr
      .asFunction<double Function(int)>();

  int ggml_fp32_to_fp16(double arg0) {
    return _ggml_fp32_to_fp16(arg0);
  }

  late final _ggml_fp32_to_fp16Ptr =
      _lookup<ffi.NativeFunction<ggml_fp16_t Function(ffi.Float)>>(
        'ggml_fp32_to_fp16',
      );
  late final _ggml_fp32_to_fp16 = _ggml_fp32_to_fp16Ptr
      .asFunction<int Function(double)>();

  void ggml_fp16_to_fp32_row(
    ffi.Pointer<ggml_fp16_t> arg0,
    ffi.Pointer<ffi.Float> arg1,
    int arg2,
  ) {
    return _ggml_fp16_to_fp32_row(arg0, arg1, arg2);
  }

  late final _ggml_fp16_to_fp32_rowPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Void Function(
            ffi.Pointer<ggml_fp16_t>,
            ffi.Pointer<ffi.Float>,
            ffi.Int64,
          )
        >
      >('ggml_fp16_to_fp32_row');
  late final _ggml_fp16_to_fp32_row = _ggml_fp16_to_fp32_rowPtr
      .asFunction<
        void Function(ffi.Pointer<ggml_fp16_t>, ffi.Pointer<ffi.Float>, int)
      >();

  void ggml_fp32_to_fp16_row(
    ffi.Pointer<ffi.Float> arg0,
    ffi.Pointer<ggml_fp16_t> arg1,
    int arg2,
  ) {
    return _ggml_fp32_to_fp16_row(arg0, arg1, arg2);
  }

  late final _ggml_fp32_to_fp16_rowPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Void Function(
            ffi.Pointer<ffi.Float>,
            ffi.Pointer<ggml_fp16_t>,
            ffi.Int64,
          )
        >
      >('ggml_fp32_to_fp16_row');
  late final _ggml_fp32_to_fp16_row = _ggml_fp32_to_fp16_rowPtr
      .asFunction<
        void Function(ffi.Pointer<ffi.Float>, ffi.Pointer<ggml_fp16_t>, int)
      >();

  ggml_bf16_t ggml_fp32_to_bf16(double arg0) {
    return _ggml_fp32_to_bf16(arg0);
  }

  late final _ggml_fp32_to_bf16Ptr =
      _lookup<ffi.NativeFunction<ggml_bf16_t Function(ffi.Float)>>(
        'ggml_fp32_to_bf16',
      );
  late final _ggml_fp32_to_bf16 = _ggml_fp32_to_bf16Ptr
      .asFunction<ggml_bf16_t Function(double)>();

  double ggml_bf16_to_fp32(ggml_bf16_t arg0) {
    return _ggml_bf16_to_fp32(arg0);
  }

  late final _ggml_bf16_to_fp32Ptr =
      _lookup<ffi.NativeFunction<ffi.Float Function(ggml_bf16_t)>>(
        'ggml_bf16_to_fp32',
      );
  late final _ggml_bf16_to_fp32 = _ggml_bf16_to_fp32Ptr
      .asFunction<double Function(ggml_bf16_t)>();

  void ggml_bf16_to_fp32_row(
    ffi.Pointer<ggml_bf16_t> arg0,
    ffi.Pointer<ffi.Float> arg1,
    int arg2,
  ) {
    return _ggml_bf16_to_fp32_row(arg0, arg1, arg2);
  }

  late final _ggml_bf16_to_fp32_rowPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Void Function(
            ffi.Pointer<ggml_bf16_t>,
            ffi.Pointer<ffi.Float>,
            ffi.Int64,
          )
        >
      >('ggml_bf16_to_fp32_row');
  late final _ggml_bf16_to_fp32_row = _ggml_bf16_to_fp32_rowPtr
      .asFunction<
        void Function(ffi.Pointer<ggml_bf16_t>, ffi.Pointer<ffi.Float>, int)
      >();

  void ggml_fp32_to_bf16_row_ref(
    ffi.Pointer<ffi.Float> arg0,
    ffi.Pointer<ggml_bf16_t> arg1,
    int arg2,
  ) {
    return _ggml_fp32_to_bf16_row_ref(arg0, arg1, arg2);
  }

  late final _ggml_fp32_to_bf16_row_refPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Void Function(
            ffi.Pointer<ffi.Float>,
            ffi.Pointer<ggml_bf16_t>,
            ffi.Int64,
          )
        >
      >('ggml_fp32_to_bf16_row_ref');
  late final _ggml_fp32_to_bf16_row_ref = _ggml_fp32_to_bf16_row_refPtr
      .asFunction<
        void Function(ffi.Pointer<ffi.Float>, ffi.Pointer<ggml_bf16_t>, int)
      >();

  void ggml_fp32_to_bf16_row(
    ffi.Pointer<ffi.Float> arg0,
    ffi.Pointer<ggml_bf16_t> arg1,
    int arg2,
  ) {
    return _ggml_fp32_to_bf16_row(arg0, arg1, arg2);
  }

  late final _ggml_fp32_to_bf16_rowPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Void Function(
            ffi.Pointer<ffi.Float>,
            ffi.Pointer<ggml_bf16_t>,
            ffi.Int64,
          )
        >
      >('ggml_fp32_to_bf16_row');
  late final _ggml_fp32_to_bf16_row = _ggml_fp32_to_bf16_rowPtr
      .asFunction<
        void Function(ffi.Pointer<ffi.Float>, ffi.Pointer<ggml_bf16_t>, int)
      >();

  late final ffi.Pointer<ffi.Size> _GGML_TENSOR_SIZE = _lookup<ffi.Size>(
    'GGML_TENSOR_SIZE',
  );

  int get GGML_TENSOR_SIZE => _GGML_TENSOR_SIZE.value;

  bool ggml_guid_matches(ggml_guid_t guid_a, ggml_guid_t guid_b) {
    return _ggml_guid_matches(guid_a, guid_b);
  }

  late final _ggml_guid_matchesPtr =
      _lookup<ffi.NativeFunction<ffi.Bool Function(ggml_guid_t, ggml_guid_t)>>(
        'ggml_guid_matches',
      );
  late final _ggml_guid_matches = _ggml_guid_matchesPtr
      .asFunction<bool Function(ggml_guid_t, ggml_guid_t)>();

  /// misc
  ffi.Pointer<ffi.Char> ggml_version() {
    return _ggml_version();
  }

  late final _ggml_versionPtr =
      _lookup<ffi.NativeFunction<ffi.Pointer<ffi.Char> Function()>>(
        'ggml_version',
      );
  late final _ggml_version = _ggml_versionPtr
      .asFunction<ffi.Pointer<ffi.Char> Function()>();

  ffi.Pointer<ffi.Char> ggml_commit() {
    return _ggml_commit();
  }

  late final _ggml_commitPtr =
      _lookup<ffi.NativeFunction<ffi.Pointer<ffi.Char> Function()>>(
        'ggml_commit',
      );
  late final _ggml_commit = _ggml_commitPtr
      .asFunction<ffi.Pointer<ffi.Char> Function()>();

  void ggml_time_init() {
    return _ggml_time_init();
  }

  late final _ggml_time_initPtr =
      _lookup<ffi.NativeFunction<ffi.Void Function()>>('ggml_time_init');
  late final _ggml_time_init = _ggml_time_initPtr.asFunction<void Function()>();

  int ggml_time_ms() {
    return _ggml_time_ms();
  }

  late final _ggml_time_msPtr =
      _lookup<ffi.NativeFunction<ffi.Int64 Function()>>('ggml_time_ms');
  late final _ggml_time_ms = _ggml_time_msPtr.asFunction<int Function()>();

  int ggml_time_us() {
    return _ggml_time_us();
  }

  late final _ggml_time_usPtr =
      _lookup<ffi.NativeFunction<ffi.Int64 Function()>>('ggml_time_us');
  late final _ggml_time_us = _ggml_time_usPtr.asFunction<int Function()>();

  int ggml_cycles() {
    return _ggml_cycles();
  }

  late final _ggml_cyclesPtr =
      _lookup<ffi.NativeFunction<ffi.Int64 Function()>>('ggml_cycles');
  late final _ggml_cycles = _ggml_cyclesPtr.asFunction<int Function()>();

  int ggml_cycles_per_ms() {
    return _ggml_cycles_per_ms();
  }

  late final _ggml_cycles_per_msPtr =
      _lookup<ffi.NativeFunction<ffi.Int64 Function()>>('ggml_cycles_per_ms');
  late final _ggml_cycles_per_ms = _ggml_cycles_per_msPtr
      .asFunction<int Function()>();

  /// accepts a UTF-8 path, even on Windows
  ffi.Pointer<FILE> ggml_fopen(
    ffi.Pointer<ffi.Char> fname,
    ffi.Pointer<ffi.Char> mode,
  ) {
    return _ggml_fopen(fname, mode);
  }

  late final _ggml_fopenPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<FILE> Function(
            ffi.Pointer<ffi.Char>,
            ffi.Pointer<ffi.Char>,
          )
        >
      >('ggml_fopen');
  late final _ggml_fopen = _ggml_fopenPtr
      .asFunction<
        ffi.Pointer<FILE> Function(ffi.Pointer<ffi.Char>, ffi.Pointer<ffi.Char>)
      >();

  void ggml_print_object(ffi.Pointer<ggml_object> obj) {
    return _ggml_print_object(obj);
  }

  late final _ggml_print_objectPtr =
      _lookup<ffi.NativeFunction<ffi.Void Function(ffi.Pointer<ggml_object>)>>(
        'ggml_print_object',
      );
  late final _ggml_print_object = _ggml_print_objectPtr
      .asFunction<void Function(ffi.Pointer<ggml_object>)>();

  void ggml_print_objects(ffi.Pointer<ggml_context> ctx) {
    return _ggml_print_objects(ctx);
  }

  late final _ggml_print_objectsPtr =
      _lookup<ffi.NativeFunction<ffi.Void Function(ffi.Pointer<ggml_context>)>>(
        'ggml_print_objects',
      );
  late final _ggml_print_objects = _ggml_print_objectsPtr
      .asFunction<void Function(ffi.Pointer<ggml_context>)>();

  int ggml_nelements(ffi.Pointer<ggml_tensor> tensor) {
    return _ggml_nelements(tensor);
  }

  late final _ggml_nelementsPtr =
      _lookup<ffi.NativeFunction<ffi.Int64 Function(ffi.Pointer<ggml_tensor>)>>(
        'ggml_nelements',
      );
  late final _ggml_nelements = _ggml_nelementsPtr
      .asFunction<int Function(ffi.Pointer<ggml_tensor>)>();

  int ggml_nrows(ffi.Pointer<ggml_tensor> tensor) {
    return _ggml_nrows(tensor);
  }

  late final _ggml_nrowsPtr =
      _lookup<ffi.NativeFunction<ffi.Int64 Function(ffi.Pointer<ggml_tensor>)>>(
        'ggml_nrows',
      );
  late final _ggml_nrows = _ggml_nrowsPtr
      .asFunction<int Function(ffi.Pointer<ggml_tensor>)>();

  int ggml_nbytes(ffi.Pointer<ggml_tensor> tensor) {
    return _ggml_nbytes(tensor);
  }

  late final _ggml_nbytesPtr =
      _lookup<ffi.NativeFunction<ffi.Size Function(ffi.Pointer<ggml_tensor>)>>(
        'ggml_nbytes',
      );
  late final _ggml_nbytes = _ggml_nbytesPtr
      .asFunction<int Function(ffi.Pointer<ggml_tensor>)>();

  int ggml_nbytes_pad(ffi.Pointer<ggml_tensor> tensor) {
    return _ggml_nbytes_pad(tensor);
  }

  late final _ggml_nbytes_padPtr =
      _lookup<ffi.NativeFunction<ffi.Size Function(ffi.Pointer<ggml_tensor>)>>(
        'ggml_nbytes_pad',
      );
  late final _ggml_nbytes_pad = _ggml_nbytes_padPtr
      .asFunction<int Function(ffi.Pointer<ggml_tensor>)>();

  int ggml_blck_size(ggml_type type) {
    return _ggml_blck_size(type.value);
  }

  late final _ggml_blck_sizePtr =
      _lookup<ffi.NativeFunction<ffi.Int64 Function(ffi.UnsignedInt)>>(
        'ggml_blck_size',
      );
  late final _ggml_blck_size = _ggml_blck_sizePtr
      .asFunction<int Function(int)>();

  int ggml_type_size(ggml_type type) {
    return _ggml_type_size(type.value);
  }

  late final _ggml_type_sizePtr =
      _lookup<ffi.NativeFunction<ffi.Size Function(ffi.UnsignedInt)>>(
        'ggml_type_size',
      );
  late final _ggml_type_size = _ggml_type_sizePtr
      .asFunction<int Function(int)>();

  int ggml_row_size(ggml_type type, int ne) {
    return _ggml_row_size(type.value, ne);
  }

  late final _ggml_row_sizePtr =
      _lookup<
        ffi.NativeFunction<ffi.Size Function(ffi.UnsignedInt, ffi.Int64)>
      >('ggml_row_size');
  late final _ggml_row_size = _ggml_row_sizePtr
      .asFunction<int Function(int, int)>();

  double ggml_type_sizef(ggml_type type) {
    return _ggml_type_sizef(type.value);
  }

  late final _ggml_type_sizefPtr =
      _lookup<ffi.NativeFunction<ffi.Double Function(ffi.UnsignedInt)>>(
        'ggml_type_sizef',
      );
  late final _ggml_type_sizef = _ggml_type_sizefPtr
      .asFunction<double Function(int)>();

  ffi.Pointer<ffi.Char> ggml_type_name(ggml_type type) {
    return _ggml_type_name(type.value);
  }

  late final _ggml_type_namePtr =
      _lookup<
        ffi.NativeFunction<ffi.Pointer<ffi.Char> Function(ffi.UnsignedInt)>
      >('ggml_type_name');
  late final _ggml_type_name = _ggml_type_namePtr
      .asFunction<ffi.Pointer<ffi.Char> Function(int)>();

  ffi.Pointer<ffi.Char> ggml_op_name(ggml_op op) {
    return _ggml_op_name(op.value);
  }

  late final _ggml_op_namePtr =
      _lookup<
        ffi.NativeFunction<ffi.Pointer<ffi.Char> Function(ffi.UnsignedInt)>
      >('ggml_op_name');
  late final _ggml_op_name = _ggml_op_namePtr
      .asFunction<ffi.Pointer<ffi.Char> Function(int)>();

  ffi.Pointer<ffi.Char> ggml_op_symbol(ggml_op op) {
    return _ggml_op_symbol(op.value);
  }

  late final _ggml_op_symbolPtr =
      _lookup<
        ffi.NativeFunction<ffi.Pointer<ffi.Char> Function(ffi.UnsignedInt)>
      >('ggml_op_symbol');
  late final _ggml_op_symbol = _ggml_op_symbolPtr
      .asFunction<ffi.Pointer<ffi.Char> Function(int)>();

  ffi.Pointer<ffi.Char> ggml_unary_op_name(ggml_unary_op op) {
    return _ggml_unary_op_name(op.value);
  }

  late final _ggml_unary_op_namePtr =
      _lookup<
        ffi.NativeFunction<ffi.Pointer<ffi.Char> Function(ffi.UnsignedInt)>
      >('ggml_unary_op_name');
  late final _ggml_unary_op_name = _ggml_unary_op_namePtr
      .asFunction<ffi.Pointer<ffi.Char> Function(int)>();

  ffi.Pointer<ffi.Char> ggml_glu_op_name(ggml_glu_op op) {
    return _ggml_glu_op_name(op.value);
  }

  late final _ggml_glu_op_namePtr =
      _lookup<
        ffi.NativeFunction<ffi.Pointer<ffi.Char> Function(ffi.UnsignedInt)>
      >('ggml_glu_op_name');
  late final _ggml_glu_op_name = _ggml_glu_op_namePtr
      .asFunction<ffi.Pointer<ffi.Char> Function(int)>();

  ffi.Pointer<ffi.Char> ggml_op_desc(ffi.Pointer<ggml_tensor> t) {
    return _ggml_op_desc(t);
  }

  late final _ggml_op_descPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ffi.Char> Function(ffi.Pointer<ggml_tensor>)
        >
      >('ggml_op_desc');
  late final _ggml_op_desc = _ggml_op_descPtr
      .asFunction<ffi.Pointer<ffi.Char> Function(ffi.Pointer<ggml_tensor>)>();

  int ggml_element_size(ffi.Pointer<ggml_tensor> tensor) {
    return _ggml_element_size(tensor);
  }

  late final _ggml_element_sizePtr =
      _lookup<ffi.NativeFunction<ffi.Size Function(ffi.Pointer<ggml_tensor>)>>(
        'ggml_element_size',
      );
  late final _ggml_element_size = _ggml_element_sizePtr
      .asFunction<int Function(ffi.Pointer<ggml_tensor>)>();

  bool ggml_is_quantized(ggml_type type) {
    return _ggml_is_quantized(type.value);
  }

  late final _ggml_is_quantizedPtr =
      _lookup<ffi.NativeFunction<ffi.Bool Function(ffi.UnsignedInt)>>(
        'ggml_is_quantized',
      );
  late final _ggml_is_quantized = _ggml_is_quantizedPtr
      .asFunction<bool Function(int)>();

  /// TODO: temporary until model loading of ggml examples is refactored
  ggml_type ggml_ftype_to_ggml_type(ggml_ftype ftype) {
    return ggml_type.fromValue(_ggml_ftype_to_ggml_type(ftype.value));
  }

  late final _ggml_ftype_to_ggml_typePtr =
      _lookup<ffi.NativeFunction<ffi.UnsignedInt Function(ffi.Int)>>(
        'ggml_ftype_to_ggml_type',
      );
  late final _ggml_ftype_to_ggml_type = _ggml_ftype_to_ggml_typePtr
      .asFunction<int Function(int)>();

  bool ggml_is_transposed(ffi.Pointer<ggml_tensor> tensor) {
    return _ggml_is_transposed(tensor);
  }

  late final _ggml_is_transposedPtr =
      _lookup<ffi.NativeFunction<ffi.Bool Function(ffi.Pointer<ggml_tensor>)>>(
        'ggml_is_transposed',
      );
  late final _ggml_is_transposed = _ggml_is_transposedPtr
      .asFunction<bool Function(ffi.Pointer<ggml_tensor>)>();

  bool ggml_is_permuted(ffi.Pointer<ggml_tensor> tensor) {
    return _ggml_is_permuted(tensor);
  }

  late final _ggml_is_permutedPtr =
      _lookup<ffi.NativeFunction<ffi.Bool Function(ffi.Pointer<ggml_tensor>)>>(
        'ggml_is_permuted',
      );
  late final _ggml_is_permuted = _ggml_is_permutedPtr
      .asFunction<bool Function(ffi.Pointer<ggml_tensor>)>();

  bool ggml_is_empty(ffi.Pointer<ggml_tensor> tensor) {
    return _ggml_is_empty(tensor);
  }

  late final _ggml_is_emptyPtr =
      _lookup<ffi.NativeFunction<ffi.Bool Function(ffi.Pointer<ggml_tensor>)>>(
        'ggml_is_empty',
      );
  late final _ggml_is_empty = _ggml_is_emptyPtr
      .asFunction<bool Function(ffi.Pointer<ggml_tensor>)>();

  bool ggml_is_scalar(ffi.Pointer<ggml_tensor> tensor) {
    return _ggml_is_scalar(tensor);
  }

  late final _ggml_is_scalarPtr =
      _lookup<ffi.NativeFunction<ffi.Bool Function(ffi.Pointer<ggml_tensor>)>>(
        'ggml_is_scalar',
      );
  late final _ggml_is_scalar = _ggml_is_scalarPtr
      .asFunction<bool Function(ffi.Pointer<ggml_tensor>)>();

  bool ggml_is_vector(ffi.Pointer<ggml_tensor> tensor) {
    return _ggml_is_vector(tensor);
  }

  late final _ggml_is_vectorPtr =
      _lookup<ffi.NativeFunction<ffi.Bool Function(ffi.Pointer<ggml_tensor>)>>(
        'ggml_is_vector',
      );
  late final _ggml_is_vector = _ggml_is_vectorPtr
      .asFunction<bool Function(ffi.Pointer<ggml_tensor>)>();

  bool ggml_is_matrix(ffi.Pointer<ggml_tensor> tensor) {
    return _ggml_is_matrix(tensor);
  }

  late final _ggml_is_matrixPtr =
      _lookup<ffi.NativeFunction<ffi.Bool Function(ffi.Pointer<ggml_tensor>)>>(
        'ggml_is_matrix',
      );
  late final _ggml_is_matrix = _ggml_is_matrixPtr
      .asFunction<bool Function(ffi.Pointer<ggml_tensor>)>();

  bool ggml_is_3d(ffi.Pointer<ggml_tensor> tensor) {
    return _ggml_is_3d(tensor);
  }

  late final _ggml_is_3dPtr =
      _lookup<ffi.NativeFunction<ffi.Bool Function(ffi.Pointer<ggml_tensor>)>>(
        'ggml_is_3d',
      );
  late final _ggml_is_3d = _ggml_is_3dPtr
      .asFunction<bool Function(ffi.Pointer<ggml_tensor>)>();

  int ggml_n_dims(ffi.Pointer<ggml_tensor> tensor) {
    return _ggml_n_dims(tensor);
  }

  late final _ggml_n_dimsPtr =
      _lookup<ffi.NativeFunction<ffi.Int Function(ffi.Pointer<ggml_tensor>)>>(
        'ggml_n_dims',
      );
  late final _ggml_n_dims = _ggml_n_dimsPtr
      .asFunction<int Function(ffi.Pointer<ggml_tensor>)>();

  /// returns whether the tensor elements can be iterated over with a flattened index (no gaps, no permutation)
  bool ggml_is_contiguous(ffi.Pointer<ggml_tensor> tensor) {
    return _ggml_is_contiguous(tensor);
  }

  late final _ggml_is_contiguousPtr =
      _lookup<ffi.NativeFunction<ffi.Bool Function(ffi.Pointer<ggml_tensor>)>>(
        'ggml_is_contiguous',
      );
  late final _ggml_is_contiguous = _ggml_is_contiguousPtr
      .asFunction<bool Function(ffi.Pointer<ggml_tensor>)>();

  bool ggml_is_contiguous_0(ffi.Pointer<ggml_tensor> tensor) {
    return _ggml_is_contiguous_0(tensor);
  }

  late final _ggml_is_contiguous_0Ptr =
      _lookup<ffi.NativeFunction<ffi.Bool Function(ffi.Pointer<ggml_tensor>)>>(
        'ggml_is_contiguous_0',
      );
  late final _ggml_is_contiguous_0 = _ggml_is_contiguous_0Ptr
      .asFunction<bool Function(ffi.Pointer<ggml_tensor>)>();

  bool ggml_is_contiguous_1(ffi.Pointer<ggml_tensor> tensor) {
    return _ggml_is_contiguous_1(tensor);
  }

  late final _ggml_is_contiguous_1Ptr =
      _lookup<ffi.NativeFunction<ffi.Bool Function(ffi.Pointer<ggml_tensor>)>>(
        'ggml_is_contiguous_1',
      );
  late final _ggml_is_contiguous_1 = _ggml_is_contiguous_1Ptr
      .asFunction<bool Function(ffi.Pointer<ggml_tensor>)>();

  bool ggml_is_contiguous_2(ffi.Pointer<ggml_tensor> tensor) {
    return _ggml_is_contiguous_2(tensor);
  }

  late final _ggml_is_contiguous_2Ptr =
      _lookup<ffi.NativeFunction<ffi.Bool Function(ffi.Pointer<ggml_tensor>)>>(
        'ggml_is_contiguous_2',
      );
  late final _ggml_is_contiguous_2 = _ggml_is_contiguous_2Ptr
      .asFunction<bool Function(ffi.Pointer<ggml_tensor>)>();

  /// returns whether the tensor elements are allocated as one contiguous block of memory (no gaps, but permutation ok)
  bool ggml_is_contiguously_allocated(ffi.Pointer<ggml_tensor> tensor) {
    return _ggml_is_contiguously_allocated(tensor);
  }

  late final _ggml_is_contiguously_allocatedPtr =
      _lookup<ffi.NativeFunction<ffi.Bool Function(ffi.Pointer<ggml_tensor>)>>(
        'ggml_is_contiguously_allocated',
      );
  late final _ggml_is_contiguously_allocated =
      _ggml_is_contiguously_allocatedPtr
          .asFunction<bool Function(ffi.Pointer<ggml_tensor>)>();

  /// true for tensor that is stored in memory as CxWxHxN and has been permuted to WxHxCxN
  bool ggml_is_contiguous_channels(ffi.Pointer<ggml_tensor> tensor) {
    return _ggml_is_contiguous_channels(tensor);
  }

  late final _ggml_is_contiguous_channelsPtr =
      _lookup<ffi.NativeFunction<ffi.Bool Function(ffi.Pointer<ggml_tensor>)>>(
        'ggml_is_contiguous_channels',
      );
  late final _ggml_is_contiguous_channels = _ggml_is_contiguous_channelsPtr
      .asFunction<bool Function(ffi.Pointer<ggml_tensor>)>();

  /// true if the elements in dimension 0 are contiguous, or there is just 1 block of elements
  bool ggml_is_contiguous_rows(ffi.Pointer<ggml_tensor> tensor) {
    return _ggml_is_contiguous_rows(tensor);
  }

  late final _ggml_is_contiguous_rowsPtr =
      _lookup<ffi.NativeFunction<ffi.Bool Function(ffi.Pointer<ggml_tensor>)>>(
        'ggml_is_contiguous_rows',
      );
  late final _ggml_is_contiguous_rows = _ggml_is_contiguous_rowsPtr
      .asFunction<bool Function(ffi.Pointer<ggml_tensor>)>();

  bool ggml_are_same_shape(
    ffi.Pointer<ggml_tensor> t0,
    ffi.Pointer<ggml_tensor> t1,
  ) {
    return _ggml_are_same_shape(t0, t1);
  }

  late final _ggml_are_same_shapePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Bool Function(ffi.Pointer<ggml_tensor>, ffi.Pointer<ggml_tensor>)
        >
      >('ggml_are_same_shape');
  late final _ggml_are_same_shape = _ggml_are_same_shapePtr
      .asFunction<
        bool Function(ffi.Pointer<ggml_tensor>, ffi.Pointer<ggml_tensor>)
      >();

  bool ggml_are_same_stride(
    ffi.Pointer<ggml_tensor> t0,
    ffi.Pointer<ggml_tensor> t1,
  ) {
    return _ggml_are_same_stride(t0, t1);
  }

  late final _ggml_are_same_stridePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Bool Function(ffi.Pointer<ggml_tensor>, ffi.Pointer<ggml_tensor>)
        >
      >('ggml_are_same_stride');
  late final _ggml_are_same_stride = _ggml_are_same_stridePtr
      .asFunction<
        bool Function(ffi.Pointer<ggml_tensor>, ffi.Pointer<ggml_tensor>)
      >();

  bool ggml_can_repeat(
    ffi.Pointer<ggml_tensor> t0,
    ffi.Pointer<ggml_tensor> t1,
  ) {
    return _ggml_can_repeat(t0, t1);
  }

  late final _ggml_can_repeatPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Bool Function(ffi.Pointer<ggml_tensor>, ffi.Pointer<ggml_tensor>)
        >
      >('ggml_can_repeat');
  late final _ggml_can_repeat = _ggml_can_repeatPtr
      .asFunction<
        bool Function(ffi.Pointer<ggml_tensor>, ffi.Pointer<ggml_tensor>)
      >();

  /// use this to compute the memory overhead of a tensor
  int ggml_tensor_overhead() {
    return _ggml_tensor_overhead();
  }

  late final _ggml_tensor_overheadPtr =
      _lookup<ffi.NativeFunction<ffi.Size Function()>>('ggml_tensor_overhead');
  late final _ggml_tensor_overhead = _ggml_tensor_overheadPtr
      .asFunction<int Function()>();

  bool ggml_validate_row_data(
    ggml_type type,
    ffi.Pointer<ffi.Void> data,
    int nbytes,
  ) {
    return _ggml_validate_row_data(type.value, data, nbytes);
  }

  late final _ggml_validate_row_dataPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Bool Function(ffi.UnsignedInt, ffi.Pointer<ffi.Void>, ffi.Size)
        >
      >('ggml_validate_row_data');
  late final _ggml_validate_row_data = _ggml_validate_row_dataPtr
      .asFunction<bool Function(int, ffi.Pointer<ffi.Void>, int)>();

  /// main
  ffi.Pointer<ggml_context> ggml_init(ggml_init_params params) {
    return _ggml_init(params);
  }

  late final _ggml_initPtr =
      _lookup<
        ffi.NativeFunction<ffi.Pointer<ggml_context> Function(ggml_init_params)>
      >('ggml_init');
  late final _ggml_init = _ggml_initPtr
      .asFunction<ffi.Pointer<ggml_context> Function(ggml_init_params)>();

  void ggml_reset(ffi.Pointer<ggml_context> ctx) {
    return _ggml_reset(ctx);
  }

  late final _ggml_resetPtr =
      _lookup<ffi.NativeFunction<ffi.Void Function(ffi.Pointer<ggml_context>)>>(
        'ggml_reset',
      );
  late final _ggml_reset = _ggml_resetPtr
      .asFunction<void Function(ffi.Pointer<ggml_context>)>();

  void ggml_free(ffi.Pointer<ggml_context> ctx) {
    return _ggml_free(ctx);
  }

  late final _ggml_freePtr =
      _lookup<ffi.NativeFunction<ffi.Void Function(ffi.Pointer<ggml_context>)>>(
        'ggml_free',
      );
  late final _ggml_free = _ggml_freePtr
      .asFunction<void Function(ffi.Pointer<ggml_context>)>();

  int ggml_used_mem(ffi.Pointer<ggml_context> ctx) {
    return _ggml_used_mem(ctx);
  }

  late final _ggml_used_memPtr =
      _lookup<ffi.NativeFunction<ffi.Size Function(ffi.Pointer<ggml_context>)>>(
        'ggml_used_mem',
      );
  late final _ggml_used_mem = _ggml_used_memPtr
      .asFunction<int Function(ffi.Pointer<ggml_context>)>();

  bool ggml_get_no_alloc(ffi.Pointer<ggml_context> ctx) {
    return _ggml_get_no_alloc(ctx);
  }

  late final _ggml_get_no_allocPtr =
      _lookup<ffi.NativeFunction<ffi.Bool Function(ffi.Pointer<ggml_context>)>>(
        'ggml_get_no_alloc',
      );
  late final _ggml_get_no_alloc = _ggml_get_no_allocPtr
      .asFunction<bool Function(ffi.Pointer<ggml_context>)>();

  void ggml_set_no_alloc(ffi.Pointer<ggml_context> ctx, bool no_alloc) {
    return _ggml_set_no_alloc(ctx, no_alloc);
  }

  late final _ggml_set_no_allocPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<ggml_context>, ffi.Bool)
        >
      >('ggml_set_no_alloc');
  late final _ggml_set_no_alloc = _ggml_set_no_allocPtr
      .asFunction<void Function(ffi.Pointer<ggml_context>, bool)>();

  ffi.Pointer<ffi.Void> ggml_get_mem_buffer(ffi.Pointer<ggml_context> ctx) {
    return _ggml_get_mem_buffer(ctx);
  }

  late final _ggml_get_mem_bufferPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ffi.Void> Function(ffi.Pointer<ggml_context>)
        >
      >('ggml_get_mem_buffer');
  late final _ggml_get_mem_buffer = _ggml_get_mem_bufferPtr
      .asFunction<ffi.Pointer<ffi.Void> Function(ffi.Pointer<ggml_context>)>();

  int ggml_get_mem_size(ffi.Pointer<ggml_context> ctx) {
    return _ggml_get_mem_size(ctx);
  }

  late final _ggml_get_mem_sizePtr =
      _lookup<ffi.NativeFunction<ffi.Size Function(ffi.Pointer<ggml_context>)>>(
        'ggml_get_mem_size',
      );
  late final _ggml_get_mem_size = _ggml_get_mem_sizePtr
      .asFunction<int Function(ffi.Pointer<ggml_context>)>();

  int ggml_get_max_tensor_size(ffi.Pointer<ggml_context> ctx) {
    return _ggml_get_max_tensor_size(ctx);
  }

  late final _ggml_get_max_tensor_sizePtr =
      _lookup<ffi.NativeFunction<ffi.Size Function(ffi.Pointer<ggml_context>)>>(
        'ggml_get_max_tensor_size',
      );
  late final _ggml_get_max_tensor_size = _ggml_get_max_tensor_sizePtr
      .asFunction<int Function(ffi.Pointer<ggml_context>)>();

  ffi.Pointer<ggml_tensor> ggml_new_tensor(
    ffi.Pointer<ggml_context> ctx,
    ggml_type type,
    int n_dims,
    ffi.Pointer<ffi.Int64> ne,
  ) {
    return _ggml_new_tensor(ctx, type.value, n_dims, ne);
  }

  late final _ggml_new_tensorPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.UnsignedInt,
            ffi.Int,
            ffi.Pointer<ffi.Int64>,
          )
        >
      >('ggml_new_tensor');
  late final _ggml_new_tensor = _ggml_new_tensorPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          int,
          int,
          ffi.Pointer<ffi.Int64>,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_new_tensor_1d(
    ffi.Pointer<ggml_context> ctx,
    ggml_type type,
    int ne0,
  ) {
    return _ggml_new_tensor_1d(ctx, type.value, ne0);
  }

  late final _ggml_new_tensor_1dPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.UnsignedInt,
            ffi.Int64,
          )
        >
      >('ggml_new_tensor_1d');
  late final _ggml_new_tensor_1d = _ggml_new_tensor_1dPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>, int, int)
      >();

  ffi.Pointer<ggml_tensor> ggml_new_tensor_2d(
    ffi.Pointer<ggml_context> ctx,
    ggml_type type,
    int ne0,
    int ne1,
  ) {
    return _ggml_new_tensor_2d(ctx, type.value, ne0, ne1);
  }

  late final _ggml_new_tensor_2dPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.UnsignedInt,
            ffi.Int64,
            ffi.Int64,
          )
        >
      >('ggml_new_tensor_2d');
  late final _ggml_new_tensor_2d = _ggml_new_tensor_2dPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          int,
          int,
          int,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_new_tensor_3d(
    ffi.Pointer<ggml_context> ctx,
    ggml_type type,
    int ne0,
    int ne1,
    int ne2,
  ) {
    return _ggml_new_tensor_3d(ctx, type.value, ne0, ne1, ne2);
  }

  late final _ggml_new_tensor_3dPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.UnsignedInt,
            ffi.Int64,
            ffi.Int64,
            ffi.Int64,
          )
        >
      >('ggml_new_tensor_3d');
  late final _ggml_new_tensor_3d = _ggml_new_tensor_3dPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          int,
          int,
          int,
          int,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_new_tensor_4d(
    ffi.Pointer<ggml_context> ctx,
    ggml_type type,
    int ne0,
    int ne1,
    int ne2,
    int ne3,
  ) {
    return _ggml_new_tensor_4d(ctx, type.value, ne0, ne1, ne2, ne3);
  }

  late final _ggml_new_tensor_4dPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.UnsignedInt,
            ffi.Int64,
            ffi.Int64,
            ffi.Int64,
            ffi.Int64,
          )
        >
      >('ggml_new_tensor_4d');
  late final _ggml_new_tensor_4d = _ggml_new_tensor_4dPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          int,
          int,
          int,
          int,
          int,
        )
      >();

  ffi.Pointer<ffi.Void> ggml_new_buffer(
    ffi.Pointer<ggml_context> ctx,
    int nbytes,
  ) {
    return _ggml_new_buffer(ctx, nbytes);
  }

  late final _ggml_new_bufferPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ffi.Void> Function(ffi.Pointer<ggml_context>, ffi.Size)
        >
      >('ggml_new_buffer');
  late final _ggml_new_buffer = _ggml_new_bufferPtr
      .asFunction<
        ffi.Pointer<ffi.Void> Function(ffi.Pointer<ggml_context>, int)
      >();

  ffi.Pointer<ggml_tensor> ggml_dup_tensor(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> src,
  ) {
    return _ggml_dup_tensor(ctx, src);
  }

  late final _ggml_dup_tensorPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_dup_tensor');
  late final _ggml_dup_tensor = _ggml_dup_tensorPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_view_tensor(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> src,
  ) {
    return _ggml_view_tensor(ctx, src);
  }

  late final _ggml_view_tensorPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_view_tensor');
  late final _ggml_view_tensor = _ggml_view_tensorPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  /// Context tensor enumeration and lookup
  ffi.Pointer<ggml_tensor> ggml_get_first_tensor(
    ffi.Pointer<ggml_context> ctx,
  ) {
    return _ggml_get_first_tensor(ctx);
  }

  late final _ggml_get_first_tensorPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>)
        >
      >('ggml_get_first_tensor');
  late final _ggml_get_first_tensor = _ggml_get_first_tensorPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>)
      >();

  ffi.Pointer<ggml_tensor> ggml_get_next_tensor(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> tensor,
  ) {
    return _ggml_get_next_tensor(ctx, tensor);
  }

  late final _ggml_get_next_tensorPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_get_next_tensor');
  late final _ggml_get_next_tensor = _ggml_get_next_tensorPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_get_tensor(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ffi.Char> name,
  ) {
    return _ggml_get_tensor(ctx, name);
  }

  late final _ggml_get_tensorPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ffi.Char>,
          )
        >
      >('ggml_get_tensor');
  late final _ggml_get_tensor = _ggml_get_tensorPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ffi.Char>,
        )
      >();

  /// Converts a flat index into coordinates
  void ggml_unravel_index(
    ffi.Pointer<ggml_tensor> tensor,
    int i,
    ffi.Pointer<ffi.Int64> i0,
    ffi.Pointer<ffi.Int64> i1,
    ffi.Pointer<ffi.Int64> i2,
    ffi.Pointer<ffi.Int64> i3,
  ) {
    return _ggml_unravel_index(tensor, i, i0, i1, i2, i3);
  }

  late final _ggml_unravel_indexPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Void Function(
            ffi.Pointer<ggml_tensor>,
            ffi.Int64,
            ffi.Pointer<ffi.Int64>,
            ffi.Pointer<ffi.Int64>,
            ffi.Pointer<ffi.Int64>,
            ffi.Pointer<ffi.Int64>,
          )
        >
      >('ggml_unravel_index');
  late final _ggml_unravel_index = _ggml_unravel_indexPtr
      .asFunction<
        void Function(
          ffi.Pointer<ggml_tensor>,
          int,
          ffi.Pointer<ffi.Int64>,
          ffi.Pointer<ffi.Int64>,
          ffi.Pointer<ffi.Int64>,
          ffi.Pointer<ffi.Int64>,
        )
      >();

  ggml_unary_op ggml_get_unary_op(ffi.Pointer<ggml_tensor> tensor) {
    return ggml_unary_op.fromValue(_ggml_get_unary_op(tensor));
  }

  late final _ggml_get_unary_opPtr =
      _lookup<
        ffi.NativeFunction<ffi.UnsignedInt Function(ffi.Pointer<ggml_tensor>)>
      >('ggml_get_unary_op');
  late final _ggml_get_unary_op = _ggml_get_unary_opPtr
      .asFunction<int Function(ffi.Pointer<ggml_tensor>)>();

  ggml_glu_op ggml_get_glu_op(ffi.Pointer<ggml_tensor> tensor) {
    return ggml_glu_op.fromValue(_ggml_get_glu_op(tensor));
  }

  late final _ggml_get_glu_opPtr =
      _lookup<
        ffi.NativeFunction<ffi.UnsignedInt Function(ffi.Pointer<ggml_tensor>)>
      >('ggml_get_glu_op');
  late final _ggml_get_glu_op = _ggml_get_glu_opPtr
      .asFunction<int Function(ffi.Pointer<ggml_tensor>)>();

  ffi.Pointer<ffi.Void> ggml_get_data(ffi.Pointer<ggml_tensor> tensor) {
    return _ggml_get_data(tensor);
  }

  late final _ggml_get_dataPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ffi.Void> Function(ffi.Pointer<ggml_tensor>)
        >
      >('ggml_get_data');
  late final _ggml_get_data = _ggml_get_dataPtr
      .asFunction<ffi.Pointer<ffi.Void> Function(ffi.Pointer<ggml_tensor>)>();

  ffi.Pointer<ffi.Float> ggml_get_data_f32(ffi.Pointer<ggml_tensor> tensor) {
    return _ggml_get_data_f32(tensor);
  }

  late final _ggml_get_data_f32Ptr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ffi.Float> Function(ffi.Pointer<ggml_tensor>)
        >
      >('ggml_get_data_f32');
  late final _ggml_get_data_f32 = _ggml_get_data_f32Ptr
      .asFunction<ffi.Pointer<ffi.Float> Function(ffi.Pointer<ggml_tensor>)>();

  ffi.Pointer<ffi.Char> ggml_get_name(ffi.Pointer<ggml_tensor> tensor) {
    return _ggml_get_name(tensor);
  }

  late final _ggml_get_namePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ffi.Char> Function(ffi.Pointer<ggml_tensor>)
        >
      >('ggml_get_name');
  late final _ggml_get_name = _ggml_get_namePtr
      .asFunction<ffi.Pointer<ffi.Char> Function(ffi.Pointer<ggml_tensor>)>();

  ffi.Pointer<ggml_tensor> ggml_set_name(
    ffi.Pointer<ggml_tensor> tensor,
    ffi.Pointer<ffi.Char> name,
  ) {
    return _ggml_set_name(tensor, name);
  }

  late final _ggml_set_namePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ffi.Char>,
          )
        >
      >('ggml_set_name');
  late final _ggml_set_name = _ggml_set_namePtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ffi.Char>,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_format_name(
    ffi.Pointer<ggml_tensor> tensor,
    ffi.Pointer<ffi.Char> fmt,
  ) {
    return _ggml_format_name(tensor, fmt);
  }

  late final _ggml_format_namePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ffi.Char>,
          )
        >
      >('ggml_format_name');
  late final _ggml_format_name = _ggml_format_namePtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ffi.Char>,
        )
      >();

  /// Tensor flags
  void ggml_set_input(ffi.Pointer<ggml_tensor> tensor) {
    return _ggml_set_input(tensor);
  }

  late final _ggml_set_inputPtr =
      _lookup<ffi.NativeFunction<ffi.Void Function(ffi.Pointer<ggml_tensor>)>>(
        'ggml_set_input',
      );
  late final _ggml_set_input = _ggml_set_inputPtr
      .asFunction<void Function(ffi.Pointer<ggml_tensor>)>();

  void ggml_set_output(ffi.Pointer<ggml_tensor> tensor) {
    return _ggml_set_output(tensor);
  }

  late final _ggml_set_outputPtr =
      _lookup<ffi.NativeFunction<ffi.Void Function(ffi.Pointer<ggml_tensor>)>>(
        'ggml_set_output',
      );
  late final _ggml_set_output = _ggml_set_outputPtr
      .asFunction<void Function(ffi.Pointer<ggml_tensor>)>();

  void ggml_set_param(ffi.Pointer<ggml_tensor> tensor) {
    return _ggml_set_param(tensor);
  }

  late final _ggml_set_paramPtr =
      _lookup<ffi.NativeFunction<ffi.Void Function(ffi.Pointer<ggml_tensor>)>>(
        'ggml_set_param',
      );
  late final _ggml_set_param = _ggml_set_paramPtr
      .asFunction<void Function(ffi.Pointer<ggml_tensor>)>();

  void ggml_set_loss(ffi.Pointer<ggml_tensor> tensor) {
    return _ggml_set_loss(tensor);
  }

  late final _ggml_set_lossPtr =
      _lookup<ffi.NativeFunction<ffi.Void Function(ffi.Pointer<ggml_tensor>)>>(
        'ggml_set_loss',
      );
  late final _ggml_set_loss = _ggml_set_lossPtr
      .asFunction<void Function(ffi.Pointer<ggml_tensor>)>();

  /// operations on tensors with backpropagation
  ffi.Pointer<ggml_tensor> ggml_dup(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_dup(ctx, a);
  }

  late final _ggml_dupPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_dup');
  late final _ggml_dup = _ggml_dupPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  /// in-place, returns view(a)
  ffi.Pointer<ggml_tensor> ggml_dup_inplace(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_dup_inplace(ctx, a);
  }

  late final _ggml_dup_inplacePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_dup_inplace');
  late final _ggml_dup_inplace = _ggml_dup_inplacePtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_add(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
  ) {
    return _ggml_add(ctx, a, b);
  }

  late final _ggml_addPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_add');
  late final _ggml_add = _ggml_addPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_add_inplace(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
  ) {
    return _ggml_add_inplace(ctx, a, b);
  }

  late final _ggml_add_inplacePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_add_inplace');
  late final _ggml_add_inplace = _ggml_add_inplacePtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_add_cast(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
    ggml_type type,
  ) {
    return _ggml_add_cast(ctx, a, b, type.value);
  }

  late final _ggml_add_castPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
            ffi.UnsignedInt,
          )
        >
      >('ggml_add_cast');
  late final _ggml_add_cast = _ggml_add_castPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          int,
        )
      >();

  /// dst[i0, i1, i2] = a[i0, i1, i2] + b[i0, ids[i1, i2]]
  ffi.Pointer<ggml_tensor> ggml_add_id(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
    ffi.Pointer<ggml_tensor> ids,
  ) {
    return _ggml_add_id(ctx, a, b, ids);
  }

  late final _ggml_add_idPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_add_id');
  late final _ggml_add_id = _ggml_add_idPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_add1(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
  ) {
    return _ggml_add1(ctx, a, b);
  }

  late final _ggml_add1Ptr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_add1');
  late final _ggml_add1 = _ggml_add1Ptr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_add1_inplace(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
  ) {
    return _ggml_add1_inplace(ctx, a, b);
  }

  late final _ggml_add1_inplacePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_add1_inplace');
  late final _ggml_add1_inplace = _ggml_add1_inplacePtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  /// dst = a
  /// view(dst, nb1, nb2, nb3, offset) += b
  /// return dst
  ffi.Pointer<ggml_tensor> ggml_acc(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
    int nb1,
    int nb2,
    int nb3,
    int offset,
  ) {
    return _ggml_acc(ctx, a, b, nb1, nb2, nb3, offset);
  }

  late final _ggml_accPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
            ffi.Size,
            ffi.Size,
            ffi.Size,
            ffi.Size,
          )
        >
      >('ggml_acc');
  late final _ggml_acc = _ggml_accPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          int,
          int,
          int,
          int,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_acc_inplace(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
    int nb1,
    int nb2,
    int nb3,
    int offset,
  ) {
    return _ggml_acc_inplace(ctx, a, b, nb1, nb2, nb3, offset);
  }

  late final _ggml_acc_inplacePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
            ffi.Size,
            ffi.Size,
            ffi.Size,
            ffi.Size,
          )
        >
      >('ggml_acc_inplace');
  late final _ggml_acc_inplace = _ggml_acc_inplacePtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          int,
          int,
          int,
          int,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_sub(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
  ) {
    return _ggml_sub(ctx, a, b);
  }

  late final _ggml_subPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_sub');
  late final _ggml_sub = _ggml_subPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_sub_inplace(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
  ) {
    return _ggml_sub_inplace(ctx, a, b);
  }

  late final _ggml_sub_inplacePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_sub_inplace');
  late final _ggml_sub_inplace = _ggml_sub_inplacePtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_mul(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
  ) {
    return _ggml_mul(ctx, a, b);
  }

  late final _ggml_mulPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_mul');
  late final _ggml_mul = _ggml_mulPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_mul_inplace(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
  ) {
    return _ggml_mul_inplace(ctx, a, b);
  }

  late final _ggml_mul_inplacePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_mul_inplace');
  late final _ggml_mul_inplace = _ggml_mul_inplacePtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_div(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
  ) {
    return _ggml_div(ctx, a, b);
  }

  late final _ggml_divPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_div');
  late final _ggml_div = _ggml_divPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_div_inplace(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
  ) {
    return _ggml_div_inplace(ctx, a, b);
  }

  late final _ggml_div_inplacePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_div_inplace');
  late final _ggml_div_inplace = _ggml_div_inplacePtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_sqr(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_sqr(ctx, a);
  }

  late final _ggml_sqrPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_sqr');
  late final _ggml_sqr = _ggml_sqrPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_sqr_inplace(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_sqr_inplace(ctx, a);
  }

  late final _ggml_sqr_inplacePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_sqr_inplace');
  late final _ggml_sqr_inplace = _ggml_sqr_inplacePtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_sqrt(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_sqrt(ctx, a);
  }

  late final _ggml_sqrtPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_sqrt');
  late final _ggml_sqrt = _ggml_sqrtPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_sqrt_inplace(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_sqrt_inplace(ctx, a);
  }

  late final _ggml_sqrt_inplacePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_sqrt_inplace');
  late final _ggml_sqrt_inplace = _ggml_sqrt_inplacePtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_log(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_log(ctx, a);
  }

  late final _ggml_logPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_log');
  late final _ggml_log = _ggml_logPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_log_inplace(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_log_inplace(ctx, a);
  }

  late final _ggml_log_inplacePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_log_inplace');
  late final _ggml_log_inplace = _ggml_log_inplacePtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_expm1(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_expm1(ctx, a);
  }

  late final _ggml_expm1Ptr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_expm1');
  late final _ggml_expm1 = _ggml_expm1Ptr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_expm1_inplace(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_expm1_inplace(ctx, a);
  }

  late final _ggml_expm1_inplacePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_expm1_inplace');
  late final _ggml_expm1_inplace = _ggml_expm1_inplacePtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_softplus(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_softplus(ctx, a);
  }

  late final _ggml_softplusPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_softplus');
  late final _ggml_softplus = _ggml_softplusPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_softplus_inplace(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_softplus_inplace(ctx, a);
  }

  late final _ggml_softplus_inplacePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_softplus_inplace');
  late final _ggml_softplus_inplace = _ggml_softplus_inplacePtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_sin(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_sin(ctx, a);
  }

  late final _ggml_sinPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_sin');
  late final _ggml_sin = _ggml_sinPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_sin_inplace(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_sin_inplace(ctx, a);
  }

  late final _ggml_sin_inplacePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_sin_inplace');
  late final _ggml_sin_inplace = _ggml_sin_inplacePtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_cos(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_cos(ctx, a);
  }

  late final _ggml_cosPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_cos');
  late final _ggml_cos = _ggml_cosPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_cos_inplace(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_cos_inplace(ctx, a);
  }

  late final _ggml_cos_inplacePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_cos_inplace');
  late final _ggml_cos_inplace = _ggml_cos_inplacePtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  /// return scalar
  ffi.Pointer<ggml_tensor> ggml_sum(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_sum(ctx, a);
  }

  late final _ggml_sumPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_sum');
  late final _ggml_sum = _ggml_sumPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  /// sums along rows, with input shape [a,b,c,d] return shape [1,b,c,d]
  ffi.Pointer<ggml_tensor> ggml_sum_rows(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_sum_rows(ctx, a);
  }

  late final _ggml_sum_rowsPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_sum_rows');
  late final _ggml_sum_rows = _ggml_sum_rowsPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_cumsum(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_cumsum(ctx, a);
  }

  late final _ggml_cumsumPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_cumsum');
  late final _ggml_cumsum = _ggml_cumsumPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  /// mean along rows
  ffi.Pointer<ggml_tensor> ggml_mean(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_mean(ctx, a);
  }

  late final _ggml_meanPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_mean');
  late final _ggml_mean = _ggml_meanPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  /// argmax along rows
  ffi.Pointer<ggml_tensor> ggml_argmax(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_argmax(ctx, a);
  }

  late final _ggml_argmaxPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_argmax');
  late final _ggml_argmax = _ggml_argmaxPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  /// count number of equal elements in a and b
  ffi.Pointer<ggml_tensor> ggml_count_equal(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
  ) {
    return _ggml_count_equal(ctx, a, b);
  }

  late final _ggml_count_equalPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_count_equal');
  late final _ggml_count_equal = _ggml_count_equalPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  /// if a is the same shape as b, and a is not parameter, return a
  /// otherwise, return a new tensor: repeat(a) to fit in b
  ffi.Pointer<ggml_tensor> ggml_repeat(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
  ) {
    return _ggml_repeat(ctx, a, b);
  }

  late final _ggml_repeatPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_repeat');
  late final _ggml_repeat = _ggml_repeatPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  /// repeat a to the specified shape
  ffi.Pointer<ggml_tensor> ggml_repeat_4d(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    int ne0,
    int ne1,
    int ne2,
    int ne3,
  ) {
    return _ggml_repeat_4d(ctx, a, ne0, ne1, ne2, ne3);
  }

  late final _ggml_repeat_4dPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Int64,
            ffi.Int64,
            ffi.Int64,
            ffi.Int64,
          )
        >
      >('ggml_repeat_4d');
  late final _ggml_repeat_4d = _ggml_repeat_4dPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          int,
          int,
          int,
          int,
        )
      >();

  /// sums repetitions in a into shape of b
  ffi.Pointer<ggml_tensor> ggml_repeat_back(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
  ) {
    return _ggml_repeat_back(ctx, a, b);
  }

  late final _ggml_repeat_backPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_repeat_back');
  late final _ggml_repeat_back = _ggml_repeat_backPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  /// concat a and b along dim
  /// used in stable-diffusion
  ffi.Pointer<ggml_tensor> ggml_concat(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
    int dim,
  ) {
    return _ggml_concat(ctx, a, b, dim);
  }

  late final _ggml_concatPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
            ffi.Int,
          )
        >
      >('ggml_concat');
  late final _ggml_concat = _ggml_concatPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          int,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_abs(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_abs(ctx, a);
  }

  late final _ggml_absPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_abs');
  late final _ggml_abs = _ggml_absPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_abs_inplace(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_abs_inplace(ctx, a);
  }

  late final _ggml_abs_inplacePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_abs_inplace');
  late final _ggml_abs_inplace = _ggml_abs_inplacePtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_sgn(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_sgn(ctx, a);
  }

  late final _ggml_sgnPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_sgn');
  late final _ggml_sgn = _ggml_sgnPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_sgn_inplace(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_sgn_inplace(ctx, a);
  }

  late final _ggml_sgn_inplacePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_sgn_inplace');
  late final _ggml_sgn_inplace = _ggml_sgn_inplacePtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_neg(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_neg(ctx, a);
  }

  late final _ggml_negPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_neg');
  late final _ggml_neg = _ggml_negPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_neg_inplace(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_neg_inplace(ctx, a);
  }

  late final _ggml_neg_inplacePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_neg_inplace');
  late final _ggml_neg_inplace = _ggml_neg_inplacePtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_step(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_step(ctx, a);
  }

  late final _ggml_stepPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_step');
  late final _ggml_step = _ggml_stepPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_step_inplace(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_step_inplace(ctx, a);
  }

  late final _ggml_step_inplacePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_step_inplace');
  late final _ggml_step_inplace = _ggml_step_inplacePtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_tanh(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_tanh(ctx, a);
  }

  late final _ggml_tanhPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_tanh');
  late final _ggml_tanh = _ggml_tanhPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_tanh_inplace(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_tanh_inplace(ctx, a);
  }

  late final _ggml_tanh_inplacePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_tanh_inplace');
  late final _ggml_tanh_inplace = _ggml_tanh_inplacePtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_elu(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_elu(ctx, a);
  }

  late final _ggml_eluPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_elu');
  late final _ggml_elu = _ggml_eluPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_elu_inplace(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_elu_inplace(ctx, a);
  }

  late final _ggml_elu_inplacePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_elu_inplace');
  late final _ggml_elu_inplace = _ggml_elu_inplacePtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_relu(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_relu(ctx, a);
  }

  late final _ggml_reluPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_relu');
  late final _ggml_relu = _ggml_reluPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_leaky_relu(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    double negative_slope,
    bool inplace,
  ) {
    return _ggml_leaky_relu(ctx, a, negative_slope, inplace);
  }

  late final _ggml_leaky_reluPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Float,
            ffi.Bool,
          )
        >
      >('ggml_leaky_relu');
  late final _ggml_leaky_relu = _ggml_leaky_reluPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          double,
          bool,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_relu_inplace(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_relu_inplace(ctx, a);
  }

  late final _ggml_relu_inplacePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_relu_inplace');
  late final _ggml_relu_inplace = _ggml_relu_inplacePtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_sigmoid(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_sigmoid(ctx, a);
  }

  late final _ggml_sigmoidPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_sigmoid');
  late final _ggml_sigmoid = _ggml_sigmoidPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_sigmoid_inplace(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_sigmoid_inplace(ctx, a);
  }

  late final _ggml_sigmoid_inplacePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_sigmoid_inplace');
  late final _ggml_sigmoid_inplace = _ggml_sigmoid_inplacePtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_gelu(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_gelu(ctx, a);
  }

  late final _ggml_geluPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_gelu');
  late final _ggml_gelu = _ggml_geluPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_gelu_inplace(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_gelu_inplace(ctx, a);
  }

  late final _ggml_gelu_inplacePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_gelu_inplace');
  late final _ggml_gelu_inplace = _ggml_gelu_inplacePtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  /// GELU using erf (error function) when possible
  /// some backends may fallback to approximation based on Abramowitz and Stegun formula
  ffi.Pointer<ggml_tensor> ggml_gelu_erf(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_gelu_erf(ctx, a);
  }

  late final _ggml_gelu_erfPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_gelu_erf');
  late final _ggml_gelu_erf = _ggml_gelu_erfPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_gelu_erf_inplace(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_gelu_erf_inplace(ctx, a);
  }

  late final _ggml_gelu_erf_inplacePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_gelu_erf_inplace');
  late final _ggml_gelu_erf_inplace = _ggml_gelu_erf_inplacePtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_gelu_quick(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_gelu_quick(ctx, a);
  }

  late final _ggml_gelu_quickPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_gelu_quick');
  late final _ggml_gelu_quick = _ggml_gelu_quickPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_gelu_quick_inplace(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_gelu_quick_inplace(ctx, a);
  }

  late final _ggml_gelu_quick_inplacePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_gelu_quick_inplace');
  late final _ggml_gelu_quick_inplace = _ggml_gelu_quick_inplacePtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_silu(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_silu(ctx, a);
  }

  late final _ggml_siluPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_silu');
  late final _ggml_silu = _ggml_siluPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_silu_inplace(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_silu_inplace(ctx, a);
  }

  late final _ggml_silu_inplacePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_silu_inplace');
  late final _ggml_silu_inplace = _ggml_silu_inplacePtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  /// a - x
  /// b - dy
  ffi.Pointer<ggml_tensor> ggml_silu_back(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
  ) {
    return _ggml_silu_back(ctx, a, b);
  }

  late final _ggml_silu_backPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_silu_back');
  late final _ggml_silu_back = _ggml_silu_backPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  /// hardswish(x) = x * relu6(x + 3) / 6
  ffi.Pointer<ggml_tensor> ggml_hardswish(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_hardswish(ctx, a);
  }

  late final _ggml_hardswishPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_hardswish');
  late final _ggml_hardswish = _ggml_hardswishPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  /// hardsigmoid(x) = relu6(x + 3) / 6
  ffi.Pointer<ggml_tensor> ggml_hardsigmoid(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_hardsigmoid(ctx, a);
  }

  late final _ggml_hardsigmoidPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_hardsigmoid');
  late final _ggml_hardsigmoid = _ggml_hardsigmoidPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_exp(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_exp(ctx, a);
  }

  late final _ggml_expPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_exp');
  late final _ggml_exp = _ggml_expPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_exp_inplace(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_exp_inplace(ctx, a);
  }

  late final _ggml_exp_inplacePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_exp_inplace');
  late final _ggml_exp_inplace = _ggml_exp_inplacePtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_floor(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_floor(ctx, a);
  }

  late final _ggml_floorPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_floor');
  late final _ggml_floor = _ggml_floorPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_floor_inplace(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_floor_inplace(ctx, a);
  }

  late final _ggml_floor_inplacePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_floor_inplace');
  late final _ggml_floor_inplace = _ggml_floor_inplacePtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_ceil(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_ceil(ctx, a);
  }

  late final _ggml_ceilPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_ceil');
  late final _ggml_ceil = _ggml_ceilPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_ceil_inplace(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_ceil_inplace(ctx, a);
  }

  late final _ggml_ceil_inplacePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_ceil_inplace');
  late final _ggml_ceil_inplace = _ggml_ceil_inplacePtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_round(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_round(ctx, a);
  }

  late final _ggml_roundPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_round');
  late final _ggml_round = _ggml_roundPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_round_inplace(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_round_inplace(ctx, a);
  }

  late final _ggml_round_inplacePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_round_inplace');
  late final _ggml_round_inplace = _ggml_round_inplacePtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  /// Truncates the fractional part of each element in the tensor (towards zero).
  /// For example: trunc(3.7) = 3.0, trunc(-2.9) = -2.0
  /// Similar to std::trunc in C/C++.
  ffi.Pointer<ggml_tensor> ggml_trunc(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_trunc(ctx, a);
  }

  late final _ggml_truncPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_trunc');
  late final _ggml_trunc = _ggml_truncPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_trunc_inplace(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_trunc_inplace(ctx, a);
  }

  late final _ggml_trunc_inplacePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_trunc_inplace');
  late final _ggml_trunc_inplace = _ggml_trunc_inplacePtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  /// xIELU activation function
  /// x = x * (c_a(alpha_n) + c_b(alpha_p, beta) * sigmoid(beta * x)) + eps * (x > 0)
  /// where c_a = softplus and c_b(a, b) = softplus(a) + b are constraining functions
  /// that constrain the positive and negative source alpha values respectively
  ffi.Pointer<ggml_tensor> ggml_xielu(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    double alpha_n,
    double alpha_p,
    double beta,
    double eps,
  ) {
    return _ggml_xielu(ctx, a, alpha_n, alpha_p, beta, eps);
  }

  late final _ggml_xieluPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Float,
            ffi.Float,
            ffi.Float,
            ffi.Float,
          )
        >
      >('ggml_xielu');
  late final _ggml_xielu = _ggml_xieluPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          double,
          double,
          double,
          double,
        )
      >();

  /// gated linear unit ops
  /// A: n columns, r rows,
  /// result is n / 2 columns, r rows,
  /// expects gate in second half of row, unless swapped is true
  ffi.Pointer<ggml_tensor> ggml_glu(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ggml_glu_op op,
    bool swapped,
  ) {
    return _ggml_glu(ctx, a, op.value, swapped);
  }

  late final _ggml_gluPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.UnsignedInt,
            ffi.Bool,
          )
        >
      >('ggml_glu');
  late final _ggml_glu = _ggml_gluPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          int,
          bool,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_reglu(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_reglu(ctx, a);
  }

  late final _ggml_regluPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_reglu');
  late final _ggml_reglu = _ggml_regluPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_reglu_swapped(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_reglu_swapped(ctx, a);
  }

  late final _ggml_reglu_swappedPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_reglu_swapped');
  late final _ggml_reglu_swapped = _ggml_reglu_swappedPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_geglu(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_geglu(ctx, a);
  }

  late final _ggml_gegluPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_geglu');
  late final _ggml_geglu = _ggml_gegluPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_geglu_swapped(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_geglu_swapped(ctx, a);
  }

  late final _ggml_geglu_swappedPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_geglu_swapped');
  late final _ggml_geglu_swapped = _ggml_geglu_swappedPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_swiglu(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_swiglu(ctx, a);
  }

  late final _ggml_swigluPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_swiglu');
  late final _ggml_swiglu = _ggml_swigluPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_swiglu_swapped(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_swiglu_swapped(ctx, a);
  }

  late final _ggml_swiglu_swappedPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_swiglu_swapped');
  late final _ggml_swiglu_swapped = _ggml_swiglu_swappedPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_geglu_erf(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_geglu_erf(ctx, a);
  }

  late final _ggml_geglu_erfPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_geglu_erf');
  late final _ggml_geglu_erf = _ggml_geglu_erfPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_geglu_erf_swapped(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_geglu_erf_swapped(ctx, a);
  }

  late final _ggml_geglu_erf_swappedPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_geglu_erf_swapped');
  late final _ggml_geglu_erf_swapped = _ggml_geglu_erf_swappedPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_geglu_quick(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_geglu_quick(ctx, a);
  }

  late final _ggml_geglu_quickPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_geglu_quick');
  late final _ggml_geglu_quick = _ggml_geglu_quickPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_geglu_quick_swapped(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_geglu_quick_swapped(ctx, a);
  }

  late final _ggml_geglu_quick_swappedPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_geglu_quick_swapped');
  late final _ggml_geglu_quick_swapped = _ggml_geglu_quick_swappedPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  /// A: n columns, r rows,
  /// B: n columns, r rows,
  ffi.Pointer<ggml_tensor> ggml_glu_split(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
    ggml_glu_op op,
  ) {
    return _ggml_glu_split(ctx, a, b, op.value);
  }

  late final _ggml_glu_splitPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
            ffi.UnsignedInt,
          )
        >
      >('ggml_glu_split');
  late final _ggml_glu_split = _ggml_glu_splitPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          int,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_reglu_split(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
  ) {
    return _ggml_reglu_split(ctx, a, b);
  }

  late final _ggml_reglu_splitPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_reglu_split');
  late final _ggml_reglu_split = _ggml_reglu_splitPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_geglu_split(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
  ) {
    return _ggml_geglu_split(ctx, a, b);
  }

  late final _ggml_geglu_splitPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_geglu_split');
  late final _ggml_geglu_split = _ggml_geglu_splitPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_swiglu_split(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
  ) {
    return _ggml_swiglu_split(ctx, a, b);
  }

  late final _ggml_swiglu_splitPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_swiglu_split');
  late final _ggml_swiglu_split = _ggml_swiglu_splitPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_geglu_erf_split(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
  ) {
    return _ggml_geglu_erf_split(ctx, a, b);
  }

  late final _ggml_geglu_erf_splitPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_geglu_erf_split');
  late final _ggml_geglu_erf_split = _ggml_geglu_erf_splitPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_geglu_quick_split(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
  ) {
    return _ggml_geglu_quick_split(ctx, a, b);
  }

  late final _ggml_geglu_quick_splitPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_geglu_quick_split');
  late final _ggml_geglu_quick_split = _ggml_geglu_quick_splitPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_swiglu_oai(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
    double alpha,
    double limit,
  ) {
    return _ggml_swiglu_oai(ctx, a, b, alpha, limit);
  }

  late final _ggml_swiglu_oaiPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
            ffi.Float,
            ffi.Float,
          )
        >
      >('ggml_swiglu_oai');
  late final _ggml_swiglu_oai = _ggml_swiglu_oaiPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          double,
          double,
        )
      >();

  /// normalize along rows
  ffi.Pointer<ggml_tensor> ggml_norm(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    double eps,
  ) {
    return _ggml_norm(ctx, a, eps);
  }

  late final _ggml_normPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Float,
          )
        >
      >('ggml_norm');
  late final _ggml_norm = _ggml_normPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          double,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_norm_inplace(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    double eps,
  ) {
    return _ggml_norm_inplace(ctx, a, eps);
  }

  late final _ggml_norm_inplacePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Float,
          )
        >
      >('ggml_norm_inplace');
  late final _ggml_norm_inplace = _ggml_norm_inplacePtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          double,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_rms_norm(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    double eps,
  ) {
    return _ggml_rms_norm(ctx, a, eps);
  }

  late final _ggml_rms_normPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Float,
          )
        >
      >('ggml_rms_norm');
  late final _ggml_rms_norm = _ggml_rms_normPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          double,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_rms_norm_inplace(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    double eps,
  ) {
    return _ggml_rms_norm_inplace(ctx, a, eps);
  }

  late final _ggml_rms_norm_inplacePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Float,
          )
        >
      >('ggml_rms_norm_inplace');
  late final _ggml_rms_norm_inplace = _ggml_rms_norm_inplacePtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          double,
        )
      >();

  /// group normalize along ne0*ne1*n_groups
  /// used in stable-diffusion
  ffi.Pointer<ggml_tensor> ggml_group_norm(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    int n_groups,
    double eps,
  ) {
    return _ggml_group_norm(ctx, a, n_groups, eps);
  }

  late final _ggml_group_normPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Int,
            ffi.Float,
          )
        >
      >('ggml_group_norm');
  late final _ggml_group_norm = _ggml_group_normPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          int,
          double,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_group_norm_inplace(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    int n_groups,
    double eps,
  ) {
    return _ggml_group_norm_inplace(ctx, a, n_groups, eps);
  }

  late final _ggml_group_norm_inplacePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Int,
            ffi.Float,
          )
        >
      >('ggml_group_norm_inplace');
  late final _ggml_group_norm_inplace = _ggml_group_norm_inplacePtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          int,
          double,
        )
      >();

  /// l2 normalize along rows
  /// used in rwkv v7
  ffi.Pointer<ggml_tensor> ggml_l2_norm(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    double eps,
  ) {
    return _ggml_l2_norm(ctx, a, eps);
  }

  late final _ggml_l2_normPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Float,
          )
        >
      >('ggml_l2_norm');
  late final _ggml_l2_norm = _ggml_l2_normPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          double,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_l2_norm_inplace(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    double eps,
  ) {
    return _ggml_l2_norm_inplace(ctx, a, eps);
  }

  late final _ggml_l2_norm_inplacePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Float,
          )
        >
      >('ggml_l2_norm_inplace');
  late final _ggml_l2_norm_inplace = _ggml_l2_norm_inplacePtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          double,
        )
      >();

  /// a - x
  /// b - dy
  ffi.Pointer<ggml_tensor> ggml_rms_norm_back(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
    double eps,
  ) {
    return _ggml_rms_norm_back(ctx, a, b, eps);
  }

  late final _ggml_rms_norm_backPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
            ffi.Float,
          )
        >
      >('ggml_rms_norm_back');
  late final _ggml_rms_norm_back = _ggml_rms_norm_backPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          double,
        )
      >();

  /// A: k columns, n rows => [ne03, ne02, n, k]
  /// B: k columns, m rows  (i.e. we transpose it internally) => [ne03 * x, ne02 * y, m, k]
  /// result is n columns, m rows => [ne03 * x, ne02 * y, m, n]
  ffi.Pointer<ggml_tensor> ggml_mul_mat(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
  ) {
    return _ggml_mul_mat(ctx, a, b);
  }

  late final _ggml_mul_matPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_mul_mat');
  late final _ggml_mul_mat = _ggml_mul_matPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  /// change the precision of a matrix multiplication
  /// set to GGML_PREC_F32 for higher precision (useful for phi-2)
  void ggml_mul_mat_set_prec(ffi.Pointer<ggml_tensor> a, ggml_prec prec) {
    return _ggml_mul_mat_set_prec(a, prec.value);
  }

  late final _ggml_mul_mat_set_precPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<ggml_tensor>, ffi.UnsignedInt)
        >
      >('ggml_mul_mat_set_prec');
  late final _ggml_mul_mat_set_prec = _ggml_mul_mat_set_precPtr
      .asFunction<void Function(ffi.Pointer<ggml_tensor>, int)>();

  /// indirect matrix multiplication
  ffi.Pointer<ggml_tensor> ggml_mul_mat_id(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> as,
    ffi.Pointer<ggml_tensor> b,
    ffi.Pointer<ggml_tensor> ids,
  ) {
    return _ggml_mul_mat_id(ctx, as, b, ids);
  }

  late final _ggml_mul_mat_idPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_mul_mat_id');
  late final _ggml_mul_mat_id = _ggml_mul_mat_idPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  /// A: m columns, n rows,
  /// B: p columns, n rows,
  /// result is m columns, p rows
  ffi.Pointer<ggml_tensor> ggml_out_prod(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
  ) {
    return _ggml_out_prod(ctx, a, b);
  }

  late final _ggml_out_prodPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_out_prod');
  late final _ggml_out_prod = _ggml_out_prodPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  /// operations on tensors without backpropagation
  ffi.Pointer<ggml_tensor> ggml_scale(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    double s,
  ) {
    return _ggml_scale(ctx, a, s);
  }

  late final _ggml_scalePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Float,
          )
        >
      >('ggml_scale');
  late final _ggml_scale = _ggml_scalePtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          double,
        )
      >();

  /// in-place, returns view(a)
  ffi.Pointer<ggml_tensor> ggml_scale_inplace(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    double s,
  ) {
    return _ggml_scale_inplace(ctx, a, s);
  }

  late final _ggml_scale_inplacePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Float,
          )
        >
      >('ggml_scale_inplace');
  late final _ggml_scale_inplace = _ggml_scale_inplacePtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          double,
        )
      >();

  /// x = s * a + b
  ffi.Pointer<ggml_tensor> ggml_scale_bias(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    double s,
    double b,
  ) {
    return _ggml_scale_bias(ctx, a, s, b);
  }

  late final _ggml_scale_biasPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Float,
            ffi.Float,
          )
        >
      >('ggml_scale_bias');
  late final _ggml_scale_bias = _ggml_scale_biasPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          double,
          double,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_scale_bias_inplace(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    double s,
    double b,
  ) {
    return _ggml_scale_bias_inplace(ctx, a, s, b);
  }

  late final _ggml_scale_bias_inplacePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Float,
            ffi.Float,
          )
        >
      >('ggml_scale_bias_inplace');
  late final _ggml_scale_bias_inplace = _ggml_scale_bias_inplacePtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          double,
          double,
        )
      >();

  /// b -> view(a,offset,nb1,nb2,3), return modified a
  ffi.Pointer<ggml_tensor> ggml_set(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
    int nb1,
    int nb2,
    int nb3,
    int offset,
  ) {
    return _ggml_set(ctx, a, b, nb1, nb2, nb3, offset);
  }

  late final _ggml_setPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
            ffi.Size,
            ffi.Size,
            ffi.Size,
            ffi.Size,
          )
        >
      >('ggml_set');
  late final _ggml_set = _ggml_setPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          int,
          int,
          int,
          int,
        )
      >();

  /// b -> view(a,offset,nb1,nb2,3), return view(a)
  ffi.Pointer<ggml_tensor> ggml_set_inplace(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
    int nb1,
    int nb2,
    int nb3,
    int offset,
  ) {
    return _ggml_set_inplace(ctx, a, b, nb1, nb2, nb3, offset);
  }

  late final _ggml_set_inplacePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
            ffi.Size,
            ffi.Size,
            ffi.Size,
            ffi.Size,
          )
        >
      >('ggml_set_inplace');
  late final _ggml_set_inplace = _ggml_set_inplacePtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          int,
          int,
          int,
          int,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_set_1d(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
    int offset,
  ) {
    return _ggml_set_1d(ctx, a, b, offset);
  }

  late final _ggml_set_1dPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
            ffi.Size,
          )
        >
      >('ggml_set_1d');
  late final _ggml_set_1d = _ggml_set_1dPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          int,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_set_1d_inplace(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
    int offset,
  ) {
    return _ggml_set_1d_inplace(ctx, a, b, offset);
  }

  late final _ggml_set_1d_inplacePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
            ffi.Size,
          )
        >
      >('ggml_set_1d_inplace');
  late final _ggml_set_1d_inplace = _ggml_set_1d_inplacePtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          int,
        )
      >();

  /// b -> view(a,offset,nb1,nb2,3), return modified a
  ffi.Pointer<ggml_tensor> ggml_set_2d(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
    int nb1,
    int offset,
  ) {
    return _ggml_set_2d(ctx, a, b, nb1, offset);
  }

  late final _ggml_set_2dPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
            ffi.Size,
            ffi.Size,
          )
        >
      >('ggml_set_2d');
  late final _ggml_set_2d = _ggml_set_2dPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          int,
          int,
        )
      >();

  /// b -> view(a,offset,nb1,nb2,3), return view(a)
  ffi.Pointer<ggml_tensor> ggml_set_2d_inplace(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
    int nb1,
    int offset,
  ) {
    return _ggml_set_2d_inplace(ctx, a, b, nb1, offset);
  }

  late final _ggml_set_2d_inplacePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
            ffi.Size,
            ffi.Size,
          )
        >
      >('ggml_set_2d_inplace');
  late final _ggml_set_2d_inplace = _ggml_set_2d_inplacePtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          int,
          int,
        )
      >();

  /// a -> b, return view(b)
  ffi.Pointer<ggml_tensor> ggml_cpy(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
  ) {
    return _ggml_cpy(ctx, a, b);
  }

  late final _ggml_cpyPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_cpy');
  late final _ggml_cpy = _ggml_cpyPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  /// note: casting from f32 to i32 will discard the fractional part
  ffi.Pointer<ggml_tensor> ggml_cast(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ggml_type type,
  ) {
    return _ggml_cast(ctx, a, type.value);
  }

  late final _ggml_castPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.UnsignedInt,
          )
        >
      >('ggml_cast');
  late final _ggml_cast = _ggml_castPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          int,
        )
      >();

  /// make contiguous
  ffi.Pointer<ggml_tensor> ggml_cont(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_cont(ctx, a);
  }

  late final _ggml_contPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_cont');
  late final _ggml_cont = _ggml_contPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  /// make contiguous, with new shape
  ffi.Pointer<ggml_tensor> ggml_cont_1d(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    int ne0,
  ) {
    return _ggml_cont_1d(ctx, a, ne0);
  }

  late final _ggml_cont_1dPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Int64,
          )
        >
      >('ggml_cont_1d');
  late final _ggml_cont_1d = _ggml_cont_1dPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          int,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_cont_2d(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    int ne0,
    int ne1,
  ) {
    return _ggml_cont_2d(ctx, a, ne0, ne1);
  }

  late final _ggml_cont_2dPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Int64,
            ffi.Int64,
          )
        >
      >('ggml_cont_2d');
  late final _ggml_cont_2d = _ggml_cont_2dPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          int,
          int,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_cont_3d(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    int ne0,
    int ne1,
    int ne2,
  ) {
    return _ggml_cont_3d(ctx, a, ne0, ne1, ne2);
  }

  late final _ggml_cont_3dPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Int64,
            ffi.Int64,
            ffi.Int64,
          )
        >
      >('ggml_cont_3d');
  late final _ggml_cont_3d = _ggml_cont_3dPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          int,
          int,
          int,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_cont_4d(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    int ne0,
    int ne1,
    int ne2,
    int ne3,
  ) {
    return _ggml_cont_4d(ctx, a, ne0, ne1, ne2, ne3);
  }

  late final _ggml_cont_4dPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Int64,
            ffi.Int64,
            ffi.Int64,
            ffi.Int64,
          )
        >
      >('ggml_cont_4d');
  late final _ggml_cont_4d = _ggml_cont_4dPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          int,
          int,
          int,
          int,
        )
      >();

  /// return view(a), b specifies the new shape
  /// TODO: when we start computing gradient, make a copy instead of view
  ffi.Pointer<ggml_tensor> ggml_reshape(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
  ) {
    return _ggml_reshape(ctx, a, b);
  }

  late final _ggml_reshapePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_reshape');
  late final _ggml_reshape = _ggml_reshapePtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  /// return view(a)
  /// TODO: when we start computing gradient, make a copy instead of view
  ffi.Pointer<ggml_tensor> ggml_reshape_1d(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    int ne0,
  ) {
    return _ggml_reshape_1d(ctx, a, ne0);
  }

  late final _ggml_reshape_1dPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Int64,
          )
        >
      >('ggml_reshape_1d');
  late final _ggml_reshape_1d = _ggml_reshape_1dPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          int,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_reshape_2d(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    int ne0,
    int ne1,
  ) {
    return _ggml_reshape_2d(ctx, a, ne0, ne1);
  }

  late final _ggml_reshape_2dPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Int64,
            ffi.Int64,
          )
        >
      >('ggml_reshape_2d');
  late final _ggml_reshape_2d = _ggml_reshape_2dPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          int,
          int,
        )
      >();

  /// return view(a)
  /// TODO: when we start computing gradient, make a copy instead of view
  ffi.Pointer<ggml_tensor> ggml_reshape_3d(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    int ne0,
    int ne1,
    int ne2,
  ) {
    return _ggml_reshape_3d(ctx, a, ne0, ne1, ne2);
  }

  late final _ggml_reshape_3dPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Int64,
            ffi.Int64,
            ffi.Int64,
          )
        >
      >('ggml_reshape_3d');
  late final _ggml_reshape_3d = _ggml_reshape_3dPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          int,
          int,
          int,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_reshape_4d(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    int ne0,
    int ne1,
    int ne2,
    int ne3,
  ) {
    return _ggml_reshape_4d(ctx, a, ne0, ne1, ne2, ne3);
  }

  late final _ggml_reshape_4dPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Int64,
            ffi.Int64,
            ffi.Int64,
            ffi.Int64,
          )
        >
      >('ggml_reshape_4d');
  late final _ggml_reshape_4d = _ggml_reshape_4dPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          int,
          int,
          int,
          int,
        )
      >();

  /// offset in bytes
  ffi.Pointer<ggml_tensor> ggml_view_1d(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    int ne0,
    int offset,
  ) {
    return _ggml_view_1d(ctx, a, ne0, offset);
  }

  late final _ggml_view_1dPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Int64,
            ffi.Size,
          )
        >
      >('ggml_view_1d');
  late final _ggml_view_1d = _ggml_view_1dPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          int,
          int,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_view_2d(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    int ne0,
    int ne1,
    int nb1,
    int offset,
  ) {
    return _ggml_view_2d(ctx, a, ne0, ne1, nb1, offset);
  }

  late final _ggml_view_2dPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Int64,
            ffi.Int64,
            ffi.Size,
            ffi.Size,
          )
        >
      >('ggml_view_2d');
  late final _ggml_view_2d = _ggml_view_2dPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          int,
          int,
          int,
          int,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_view_3d(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    int ne0,
    int ne1,
    int ne2,
    int nb1,
    int nb2,
    int offset,
  ) {
    return _ggml_view_3d(ctx, a, ne0, ne1, ne2, nb1, nb2, offset);
  }

  late final _ggml_view_3dPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Int64,
            ffi.Int64,
            ffi.Int64,
            ffi.Size,
            ffi.Size,
            ffi.Size,
          )
        >
      >('ggml_view_3d');
  late final _ggml_view_3d = _ggml_view_3dPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          int,
          int,
          int,
          int,
          int,
          int,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_view_4d(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    int ne0,
    int ne1,
    int ne2,
    int ne3,
    int nb1,
    int nb2,
    int nb3,
    int offset,
  ) {
    return _ggml_view_4d(ctx, a, ne0, ne1, ne2, ne3, nb1, nb2, nb3, offset);
  }

  late final _ggml_view_4dPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Int64,
            ffi.Int64,
            ffi.Int64,
            ffi.Int64,
            ffi.Size,
            ffi.Size,
            ffi.Size,
            ffi.Size,
          )
        >
      >('ggml_view_4d');
  late final _ggml_view_4d = _ggml_view_4dPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          int,
          int,
          int,
          int,
          int,
          int,
          int,
          int,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_permute(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    int axis0,
    int axis1,
    int axis2,
    int axis3,
  ) {
    return _ggml_permute(ctx, a, axis0, axis1, axis2, axis3);
  }

  late final _ggml_permutePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Int,
            ffi.Int,
            ffi.Int,
            ffi.Int,
          )
        >
      >('ggml_permute');
  late final _ggml_permute = _ggml_permutePtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          int,
          int,
          int,
          int,
        )
      >();

  /// alias for ggml_permute(ctx, a, 1, 0, 2, 3)
  ffi.Pointer<ggml_tensor> ggml_transpose(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_transpose(ctx, a);
  }

  late final _ggml_transposePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_transpose');
  late final _ggml_transpose = _ggml_transposePtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  /// supports 4D a:
  /// a     [n_embd, ne1, ne2, ne3]
  /// b I32 [n_rows, ne2, ne3, 1]
  ///
  /// return [n_embd, n_rows, ne2, ne3]
  ffi.Pointer<ggml_tensor> ggml_get_rows(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
  ) {
    return _ggml_get_rows(ctx, a, b);
  }

  late final _ggml_get_rowsPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_get_rows');
  late final _ggml_get_rows = _ggml_get_rowsPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_get_rows_back(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
    ffi.Pointer<ggml_tensor> c,
  ) {
    return _ggml_get_rows_back(ctx, a, b, c);
  }

  late final _ggml_get_rows_backPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_get_rows_back');
  late final _ggml_get_rows_back = _ggml_get_rows_backPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  /// a TD  [n_embd, ne1,    ne2,    ne3]
  /// b TS  [n_embd, n_rows, ne02,   ne03] | ne02 == ne2, ne03 == ne3
  /// c I64 [n_rows, ne11,   ne12,   1]    | c[i] in [0, ne1)
  ///
  /// undefined behavior if destination rows overlap
  ///
  /// broadcast:
  /// ne2 % ne11 == 0
  /// ne3 % ne12 == 0
  ///
  /// return view(a)
  ffi.Pointer<ggml_tensor> ggml_set_rows(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
    ffi.Pointer<ggml_tensor> c,
  ) {
    return _ggml_set_rows(ctx, a, b, c);
  }

  late final _ggml_set_rowsPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_set_rows');
  late final _ggml_set_rows = _ggml_set_rowsPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_diag(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_diag(ctx, a);
  }

  late final _ggml_diagPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_diag');
  late final _ggml_diag = _ggml_diagPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  /// set elements above the diagonal to -INF
  ffi.Pointer<ggml_tensor> ggml_diag_mask_inf(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    int n_past,
  ) {
    return _ggml_diag_mask_inf(ctx, a, n_past);
  }

  late final _ggml_diag_mask_infPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Int,
          )
        >
      >('ggml_diag_mask_inf');
  late final _ggml_diag_mask_inf = _ggml_diag_mask_infPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          int,
        )
      >();

  /// in-place, returns view(a)
  ffi.Pointer<ggml_tensor> ggml_diag_mask_inf_inplace(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    int n_past,
  ) {
    return _ggml_diag_mask_inf_inplace(ctx, a, n_past);
  }

  late final _ggml_diag_mask_inf_inplacePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Int,
          )
        >
      >('ggml_diag_mask_inf_inplace');
  late final _ggml_diag_mask_inf_inplace = _ggml_diag_mask_inf_inplacePtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          int,
        )
      >();

  /// set elements above the diagonal to 0
  ffi.Pointer<ggml_tensor> ggml_diag_mask_zero(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    int n_past,
  ) {
    return _ggml_diag_mask_zero(ctx, a, n_past);
  }

  late final _ggml_diag_mask_zeroPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Int,
          )
        >
      >('ggml_diag_mask_zero');
  late final _ggml_diag_mask_zero = _ggml_diag_mask_zeroPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          int,
        )
      >();

  /// in-place, returns view(a)
  ffi.Pointer<ggml_tensor> ggml_diag_mask_zero_inplace(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    int n_past,
  ) {
    return _ggml_diag_mask_zero_inplace(ctx, a, n_past);
  }

  late final _ggml_diag_mask_zero_inplacePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Int,
          )
        >
      >('ggml_diag_mask_zero_inplace');
  late final _ggml_diag_mask_zero_inplace = _ggml_diag_mask_zero_inplacePtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          int,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_soft_max(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_soft_max(ctx, a);
  }

  late final _ggml_soft_maxPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_soft_max');
  late final _ggml_soft_max = _ggml_soft_maxPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  /// in-place, returns view(a)
  ffi.Pointer<ggml_tensor> ggml_soft_max_inplace(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_soft_max_inplace(ctx, a);
  }

  late final _ggml_soft_max_inplacePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_soft_max_inplace');
  late final _ggml_soft_max_inplace = _ggml_soft_max_inplacePtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  /// a    [ne0, ne01, ne02, ne03]
  /// mask [ne0, ne11, ne12, ne13] | ne11 >= ne01, F16 or F32, optional
  ///
  /// broadcast:
  /// ne02 % ne12 == 0
  /// ne03 % ne13 == 0
  ///
  /// fused soft_max(a*scale + mask*(ALiBi slope))
  /// max_bias = 0.0f for no ALiBi
  ffi.Pointer<ggml_tensor> ggml_soft_max_ext(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> mask,
    double scale,
    double max_bias,
  ) {
    return _ggml_soft_max_ext(ctx, a, mask, scale, max_bias);
  }

  late final _ggml_soft_max_extPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
            ffi.Float,
            ffi.Float,
          )
        >
      >('ggml_soft_max_ext');
  late final _ggml_soft_max_ext = _ggml_soft_max_extPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          double,
          double,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_soft_max_ext_inplace(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> mask,
    double scale,
    double max_bias,
  ) {
    return _ggml_soft_max_ext_inplace(ctx, a, mask, scale, max_bias);
  }

  late final _ggml_soft_max_ext_inplacePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
            ffi.Float,
            ffi.Float,
          )
        >
      >('ggml_soft_max_ext_inplace');
  late final _ggml_soft_max_ext_inplace = _ggml_soft_max_ext_inplacePtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          double,
          double,
        )
      >();

  void ggml_soft_max_add_sinks(
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> sinks,
  ) {
    return _ggml_soft_max_add_sinks(a, sinks);
  }

  late final _ggml_soft_max_add_sinksPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<ggml_tensor>, ffi.Pointer<ggml_tensor>)
        >
      >('ggml_soft_max_add_sinks');
  late final _ggml_soft_max_add_sinks = _ggml_soft_max_add_sinksPtr
      .asFunction<
        void Function(ffi.Pointer<ggml_tensor>, ffi.Pointer<ggml_tensor>)
      >();

  ffi.Pointer<ggml_tensor> ggml_soft_max_ext_back(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
    double scale,
    double max_bias,
  ) {
    return _ggml_soft_max_ext_back(ctx, a, b, scale, max_bias);
  }

  late final _ggml_soft_max_ext_backPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
            ffi.Float,
            ffi.Float,
          )
        >
      >('ggml_soft_max_ext_back');
  late final _ggml_soft_max_ext_back = _ggml_soft_max_ext_backPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          double,
          double,
        )
      >();

  /// in-place, returns view(a)
  ffi.Pointer<ggml_tensor> ggml_soft_max_ext_back_inplace(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
    double scale,
    double max_bias,
  ) {
    return _ggml_soft_max_ext_back_inplace(ctx, a, b, scale, max_bias);
  }

  late final _ggml_soft_max_ext_back_inplacePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
            ffi.Float,
            ffi.Float,
          )
        >
      >('ggml_soft_max_ext_back_inplace');
  late final _ggml_soft_max_ext_back_inplace =
      _ggml_soft_max_ext_back_inplacePtr
          .asFunction<
            ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ggml_tensor>,
              double,
              double,
            )
          >();

  /// rotary position embedding
  /// if (mode & 1) - skip n_past elements (NOT SUPPORTED)
  /// if (mode & GGML_ROPE_TYPE_NEOX) - GPT-NeoX style
  ///
  /// b is an int32 vector with size a->ne[2], it contains the positions
  ffi.Pointer<ggml_tensor> ggml_rope(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
    int n_dims,
    int mode,
  ) {
    return _ggml_rope(ctx, a, b, n_dims, mode);
  }

  late final _ggml_ropePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
            ffi.Int,
            ffi.Int,
          )
        >
      >('ggml_rope');
  late final _ggml_rope = _ggml_ropePtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          int,
          int,
        )
      >();

  /// in-place, returns view(a)
  ffi.Pointer<ggml_tensor> ggml_rope_inplace(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
    int n_dims,
    int mode,
  ) {
    return _ggml_rope_inplace(ctx, a, b, n_dims, mode);
  }

  late final _ggml_rope_inplacePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
            ffi.Int,
            ffi.Int,
          )
        >
      >('ggml_rope_inplace');
  late final _ggml_rope_inplace = _ggml_rope_inplacePtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          int,
          int,
        )
      >();

  /// custom RoPE
  /// c is freq factors (e.g. phi3-128k), (optional)
  ffi.Pointer<ggml_tensor> ggml_rope_ext(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
    ffi.Pointer<ggml_tensor> c,
    int n_dims,
    int mode,
    int n_ctx_orig,
    double freq_base,
    double freq_scale,
    double ext_factor,
    double attn_factor,
    double beta_fast,
    double beta_slow,
  ) {
    return _ggml_rope_ext(
      ctx,
      a,
      b,
      c,
      n_dims,
      mode,
      n_ctx_orig,
      freq_base,
      freq_scale,
      ext_factor,
      attn_factor,
      beta_fast,
      beta_slow,
    );
  }

  late final _ggml_rope_extPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
            ffi.Int,
            ffi.Int,
            ffi.Int,
            ffi.Float,
            ffi.Float,
            ffi.Float,
            ffi.Float,
            ffi.Float,
            ffi.Float,
          )
        >
      >('ggml_rope_ext');
  late final _ggml_rope_ext = _ggml_rope_extPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          int,
          int,
          int,
          double,
          double,
          double,
          double,
          double,
          double,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_rope_multi(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
    ffi.Pointer<ggml_tensor> c,
    int n_dims,
    ffi.Pointer<ffi.Int> sections,
    int mode,
    int n_ctx_orig,
    double freq_base,
    double freq_scale,
    double ext_factor,
    double attn_factor,
    double beta_fast,
    double beta_slow,
  ) {
    return _ggml_rope_multi(
      ctx,
      a,
      b,
      c,
      n_dims,
      sections,
      mode,
      n_ctx_orig,
      freq_base,
      freq_scale,
      ext_factor,
      attn_factor,
      beta_fast,
      beta_slow,
    );
  }

  late final _ggml_rope_multiPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
            ffi.Int,
            ffi.Pointer<ffi.Int>,
            ffi.Int,
            ffi.Int,
            ffi.Float,
            ffi.Float,
            ffi.Float,
            ffi.Float,
            ffi.Float,
            ffi.Float,
          )
        >
      >('ggml_rope_multi');
  late final _ggml_rope_multi = _ggml_rope_multiPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          int,
          ffi.Pointer<ffi.Int>,
          int,
          int,
          double,
          double,
          double,
          double,
          double,
          double,
        )
      >();

  /// in-place, returns view(a)
  ffi.Pointer<ggml_tensor> ggml_rope_ext_inplace(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
    ffi.Pointer<ggml_tensor> c,
    int n_dims,
    int mode,
    int n_ctx_orig,
    double freq_base,
    double freq_scale,
    double ext_factor,
    double attn_factor,
    double beta_fast,
    double beta_slow,
  ) {
    return _ggml_rope_ext_inplace(
      ctx,
      a,
      b,
      c,
      n_dims,
      mode,
      n_ctx_orig,
      freq_base,
      freq_scale,
      ext_factor,
      attn_factor,
      beta_fast,
      beta_slow,
    );
  }

  late final _ggml_rope_ext_inplacePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
            ffi.Int,
            ffi.Int,
            ffi.Int,
            ffi.Float,
            ffi.Float,
            ffi.Float,
            ffi.Float,
            ffi.Float,
            ffi.Float,
          )
        >
      >('ggml_rope_ext_inplace');
  late final _ggml_rope_ext_inplace = _ggml_rope_ext_inplacePtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          int,
          int,
          int,
          double,
          double,
          double,
          double,
          double,
          double,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_rope_multi_inplace(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
    ffi.Pointer<ggml_tensor> c,
    int n_dims,
    ffi.Pointer<ffi.Int> sections,
    int mode,
    int n_ctx_orig,
    double freq_base,
    double freq_scale,
    double ext_factor,
    double attn_factor,
    double beta_fast,
    double beta_slow,
  ) {
    return _ggml_rope_multi_inplace(
      ctx,
      a,
      b,
      c,
      n_dims,
      sections,
      mode,
      n_ctx_orig,
      freq_base,
      freq_scale,
      ext_factor,
      attn_factor,
      beta_fast,
      beta_slow,
    );
  }

  late final _ggml_rope_multi_inplacePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
            ffi.Int,
            ffi.Pointer<ffi.Int>,
            ffi.Int,
            ffi.Int,
            ffi.Float,
            ffi.Float,
            ffi.Float,
            ffi.Float,
            ffi.Float,
            ffi.Float,
          )
        >
      >('ggml_rope_multi_inplace');
  late final _ggml_rope_multi_inplace = _ggml_rope_multi_inplacePtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          int,
          ffi.Pointer<ffi.Int>,
          int,
          int,
          double,
          double,
          double,
          double,
          double,
          double,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_rope_custom(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
    int n_dims,
    int mode,
    int n_ctx_orig,
    double freq_base,
    double freq_scale,
    double ext_factor,
    double attn_factor,
    double beta_fast,
    double beta_slow,
  ) {
    return _ggml_rope_custom(
      ctx,
      a,
      b,
      n_dims,
      mode,
      n_ctx_orig,
      freq_base,
      freq_scale,
      ext_factor,
      attn_factor,
      beta_fast,
      beta_slow,
    );
  }

  late final _ggml_rope_customPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
            ffi.Int,
            ffi.Int,
            ffi.Int,
            ffi.Float,
            ffi.Float,
            ffi.Float,
            ffi.Float,
            ffi.Float,
            ffi.Float,
          )
        >
      >('ggml_rope_custom');
  late final _ggml_rope_custom = _ggml_rope_customPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          int,
          int,
          int,
          double,
          double,
          double,
          double,
          double,
          double,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_rope_custom_inplace(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
    int n_dims,
    int mode,
    int n_ctx_orig,
    double freq_base,
    double freq_scale,
    double ext_factor,
    double attn_factor,
    double beta_fast,
    double beta_slow,
  ) {
    return _ggml_rope_custom_inplace(
      ctx,
      a,
      b,
      n_dims,
      mode,
      n_ctx_orig,
      freq_base,
      freq_scale,
      ext_factor,
      attn_factor,
      beta_fast,
      beta_slow,
    );
  }

  late final _ggml_rope_custom_inplacePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
            ffi.Int,
            ffi.Int,
            ffi.Int,
            ffi.Float,
            ffi.Float,
            ffi.Float,
            ffi.Float,
            ffi.Float,
            ffi.Float,
          )
        >
      >('ggml_rope_custom_inplace');
  late final _ggml_rope_custom_inplace = _ggml_rope_custom_inplacePtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          int,
          int,
          int,
          double,
          double,
          double,
          double,
          double,
          double,
        )
      >();

  /// compute correction dims for YaRN RoPE scaling
  void ggml_rope_yarn_corr_dims(
    int n_dims,
    int n_ctx_orig,
    double freq_base,
    double beta_fast,
    double beta_slow,
    ffi.Pointer<ffi.Float> dims,
  ) {
    return _ggml_rope_yarn_corr_dims(
      n_dims,
      n_ctx_orig,
      freq_base,
      beta_fast,
      beta_slow,
      dims,
    );
  }

  late final _ggml_rope_yarn_corr_dimsPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Void Function(
            ffi.Int,
            ffi.Int,
            ffi.Float,
            ffi.Float,
            ffi.Float,
            ffi.Pointer<ffi.Float>,
          )
        >
      >('ggml_rope_yarn_corr_dims');
  late final _ggml_rope_yarn_corr_dims = _ggml_rope_yarn_corr_dimsPtr
      .asFunction<
        void Function(int, int, double, double, double, ffi.Pointer<ffi.Float>)
      >();

  /// rotary position embedding backward, i.e compute dx from dy
  /// a - dy
  ffi.Pointer<ggml_tensor> ggml_rope_ext_back(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
    ffi.Pointer<ggml_tensor> c,
    int n_dims,
    int mode,
    int n_ctx_orig,
    double freq_base,
    double freq_scale,
    double ext_factor,
    double attn_factor,
    double beta_fast,
    double beta_slow,
  ) {
    return _ggml_rope_ext_back(
      ctx,
      a,
      b,
      c,
      n_dims,
      mode,
      n_ctx_orig,
      freq_base,
      freq_scale,
      ext_factor,
      attn_factor,
      beta_fast,
      beta_slow,
    );
  }

  late final _ggml_rope_ext_backPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
            ffi.Int,
            ffi.Int,
            ffi.Int,
            ffi.Float,
            ffi.Float,
            ffi.Float,
            ffi.Float,
            ffi.Float,
            ffi.Float,
          )
        >
      >('ggml_rope_ext_back');
  late final _ggml_rope_ext_back = _ggml_rope_ext_backPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          int,
          int,
          int,
          double,
          double,
          double,
          double,
          double,
          double,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_rope_multi_back(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
    ffi.Pointer<ggml_tensor> c,
    int n_dims,
    ffi.Pointer<ffi.Int> sections,
    int mode,
    int n_ctx_orig,
    double freq_base,
    double freq_scale,
    double ext_factor,
    double attn_factor,
    double beta_fast,
    double beta_slow,
  ) {
    return _ggml_rope_multi_back(
      ctx,
      a,
      b,
      c,
      n_dims,
      sections,
      mode,
      n_ctx_orig,
      freq_base,
      freq_scale,
      ext_factor,
      attn_factor,
      beta_fast,
      beta_slow,
    );
  }

  late final _ggml_rope_multi_backPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
            ffi.Int,
            ffi.Pointer<ffi.Int>,
            ffi.Int,
            ffi.Int,
            ffi.Float,
            ffi.Float,
            ffi.Float,
            ffi.Float,
            ffi.Float,
            ffi.Float,
          )
        >
      >('ggml_rope_multi_back');
  late final _ggml_rope_multi_back = _ggml_rope_multi_backPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          int,
          ffi.Pointer<ffi.Int>,
          int,
          int,
          double,
          double,
          double,
          double,
          double,
          double,
        )
      >();

  /// clamp
  /// in-place, returns view(a)
  ffi.Pointer<ggml_tensor> ggml_clamp(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    double min,
    double max,
  ) {
    return _ggml_clamp(ctx, a, min, max);
  }

  late final _ggml_clampPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Float,
            ffi.Float,
          )
        >
      >('ggml_clamp');
  late final _ggml_clamp = _ggml_clampPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          double,
          double,
        )
      >();

  /// im2col
  /// converts data into a format that effectively results in a convolution when combined with matrix multiplication
  ffi.Pointer<ggml_tensor> ggml_im2col(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
    int s0,
    int s1,
    int p0,
    int p1,
    int d0,
    int d1,
    bool is_2D,
    ggml_type dst_type,
  ) {
    return _ggml_im2col(
      ctx,
      a,
      b,
      s0,
      s1,
      p0,
      p1,
      d0,
      d1,
      is_2D,
      dst_type.value,
    );
  }

  late final _ggml_im2colPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
            ffi.Int,
            ffi.Int,
            ffi.Int,
            ffi.Int,
            ffi.Int,
            ffi.Int,
            ffi.Bool,
            ffi.UnsignedInt,
          )
        >
      >('ggml_im2col');
  late final _ggml_im2col = _ggml_im2colPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          int,
          int,
          int,
          int,
          int,
          int,
          bool,
          int,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_im2col_back(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
    ffi.Pointer<ffi.Int64> ne,
    int s0,
    int s1,
    int p0,
    int p1,
    int d0,
    int d1,
    bool is_2D,
  ) {
    return _ggml_im2col_back(ctx, a, b, ne, s0, s1, p0, p1, d0, d1, is_2D);
  }

  late final _ggml_im2col_backPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ffi.Int64>,
            ffi.Int,
            ffi.Int,
            ffi.Int,
            ffi.Int,
            ffi.Int,
            ffi.Int,
            ffi.Bool,
          )
        >
      >('ggml_im2col_back');
  late final _ggml_im2col_back = _ggml_im2col_backPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ffi.Int64>,
          int,
          int,
          int,
          int,
          int,
          int,
          bool,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_conv_1d(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
    int s0,
    int p0,
    int d0,
  ) {
    return _ggml_conv_1d(ctx, a, b, s0, p0, d0);
  }

  late final _ggml_conv_1dPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
            ffi.Int,
            ffi.Int,
            ffi.Int,
          )
        >
      >('ggml_conv_1d');
  late final _ggml_conv_1d = _ggml_conv_1dPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          int,
          int,
          int,
        )
      >();

  /// conv_1d with padding = half
  /// alias for ggml_conv_1d(a, b, s, a->ne[0]/2, d)
  ffi.Pointer<ggml_tensor> ggml_conv_1d_ph(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
    int s,
    int d,
  ) {
    return _ggml_conv_1d_ph(ctx, a, b, s, d);
  }

  late final _ggml_conv_1d_phPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
            ffi.Int,
            ffi.Int,
          )
        >
      >('ggml_conv_1d_ph');
  late final _ggml_conv_1d_ph = _ggml_conv_1d_phPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          int,
          int,
        )
      >();

  /// depthwise
  /// TODO: this is very likely wrong for some cases! - needs more testing
  ffi.Pointer<ggml_tensor> ggml_conv_1d_dw(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
    int s0,
    int p0,
    int d0,
  ) {
    return _ggml_conv_1d_dw(ctx, a, b, s0, p0, d0);
  }

  late final _ggml_conv_1d_dwPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
            ffi.Int,
            ffi.Int,
            ffi.Int,
          )
        >
      >('ggml_conv_1d_dw');
  late final _ggml_conv_1d_dw = _ggml_conv_1d_dwPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          int,
          int,
          int,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_conv_1d_dw_ph(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
    int s0,
    int d0,
  ) {
    return _ggml_conv_1d_dw_ph(ctx, a, b, s0, d0);
  }

  late final _ggml_conv_1d_dw_phPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
            ffi.Int,
            ffi.Int,
          )
        >
      >('ggml_conv_1d_dw_ph');
  late final _ggml_conv_1d_dw_ph = _ggml_conv_1d_dw_phPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          int,
          int,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_conv_transpose_1d(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
    int s0,
    int p0,
    int d0,
  ) {
    return _ggml_conv_transpose_1d(ctx, a, b, s0, p0, d0);
  }

  late final _ggml_conv_transpose_1dPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
            ffi.Int,
            ffi.Int,
            ffi.Int,
          )
        >
      >('ggml_conv_transpose_1d');
  late final _ggml_conv_transpose_1d = _ggml_conv_transpose_1dPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          int,
          int,
          int,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_conv_2d(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
    int s0,
    int s1,
    int p0,
    int p1,
    int d0,
    int d1,
  ) {
    return _ggml_conv_2d(ctx, a, b, s0, s1, p0, p1, d0, d1);
  }

  late final _ggml_conv_2dPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
            ffi.Int,
            ffi.Int,
            ffi.Int,
            ffi.Int,
            ffi.Int,
            ffi.Int,
          )
        >
      >('ggml_conv_2d');
  late final _ggml_conv_2d = _ggml_conv_2dPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          int,
          int,
          int,
          int,
          int,
          int,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_im2col_3d(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
    int IC,
    int s0,
    int s1,
    int s2,
    int p0,
    int p1,
    int p2,
    int d0,
    int d1,
    int d2,
    ggml_type dst_type,
  ) {
    return _ggml_im2col_3d(
      ctx,
      a,
      b,
      IC,
      s0,
      s1,
      s2,
      p0,
      p1,
      p2,
      d0,
      d1,
      d2,
      dst_type.value,
    );
  }

  late final _ggml_im2col_3dPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
            ffi.Int64,
            ffi.Int,
            ffi.Int,
            ffi.Int,
            ffi.Int,
            ffi.Int,
            ffi.Int,
            ffi.Int,
            ffi.Int,
            ffi.Int,
            ffi.UnsignedInt,
          )
        >
      >('ggml_im2col_3d');
  late final _ggml_im2col_3d = _ggml_im2col_3dPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          int,
          int,
          int,
          int,
          int,
          int,
          int,
          int,
          int,
          int,
          int,
        )
      >();

  /// a: [OC*IC, KD, KH, KW]
  /// b: [N*IC, ID, IH, IW]
  /// result: [N*OC, OD, OH, OW]
  ffi.Pointer<ggml_tensor> ggml_conv_3d(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
    int IC,
    int s0,
    int s1,
    int s2,
    int p0,
    int p1,
    int p2,
    int d0,
    int d1,
    int d2,
  ) {
    return _ggml_conv_3d(ctx, a, b, IC, s0, s1, s2, p0, p1, p2, d0, d1, d2);
  }

  late final _ggml_conv_3dPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
            ffi.Int64,
            ffi.Int,
            ffi.Int,
            ffi.Int,
            ffi.Int,
            ffi.Int,
            ffi.Int,
            ffi.Int,
            ffi.Int,
            ffi.Int,
          )
        >
      >('ggml_conv_3d');
  late final _ggml_conv_3d = _ggml_conv_3dPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          int,
          int,
          int,
          int,
          int,
          int,
          int,
          int,
          int,
          int,
        )
      >();

  /// kernel size is a->ne[0] x a->ne[1]
  /// stride is equal to kernel size
  /// padding is zero
  /// example:
  /// a:     16   16    3  768
  /// b:   1024 1024    3    1
  /// res:   64   64  768    1
  /// used in sam
  ffi.Pointer<ggml_tensor> ggml_conv_2d_sk_p0(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
  ) {
    return _ggml_conv_2d_sk_p0(ctx, a, b);
  }

  late final _ggml_conv_2d_sk_p0Ptr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_conv_2d_sk_p0');
  late final _ggml_conv_2d_sk_p0 = _ggml_conv_2d_sk_p0Ptr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  /// kernel size is a->ne[0] x a->ne[1]
  /// stride is 1
  /// padding is half
  /// example:
  /// a:      3    3    256  256
  /// b:     64   64    256    1
  /// res:   64   64    256    1
  /// used in sam
  ffi.Pointer<ggml_tensor> ggml_conv_2d_s1_ph(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
  ) {
    return _ggml_conv_2d_s1_ph(ctx, a, b);
  }

  late final _ggml_conv_2d_s1_phPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_conv_2d_s1_ph');
  late final _ggml_conv_2d_s1_ph = _ggml_conv_2d_s1_phPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  /// depthwise (via im2col and mul_mat)
  ffi.Pointer<ggml_tensor> ggml_conv_2d_dw(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
    int s0,
    int s1,
    int p0,
    int p1,
    int d0,
    int d1,
  ) {
    return _ggml_conv_2d_dw(ctx, a, b, s0, s1, p0, p1, d0, d1);
  }

  late final _ggml_conv_2d_dwPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
            ffi.Int,
            ffi.Int,
            ffi.Int,
            ffi.Int,
            ffi.Int,
            ffi.Int,
          )
        >
      >('ggml_conv_2d_dw');
  late final _ggml_conv_2d_dw = _ggml_conv_2d_dwPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          int,
          int,
          int,
          int,
          int,
          int,
        )
      >();

  /// Depthwise 2D convolution
  /// may be faster than ggml_conv_2d_dw, but not available in all backends
  /// a:   KW    KH    1    C    convolution kernel
  /// b:   W     H     C    N    input data
  /// res: W_out H_out C    N
  ffi.Pointer<ggml_tensor> ggml_conv_2d_dw_direct(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
    int stride0,
    int stride1,
    int pad0,
    int pad1,
    int dilation0,
    int dilation1,
  ) {
    return _ggml_conv_2d_dw_direct(
      ctx,
      a,
      b,
      stride0,
      stride1,
      pad0,
      pad1,
      dilation0,
      dilation1,
    );
  }

  late final _ggml_conv_2d_dw_directPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
            ffi.Int,
            ffi.Int,
            ffi.Int,
            ffi.Int,
            ffi.Int,
            ffi.Int,
          )
        >
      >('ggml_conv_2d_dw_direct');
  late final _ggml_conv_2d_dw_direct = _ggml_conv_2d_dw_directPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          int,
          int,
          int,
          int,
          int,
          int,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_conv_transpose_2d_p0(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
    int stride,
  ) {
    return _ggml_conv_transpose_2d_p0(ctx, a, b, stride);
  }

  late final _ggml_conv_transpose_2d_p0Ptr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
            ffi.Int,
          )
        >
      >('ggml_conv_transpose_2d_p0');
  late final _ggml_conv_transpose_2d_p0 = _ggml_conv_transpose_2d_p0Ptr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          int,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_conv_2d_direct(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
    int s0,
    int s1,
    int p0,
    int p1,
    int d0,
    int d1,
  ) {
    return _ggml_conv_2d_direct(ctx, a, b, s0, s1, p0, p1, d0, d1);
  }

  late final _ggml_conv_2d_directPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
            ffi.Int,
            ffi.Int,
            ffi.Int,
            ffi.Int,
            ffi.Int,
            ffi.Int,
          )
        >
      >('ggml_conv_2d_direct');
  late final _ggml_conv_2d_direct = _ggml_conv_2d_directPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          int,
          int,
          int,
          int,
          int,
          int,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_conv_3d_direct(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
    int s0,
    int s1,
    int s2,
    int p0,
    int p1,
    int p2,
    int d0,
    int d1,
    int d2,
    int n_channels,
    int n_batch,
    int n_channels_out,
  ) {
    return _ggml_conv_3d_direct(
      ctx,
      a,
      b,
      s0,
      s1,
      s2,
      p0,
      p1,
      p2,
      d0,
      d1,
      d2,
      n_channels,
      n_batch,
      n_channels_out,
    );
  }

  late final _ggml_conv_3d_directPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
            ffi.Int,
            ffi.Int,
            ffi.Int,
            ffi.Int,
            ffi.Int,
            ffi.Int,
            ffi.Int,
            ffi.Int,
            ffi.Int,
            ffi.Int,
            ffi.Int,
            ffi.Int,
          )
        >
      >('ggml_conv_3d_direct');
  late final _ggml_conv_3d_direct = _ggml_conv_3d_directPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          int,
          int,
          int,
          int,
          int,
          int,
          int,
          int,
          int,
          int,
          int,
          int,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_pool_1d(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ggml_op_pool op,
    int k0,
    int s0,
    int p0,
  ) {
    return _ggml_pool_1d(ctx, a, op.value, k0, s0, p0);
  }

  late final _ggml_pool_1dPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.UnsignedInt,
            ffi.Int,
            ffi.Int,
            ffi.Int,
          )
        >
      >('ggml_pool_1d');
  late final _ggml_pool_1d = _ggml_pool_1dPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          int,
          int,
          int,
          int,
        )
      >();

  /// the result will have 2*p0 padding for the first dimension
  /// and 2*p1 padding for the second dimension
  ffi.Pointer<ggml_tensor> ggml_pool_2d(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ggml_op_pool op,
    int k0,
    int k1,
    int s0,
    int s1,
    double p0,
    double p1,
  ) {
    return _ggml_pool_2d(ctx, a, op.value, k0, k1, s0, s1, p0, p1);
  }

  late final _ggml_pool_2dPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.UnsignedInt,
            ffi.Int,
            ffi.Int,
            ffi.Int,
            ffi.Int,
            ffi.Float,
            ffi.Float,
          )
        >
      >('ggml_pool_2d');
  late final _ggml_pool_2d = _ggml_pool_2dPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          int,
          int,
          int,
          int,
          int,
          double,
          double,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_pool_2d_back(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> af,
    ggml_op_pool op,
    int k0,
    int k1,
    int s0,
    int s1,
    double p0,
    double p1,
  ) {
    return _ggml_pool_2d_back(ctx, a, af, op.value, k0, k1, s0, s1, p0, p1);
  }

  late final _ggml_pool_2d_backPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
            ffi.UnsignedInt,
            ffi.Int,
            ffi.Int,
            ffi.Int,
            ffi.Int,
            ffi.Float,
            ffi.Float,
          )
        >
      >('ggml_pool_2d_back');
  late final _ggml_pool_2d_back = _ggml_pool_2d_backPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          int,
          int,
          int,
          int,
          int,
          double,
          double,
        )
      >();

  /// interpolate
  /// multiplies ne0 and ne1 by scale factor
  ffi.Pointer<ggml_tensor> ggml_upscale(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    int scale_factor,
    ggml_scale_mode mode,
  ) {
    return _ggml_upscale(ctx, a, scale_factor, mode.value);
  }

  late final _ggml_upscalePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Int,
            ffi.UnsignedInt,
          )
        >
      >('ggml_upscale');
  late final _ggml_upscale = _ggml_upscalePtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          int,
          int,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_upscale_ext(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    int ne0,
    int ne1,
    int ne2,
    int ne3,
    ggml_scale_mode mode,
  ) {
    return _ggml_upscale_ext(ctx, a, ne0, ne1, ne2, ne3, mode.value);
  }

  late final _ggml_upscale_extPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Int,
            ffi.Int,
            ffi.Int,
            ffi.Int,
            ffi.UnsignedInt,
          )
        >
      >('ggml_upscale_ext');
  late final _ggml_upscale_ext = _ggml_upscale_extPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          int,
          int,
          int,
          int,
          int,
        )
      >();

  /// Up- or downsamples the input to the specified size.
  /// 2D scale modes (eg. bilinear) are applied to the first two dimensions.
  ffi.Pointer<ggml_tensor> ggml_interpolate(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    int ne0,
    int ne1,
    int ne2,
    int ne3,
    int mode,
  ) {
    return _ggml_interpolate(ctx, a, ne0, ne1, ne2, ne3, mode);
  }

  late final _ggml_interpolatePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Int64,
            ffi.Int64,
            ffi.Int64,
            ffi.Int64,
            ffi.Uint32,
          )
        >
      >('ggml_interpolate');
  late final _ggml_interpolate = _ggml_interpolatePtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          int,
          int,
          int,
          int,
          int,
        )
      >();

  /// pad each dimension with zeros: [x, ..., x] -> [x, ..., x, 0, ..., 0]
  ffi.Pointer<ggml_tensor> ggml_pad(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    int p0,
    int p1,
    int p2,
    int p3,
  ) {
    return _ggml_pad(ctx, a, p0, p1, p2, p3);
  }

  late final _ggml_padPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Int,
            ffi.Int,
            ffi.Int,
            ffi.Int,
          )
        >
      >('ggml_pad');
  late final _ggml_pad = _ggml_padPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          int,
          int,
          int,
          int,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_pad_ext(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    int lp0,
    int rp0,
    int lp1,
    int rp1,
    int lp2,
    int rp2,
    int lp3,
    int rp3,
  ) {
    return _ggml_pad_ext(ctx, a, lp0, rp0, lp1, rp1, lp2, rp2, lp3, rp3);
  }

  late final _ggml_pad_extPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Int,
            ffi.Int,
            ffi.Int,
            ffi.Int,
            ffi.Int,
            ffi.Int,
            ffi.Int,
            ffi.Int,
          )
        >
      >('ggml_pad_ext');
  late final _ggml_pad_ext = _ggml_pad_extPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          int,
          int,
          int,
          int,
          int,
          int,
          int,
          int,
        )
      >();

  /// pad each dimension with reflection: [a, b, c, d] -> [b, a, b, c, d, c]
  ffi.Pointer<ggml_tensor> ggml_pad_reflect_1d(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    int p0,
    int p1,
  ) {
    return _ggml_pad_reflect_1d(ctx, a, p0, p1);
  }

  late final _ggml_pad_reflect_1dPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Int,
            ffi.Int,
          )
        >
      >('ggml_pad_reflect_1d');
  late final _ggml_pad_reflect_1d = _ggml_pad_reflect_1dPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          int,
          int,
        )
      >();

  /// Move tensor elements by an offset given for each dimension. Elements that
  /// are shifted beyond the last position are wrapped around to the beginning.
  ffi.Pointer<ggml_tensor> ggml_roll(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    int shift0,
    int shift1,
    int shift2,
    int shift3,
  ) {
    return _ggml_roll(ctx, a, shift0, shift1, shift2, shift3);
  }

  late final _ggml_rollPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Int,
            ffi.Int,
            ffi.Int,
            ffi.Int,
          )
        >
      >('ggml_roll');
  late final _ggml_roll = _ggml_rollPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          int,
          int,
          int,
          int,
        )
      >();

  /// Convert matrix into a triangular one (upper, strict upper, lower or strict lower) by writing
  /// zeroes everywhere outside the masked area
  ffi.Pointer<ggml_tensor> ggml_tri(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ggml_tri_type type,
  ) {
    return _ggml_tri(ctx, a, type.value);
  }

  late final _ggml_triPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.UnsignedInt,
          )
        >
      >('ggml_tri');
  late final _ggml_tri = _ggml_triPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          int,
        )
      >();

  /// Fill tensor a with constant c
  ffi.Pointer<ggml_tensor> ggml_fill(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    double c,
  ) {
    return _ggml_fill(ctx, a, c);
  }

  late final _ggml_fillPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Float,
          )
        >
      >('ggml_fill');
  late final _ggml_fill = _ggml_fillPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          double,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_fill_inplace(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    double c,
  ) {
    return _ggml_fill_inplace(ctx, a, c);
  }

  late final _ggml_fill_inplacePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Float,
          )
        >
      >('ggml_fill_inplace');
  late final _ggml_fill_inplace = _ggml_fill_inplacePtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          double,
        )
      >();

  /// Ref: https://github.com/CompVis/stable-diffusion/blob/main/ldm/modules/diffusionmodules/util.py#L151
  /// timesteps: [N,]
  /// return: [N, dim]
  ffi.Pointer<ggml_tensor> ggml_timestep_embedding(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> timesteps,
    int dim,
    int max_period,
  ) {
    return _ggml_timestep_embedding(ctx, timesteps, dim, max_period);
  }

  late final _ggml_timestep_embeddingPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Int,
            ffi.Int,
          )
        >
      >('ggml_timestep_embedding');
  late final _ggml_timestep_embedding = _ggml_timestep_embeddingPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          int,
          int,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_argsort(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ggml_sort_order order,
  ) {
    return _ggml_argsort(ctx, a, order.value);
  }

  late final _ggml_argsortPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.UnsignedInt,
          )
        >
      >('ggml_argsort');
  late final _ggml_argsort = _ggml_argsortPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          int,
        )
      >();

  /// similar to ggml_top_k but implemented as `argsort` + `view`
  ffi.Pointer<ggml_tensor> ggml_argsort_top_k(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    int k,
  ) {
    return _ggml_argsort_top_k(ctx, a, k);
  }

  late final _ggml_argsort_top_kPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Int,
          )
        >
      >('ggml_argsort_top_k');
  late final _ggml_argsort_top_k = _ggml_argsort_top_kPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          int,
        )
      >();

  /// top k elements per row
  /// note: the resulting top k indices are in no particular order
  ffi.Pointer<ggml_tensor> ggml_top_k(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    int k,
  ) {
    return _ggml_top_k(ctx, a, k);
  }

  late final _ggml_top_kPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Int,
          )
        >
      >('ggml_top_k');
  late final _ggml_top_k = _ggml_top_kPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          int,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_arange(
    ffi.Pointer<ggml_context> ctx,
    double start,
    double stop,
    double step,
  ) {
    return _ggml_arange(ctx, start, stop, step);
  }

  late final _ggml_arangePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Float,
            ffi.Float,
            ffi.Float,
          )
        >
      >('ggml_arange');
  late final _ggml_arange = _ggml_arangePtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          double,
          double,
          double,
        )
      >();

  /// q:    [n_embd_k, n_batch,     n_head,    ne3 ]
  /// k:    [n_embd_k, n_kv,        n_head_kv, ne3 ]
  /// v:    [n_embd_v, n_kv,        n_head_kv, ne3 ] !! not transposed !!
  /// mask: [n_kv,     n_batch_pad, ne32,      ne33] !! n_batch_pad = GGML_PAD(n_batch, GGML_KQ_MASK_PAD) !!
  /// res:  [n_embd_v, n_head,      n_batch,   ne3 ] !! permuted !!
  ///
  /// broadcast:
  /// n_head % n_head_kv == 0
  /// n_head % ne32      == 0
  /// ne3    % ne33      == 0
  ffi.Pointer<ggml_tensor> ggml_flash_attn_ext(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> q,
    ffi.Pointer<ggml_tensor> k,
    ffi.Pointer<ggml_tensor> v,
    ffi.Pointer<ggml_tensor> mask,
    double scale,
    double max_bias,
    double logit_softcap,
  ) {
    return _ggml_flash_attn_ext(
      ctx,
      q,
      k,
      v,
      mask,
      scale,
      max_bias,
      logit_softcap,
    );
  }

  late final _ggml_flash_attn_extPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
            ffi.Float,
            ffi.Float,
            ffi.Float,
          )
        >
      >('ggml_flash_attn_ext');
  late final _ggml_flash_attn_ext = _ggml_flash_attn_extPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          double,
          double,
          double,
        )
      >();

  void ggml_flash_attn_ext_set_prec(
    ffi.Pointer<ggml_tensor> a,
    ggml_prec prec,
  ) {
    return _ggml_flash_attn_ext_set_prec(a, prec.value);
  }

  late final _ggml_flash_attn_ext_set_precPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<ggml_tensor>, ffi.UnsignedInt)
        >
      >('ggml_flash_attn_ext_set_prec');
  late final _ggml_flash_attn_ext_set_prec = _ggml_flash_attn_ext_set_precPtr
      .asFunction<void Function(ffi.Pointer<ggml_tensor>, int)>();

  ggml_prec ggml_flash_attn_ext_get_prec(ffi.Pointer<ggml_tensor> a) {
    return ggml_prec.fromValue(_ggml_flash_attn_ext_get_prec(a));
  }

  late final _ggml_flash_attn_ext_get_precPtr =
      _lookup<
        ffi.NativeFunction<ffi.UnsignedInt Function(ffi.Pointer<ggml_tensor>)>
      >('ggml_flash_attn_ext_get_prec');
  late final _ggml_flash_attn_ext_get_prec = _ggml_flash_attn_ext_get_precPtr
      .asFunction<int Function(ffi.Pointer<ggml_tensor>)>();

  void ggml_flash_attn_ext_add_sinks(
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> sinks,
  ) {
    return _ggml_flash_attn_ext_add_sinks(a, sinks);
  }

  late final _ggml_flash_attn_ext_add_sinksPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<ggml_tensor>, ffi.Pointer<ggml_tensor>)
        >
      >('ggml_flash_attn_ext_add_sinks');
  late final _ggml_flash_attn_ext_add_sinks = _ggml_flash_attn_ext_add_sinksPtr
      .asFunction<
        void Function(ffi.Pointer<ggml_tensor>, ffi.Pointer<ggml_tensor>)
      >();

  /// TODO: needs to be adapted to ggml_flash_attn_ext
  ffi.Pointer<ggml_tensor> ggml_flash_attn_back(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> q,
    ffi.Pointer<ggml_tensor> k,
    ffi.Pointer<ggml_tensor> v,
    ffi.Pointer<ggml_tensor> d,
    bool masked,
  ) {
    return _ggml_flash_attn_back(ctx, q, k, v, d, masked);
  }

  late final _ggml_flash_attn_backPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
            ffi.Bool,
          )
        >
      >('ggml_flash_attn_back');
  late final _ggml_flash_attn_back = _ggml_flash_attn_backPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          bool,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_ssm_conv(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> sx,
    ffi.Pointer<ggml_tensor> c,
  ) {
    return _ggml_ssm_conv(ctx, sx, c);
  }

  late final _ggml_ssm_convPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_ssm_conv');
  late final _ggml_ssm_conv = _ggml_ssm_convPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_ssm_scan(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> s,
    ffi.Pointer<ggml_tensor> x,
    ffi.Pointer<ggml_tensor> dt,
    ffi.Pointer<ggml_tensor> A,
    ffi.Pointer<ggml_tensor> B,
    ffi.Pointer<ggml_tensor> C,
    ffi.Pointer<ggml_tensor> ids,
  ) {
    return _ggml_ssm_scan(ctx, s, x, dt, A, B, C, ids);
  }

  late final _ggml_ssm_scanPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_ssm_scan');
  late final _ggml_ssm_scan = _ggml_ssm_scanPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  /// partition into non-overlapping windows with padding if needed
  /// example:
  /// a:   768   64   64    1
  /// w:    14
  /// res: 768   14   14    25
  /// used in sam
  ffi.Pointer<ggml_tensor> ggml_win_part(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    int w,
  ) {
    return _ggml_win_part(ctx, a, w);
  }

  late final _ggml_win_partPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Int,
          )
        >
      >('ggml_win_part');
  late final _ggml_win_part = _ggml_win_partPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          int,
        )
      >();

  /// reverse of ggml_win_part
  /// used in sam
  ffi.Pointer<ggml_tensor> ggml_win_unpart(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    int w0,
    int h0,
    int w,
  ) {
    return _ggml_win_unpart(ctx, a, w0, h0, w);
  }

  late final _ggml_win_unpartPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Int,
            ffi.Int,
            ffi.Int,
          )
        >
      >('ggml_win_unpart');
  late final _ggml_win_unpart = _ggml_win_unpartPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          int,
          int,
          int,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_unary(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ggml_unary_op op,
  ) {
    return _ggml_unary(ctx, a, op.value);
  }

  late final _ggml_unaryPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.UnsignedInt,
          )
        >
      >('ggml_unary');
  late final _ggml_unary = _ggml_unaryPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          int,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_unary_inplace(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ggml_unary_op op,
  ) {
    return _ggml_unary_inplace(ctx, a, op.value);
  }

  late final _ggml_unary_inplacePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.UnsignedInt,
          )
        >
      >('ggml_unary_inplace');
  late final _ggml_unary_inplace = _ggml_unary_inplacePtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          int,
        )
      >();

  /// used in sam
  ffi.Pointer<ggml_tensor> ggml_get_rel_pos(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    int qh,
    int kh,
  ) {
    return _ggml_get_rel_pos(ctx, a, qh, kh);
  }

  late final _ggml_get_rel_posPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Int,
            ffi.Int,
          )
        >
      >('ggml_get_rel_pos');
  late final _ggml_get_rel_pos = _ggml_get_rel_posPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          int,
          int,
        )
      >();

  /// used in sam
  ffi.Pointer<ggml_tensor> ggml_add_rel_pos(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> pw,
    ffi.Pointer<ggml_tensor> ph,
  ) {
    return _ggml_add_rel_pos(ctx, a, pw, ph);
  }

  late final _ggml_add_rel_posPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_add_rel_pos');
  late final _ggml_add_rel_pos = _ggml_add_rel_posPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_add_rel_pos_inplace(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> pw,
    ffi.Pointer<ggml_tensor> ph,
  ) {
    return _ggml_add_rel_pos_inplace(ctx, a, pw, ph);
  }

  late final _ggml_add_rel_pos_inplacePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_add_rel_pos_inplace');
  late final _ggml_add_rel_pos_inplace = _ggml_add_rel_pos_inplacePtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_rwkv_wkv6(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> k,
    ffi.Pointer<ggml_tensor> v,
    ffi.Pointer<ggml_tensor> r,
    ffi.Pointer<ggml_tensor> tf,
    ffi.Pointer<ggml_tensor> td,
    ffi.Pointer<ggml_tensor> state,
  ) {
    return _ggml_rwkv_wkv6(ctx, k, v, r, tf, td, state);
  }

  late final _ggml_rwkv_wkv6Ptr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_rwkv_wkv6');
  late final _ggml_rwkv_wkv6 = _ggml_rwkv_wkv6Ptr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_gated_linear_attn(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> k,
    ffi.Pointer<ggml_tensor> v,
    ffi.Pointer<ggml_tensor> q,
    ffi.Pointer<ggml_tensor> g,
    ffi.Pointer<ggml_tensor> state,
    double scale,
  ) {
    return _ggml_gated_linear_attn(ctx, k, v, q, g, state, scale);
  }

  late final _ggml_gated_linear_attnPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
            ffi.Float,
          )
        >
      >('ggml_gated_linear_attn');
  late final _ggml_gated_linear_attn = _ggml_gated_linear_attnPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          double,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_rwkv_wkv7(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> r,
    ffi.Pointer<ggml_tensor> w,
    ffi.Pointer<ggml_tensor> k,
    ffi.Pointer<ggml_tensor> v,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
    ffi.Pointer<ggml_tensor> state,
  ) {
    return _ggml_rwkv_wkv7(ctx, r, w, k, v, a, b, state);
  }

  late final _ggml_rwkv_wkv7Ptr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_rwkv_wkv7');
  late final _ggml_rwkv_wkv7 = _ggml_rwkv_wkv7Ptr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  /// Solves a specific equation of the form Ax=B, where A is a triangular matrix
  /// without zeroes on the diagonal (i.e. invertible).
  /// B can have any number of columns, but must have the same number of rows as A
  /// If A is [n, n] and B is [n, m], then the result will be [n, m] as well
  /// Has O(n^3) complexity (unlike most matrix ops out there), so use on cases
  /// where n > 100 sparingly, pre-chunk if necessary.
  ///
  /// If left = false, solves xA=B instead
  /// If lower = false, assumes upper triangular instead
  /// If uni = true, assumes diagonal of A to be all ones (will override actual values)
  ///
  /// TODO: currently only lower, right, non-unitriangular variant is implemented
  ffi.Pointer<ggml_tensor> ggml_solve_tri(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
    bool left,
    bool lower,
    bool uni,
  ) {
    return _ggml_solve_tri(ctx, a, b, left, lower, uni);
  }

  late final _ggml_solve_triPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
            ffi.Bool,
            ffi.Bool,
            ffi.Bool,
          )
        >
      >('ggml_solve_tri');
  late final _ggml_solve_tri = _ggml_solve_triPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          bool,
          bool,
          bool,
        )
      >();

  /// n_tasks == GGML_N_TASKS_MAX means to use max number of tasks
  ffi.Pointer<ggml_tensor> ggml_map_custom1(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ggml_custom1_op_t fun,
    int n_tasks,
    ffi.Pointer<ffi.Void> userdata,
  ) {
    return _ggml_map_custom1(ctx, a, fun, n_tasks, userdata);
  }

  late final _ggml_map_custom1Ptr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ggml_custom1_op_t,
            ffi.Int,
            ffi.Pointer<ffi.Void>,
          )
        >
      >('ggml_map_custom1');
  late final _ggml_map_custom1 = _ggml_map_custom1Ptr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ggml_custom1_op_t,
          int,
          ffi.Pointer<ffi.Void>,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_map_custom1_inplace(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ggml_custom1_op_t fun,
    int n_tasks,
    ffi.Pointer<ffi.Void> userdata,
  ) {
    return _ggml_map_custom1_inplace(ctx, a, fun, n_tasks, userdata);
  }

  late final _ggml_map_custom1_inplacePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ggml_custom1_op_t,
            ffi.Int,
            ffi.Pointer<ffi.Void>,
          )
        >
      >('ggml_map_custom1_inplace');
  late final _ggml_map_custom1_inplace = _ggml_map_custom1_inplacePtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ggml_custom1_op_t,
          int,
          ffi.Pointer<ffi.Void>,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_map_custom2(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
    ggml_custom2_op_t fun,
    int n_tasks,
    ffi.Pointer<ffi.Void> userdata,
  ) {
    return _ggml_map_custom2(ctx, a, b, fun, n_tasks, userdata);
  }

  late final _ggml_map_custom2Ptr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
            ggml_custom2_op_t,
            ffi.Int,
            ffi.Pointer<ffi.Void>,
          )
        >
      >('ggml_map_custom2');
  late final _ggml_map_custom2 = _ggml_map_custom2Ptr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          ggml_custom2_op_t,
          int,
          ffi.Pointer<ffi.Void>,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_map_custom2_inplace(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
    ggml_custom2_op_t fun,
    int n_tasks,
    ffi.Pointer<ffi.Void> userdata,
  ) {
    return _ggml_map_custom2_inplace(ctx, a, b, fun, n_tasks, userdata);
  }

  late final _ggml_map_custom2_inplacePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
            ggml_custom2_op_t,
            ffi.Int,
            ffi.Pointer<ffi.Void>,
          )
        >
      >('ggml_map_custom2_inplace');
  late final _ggml_map_custom2_inplace = _ggml_map_custom2_inplacePtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          ggml_custom2_op_t,
          int,
          ffi.Pointer<ffi.Void>,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_map_custom3(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
    ffi.Pointer<ggml_tensor> c,
    ggml_custom3_op_t fun,
    int n_tasks,
    ffi.Pointer<ffi.Void> userdata,
  ) {
    return _ggml_map_custom3(ctx, a, b, c, fun, n_tasks, userdata);
  }

  late final _ggml_map_custom3Ptr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
            ggml_custom3_op_t,
            ffi.Int,
            ffi.Pointer<ffi.Void>,
          )
        >
      >('ggml_map_custom3');
  late final _ggml_map_custom3 = _ggml_map_custom3Ptr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          ggml_custom3_op_t,
          int,
          ffi.Pointer<ffi.Void>,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_map_custom3_inplace(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
    ffi.Pointer<ggml_tensor> c,
    ggml_custom3_op_t fun,
    int n_tasks,
    ffi.Pointer<ffi.Void> userdata,
  ) {
    return _ggml_map_custom3_inplace(ctx, a, b, c, fun, n_tasks, userdata);
  }

  late final _ggml_map_custom3_inplacePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
            ggml_custom3_op_t,
            ffi.Int,
            ffi.Pointer<ffi.Void>,
          )
        >
      >('ggml_map_custom3_inplace');
  late final _ggml_map_custom3_inplace = _ggml_map_custom3_inplacePtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          ggml_custom3_op_t,
          int,
          ffi.Pointer<ffi.Void>,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_custom_4d(
    ffi.Pointer<ggml_context> ctx,
    ggml_type type,
    int ne0,
    int ne1,
    int ne2,
    int ne3,
    ffi.Pointer<ffi.Pointer<ggml_tensor>> args,
    int n_args,
    ggml_custom_op_t fun,
    int n_tasks,
    ffi.Pointer<ffi.Void> userdata,
  ) {
    return _ggml_custom_4d(
      ctx,
      type.value,
      ne0,
      ne1,
      ne2,
      ne3,
      args,
      n_args,
      fun,
      n_tasks,
      userdata,
    );
  }

  late final _ggml_custom_4dPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.UnsignedInt,
            ffi.Int64,
            ffi.Int64,
            ffi.Int64,
            ffi.Int64,
            ffi.Pointer<ffi.Pointer<ggml_tensor>>,
            ffi.Int,
            ggml_custom_op_t,
            ffi.Int,
            ffi.Pointer<ffi.Void>,
          )
        >
      >('ggml_custom_4d');
  late final _ggml_custom_4d = _ggml_custom_4dPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          int,
          int,
          int,
          int,
          int,
          ffi.Pointer<ffi.Pointer<ggml_tensor>>,
          int,
          ggml_custom_op_t,
          int,
          ffi.Pointer<ffi.Void>,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_custom_inplace(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ffi.Pointer<ggml_tensor>> args,
    int n_args,
    ggml_custom_op_t fun,
    int n_tasks,
    ffi.Pointer<ffi.Void> userdata,
  ) {
    return _ggml_custom_inplace(ctx, a, args, n_args, fun, n_tasks, userdata);
  }

  late final _ggml_custom_inplacePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ffi.Pointer<ggml_tensor>>,
            ffi.Int,
            ggml_custom_op_t,
            ffi.Int,
            ffi.Pointer<ffi.Void>,
          )
        >
      >('ggml_custom_inplace');
  late final _ggml_custom_inplace = _ggml_custom_inplacePtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ffi.Pointer<ggml_tensor>>,
          int,
          ggml_custom_op_t,
          int,
          ffi.Pointer<ffi.Void>,
        )
      >();

  /// loss function
  ffi.Pointer<ggml_tensor> ggml_cross_entropy_loss(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
  ) {
    return _ggml_cross_entropy_loss(ctx, a, b);
  }

  late final _ggml_cross_entropy_lossPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_cross_entropy_loss');
  late final _ggml_cross_entropy_loss = _ggml_cross_entropy_lossPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_cross_entropy_loss_back(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
    ffi.Pointer<ggml_tensor> c,
  ) {
    return _ggml_cross_entropy_loss_back(ctx, a, b, c);
  }

  late final _ggml_cross_entropy_loss_backPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_cross_entropy_loss_back');
  late final _ggml_cross_entropy_loss_back = _ggml_cross_entropy_loss_backPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  /// AdamW optimizer step
  /// Paper: https://arxiv.org/pdf/1711.05101v3.pdf
  /// PyTorch: https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html
  ffi.Pointer<ggml_tensor> ggml_opt_step_adamw(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> grad,
    ffi.Pointer<ggml_tensor> m,
    ffi.Pointer<ggml_tensor> v,
    ffi.Pointer<ggml_tensor> adamw_params,
  ) {
    return _ggml_opt_step_adamw(ctx, a, grad, m, v, adamw_params);
  }

  late final _ggml_opt_step_adamwPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_opt_step_adamw');
  late final _ggml_opt_step_adamw = _ggml_opt_step_adamwPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  /// stochastic gradient descent step (with weight decay)
  ffi.Pointer<ggml_tensor> ggml_opt_step_sgd(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> grad,
    ffi.Pointer<ggml_tensor> sgd_params,
  ) {
    return _ggml_opt_step_sgd(ctx, a, grad, sgd_params);
  }

  late final _ggml_opt_step_sgdPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_opt_step_sgd');
  late final _ggml_opt_step_sgd = _ggml_opt_step_sgdPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  /// automatic differentiation
  void ggml_build_forward_expand(
    ffi.Pointer<ggml_cgraph> cgraph,
    ffi.Pointer<ggml_tensor> tensor,
  ) {
    return _ggml_build_forward_expand(cgraph, tensor);
  }

  late final _ggml_build_forward_expandPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<ggml_cgraph>, ffi.Pointer<ggml_tensor>)
        >
      >('ggml_build_forward_expand');
  late final _ggml_build_forward_expand = _ggml_build_forward_expandPtr
      .asFunction<
        void Function(ffi.Pointer<ggml_cgraph>, ffi.Pointer<ggml_tensor>)
      >();

  void ggml_build_backward_expand(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_cgraph> cgraph,
    ffi.Pointer<ffi.Pointer<ggml_tensor>> grad_accs,
  ) {
    return _ggml_build_backward_expand(ctx, cgraph, grad_accs);
  }

  late final _ggml_build_backward_expandPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Void Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_cgraph>,
            ffi.Pointer<ffi.Pointer<ggml_tensor>>,
          )
        >
      >('ggml_build_backward_expand');
  late final _ggml_build_backward_expand = _ggml_build_backward_expandPtr
      .asFunction<
        void Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_cgraph>,
          ffi.Pointer<ffi.Pointer<ggml_tensor>>,
        )
      >();

  /// graph allocation in a context
  ffi.Pointer<ggml_cgraph> ggml_new_graph(ffi.Pointer<ggml_context> ctx) {
    return _ggml_new_graph(ctx);
  }

  late final _ggml_new_graphPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_cgraph> Function(ffi.Pointer<ggml_context>)
        >
      >('ggml_new_graph');
  late final _ggml_new_graph = _ggml_new_graphPtr
      .asFunction<
        ffi.Pointer<ggml_cgraph> Function(ffi.Pointer<ggml_context>)
      >();

  ffi.Pointer<ggml_cgraph> ggml_new_graph_custom(
    ffi.Pointer<ggml_context> ctx,
    int size,
    bool grads,
  ) {
    return _ggml_new_graph_custom(ctx, size, grads);
  }

  late final _ggml_new_graph_customPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_cgraph> Function(
            ffi.Pointer<ggml_context>,
            ffi.Size,
            ffi.Bool,
          )
        >
      >('ggml_new_graph_custom');
  late final _ggml_new_graph_custom = _ggml_new_graph_customPtr
      .asFunction<
        ffi.Pointer<ggml_cgraph> Function(ffi.Pointer<ggml_context>, int, bool)
      >();

  ffi.Pointer<ggml_cgraph> ggml_graph_dup(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_cgraph> cgraph,
    bool force_grads,
  ) {
    return _ggml_graph_dup(ctx, cgraph, force_grads);
  }

  late final _ggml_graph_dupPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_cgraph> Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_cgraph>,
            ffi.Bool,
          )
        >
      >('ggml_graph_dup');
  late final _ggml_graph_dup = _ggml_graph_dupPtr
      .asFunction<
        ffi.Pointer<ggml_cgraph> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_cgraph>,
          bool,
        )
      >();

  void ggml_graph_cpy(
    ffi.Pointer<ggml_cgraph> src,
    ffi.Pointer<ggml_cgraph> dst,
  ) {
    return _ggml_graph_cpy(src, dst);
  }

  late final _ggml_graph_cpyPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<ggml_cgraph>, ffi.Pointer<ggml_cgraph>)
        >
      >('ggml_graph_cpy');
  late final _ggml_graph_cpy = _ggml_graph_cpyPtr
      .asFunction<
        void Function(ffi.Pointer<ggml_cgraph>, ffi.Pointer<ggml_cgraph>)
      >();

  void ggml_graph_reset(ffi.Pointer<ggml_cgraph> cgraph) {
    return _ggml_graph_reset(cgraph);
  }

  late final _ggml_graph_resetPtr =
      _lookup<ffi.NativeFunction<ffi.Void Function(ffi.Pointer<ggml_cgraph>)>>(
        'ggml_graph_reset',
      );
  late final _ggml_graph_reset = _ggml_graph_resetPtr
      .asFunction<void Function(ffi.Pointer<ggml_cgraph>)>();

  void ggml_graph_clear(ffi.Pointer<ggml_cgraph> cgraph) {
    return _ggml_graph_clear(cgraph);
  }

  late final _ggml_graph_clearPtr =
      _lookup<ffi.NativeFunction<ffi.Void Function(ffi.Pointer<ggml_cgraph>)>>(
        'ggml_graph_clear',
      );
  late final _ggml_graph_clear = _ggml_graph_clearPtr
      .asFunction<void Function(ffi.Pointer<ggml_cgraph>)>();

  int ggml_graph_size(ffi.Pointer<ggml_cgraph> cgraph) {
    return _ggml_graph_size(cgraph);
  }

  late final _ggml_graph_sizePtr =
      _lookup<ffi.NativeFunction<ffi.Int Function(ffi.Pointer<ggml_cgraph>)>>(
        'ggml_graph_size',
      );
  late final _ggml_graph_size = _ggml_graph_sizePtr
      .asFunction<int Function(ffi.Pointer<ggml_cgraph>)>();

  ffi.Pointer<ggml_tensor> ggml_graph_node(
    ffi.Pointer<ggml_cgraph> cgraph,
    int i,
  ) {
    return _ggml_graph_node(cgraph, i);
  }

  late final _ggml_graph_nodePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_cgraph>, ffi.Int)
        >
      >('ggml_graph_node');
  late final _ggml_graph_node = _ggml_graph_nodePtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_cgraph>, int)
      >();

  ffi.Pointer<ffi.Pointer<ggml_tensor>> ggml_graph_nodes(
    ffi.Pointer<ggml_cgraph> cgraph,
  ) {
    return _ggml_graph_nodes(cgraph);
  }

  late final _ggml_graph_nodesPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ffi.Pointer<ggml_tensor>> Function(
            ffi.Pointer<ggml_cgraph>,
          )
        >
      >('ggml_graph_nodes');
  late final _ggml_graph_nodes = _ggml_graph_nodesPtr
      .asFunction<
        ffi.Pointer<ffi.Pointer<ggml_tensor>> Function(ffi.Pointer<ggml_cgraph>)
      >();

  int ggml_graph_n_nodes(ffi.Pointer<ggml_cgraph> cgraph) {
    return _ggml_graph_n_nodes(cgraph);
  }

  late final _ggml_graph_n_nodesPtr =
      _lookup<ffi.NativeFunction<ffi.Int Function(ffi.Pointer<ggml_cgraph>)>>(
        'ggml_graph_n_nodes',
      );
  late final _ggml_graph_n_nodes = _ggml_graph_n_nodesPtr
      .asFunction<int Function(ffi.Pointer<ggml_cgraph>)>();

  void ggml_graph_add_node(
    ffi.Pointer<ggml_cgraph> cgraph,
    ffi.Pointer<ggml_tensor> tensor,
  ) {
    return _ggml_graph_add_node(cgraph, tensor);
  }

  late final _ggml_graph_add_nodePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<ggml_cgraph>, ffi.Pointer<ggml_tensor>)
        >
      >('ggml_graph_add_node');
  late final _ggml_graph_add_node = _ggml_graph_add_nodePtr
      .asFunction<
        void Function(ffi.Pointer<ggml_cgraph>, ffi.Pointer<ggml_tensor>)
      >();

  int ggml_graph_overhead() {
    return _ggml_graph_overhead();
  }

  late final _ggml_graph_overheadPtr =
      _lookup<ffi.NativeFunction<ffi.Size Function()>>('ggml_graph_overhead');
  late final _ggml_graph_overhead = _ggml_graph_overheadPtr
      .asFunction<int Function()>();

  int ggml_graph_overhead_custom(int size, bool grads) {
    return _ggml_graph_overhead_custom(size, grads);
  }

  late final _ggml_graph_overhead_customPtr =
      _lookup<ffi.NativeFunction<ffi.Size Function(ffi.Size, ffi.Bool)>>(
        'ggml_graph_overhead_custom',
      );
  late final _ggml_graph_overhead_custom = _ggml_graph_overhead_customPtr
      .asFunction<int Function(int, bool)>();

  ffi.Pointer<ggml_tensor> ggml_graph_get_tensor(
    ffi.Pointer<ggml_cgraph> cgraph,
    ffi.Pointer<ffi.Char> name,
  ) {
    return _ggml_graph_get_tensor(cgraph, name);
  }

  late final _ggml_graph_get_tensorPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_cgraph>,
            ffi.Pointer<ffi.Char>,
          )
        >
      >('ggml_graph_get_tensor');
  late final _ggml_graph_get_tensor = _ggml_graph_get_tensorPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_cgraph>,
          ffi.Pointer<ffi.Char>,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_graph_get_grad(
    ffi.Pointer<ggml_cgraph> cgraph,
    ffi.Pointer<ggml_tensor> node,
  ) {
    return _ggml_graph_get_grad(cgraph, node);
  }

  late final _ggml_graph_get_gradPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_cgraph>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_graph_get_grad');
  late final _ggml_graph_get_grad = _ggml_graph_get_gradPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_cgraph>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  ffi.Pointer<ggml_tensor> ggml_graph_get_grad_acc(
    ffi.Pointer<ggml_cgraph> cgraph,
    ffi.Pointer<ggml_tensor> node,
  ) {
    return _ggml_graph_get_grad_acc(cgraph, node);
  }

  late final _ggml_graph_get_grad_accPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_cgraph>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_graph_get_grad_acc');
  late final _ggml_graph_get_grad_acc = _ggml_graph_get_grad_accPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_cgraph>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  /// print info and performance information for the graph
  void ggml_graph_print(ffi.Pointer<ggml_cgraph> cgraph) {
    return _ggml_graph_print(cgraph);
  }

  late final _ggml_graph_printPtr =
      _lookup<ffi.NativeFunction<ffi.Void Function(ffi.Pointer<ggml_cgraph>)>>(
        'ggml_graph_print',
      );
  late final _ggml_graph_print = _ggml_graph_printPtr
      .asFunction<void Function(ffi.Pointer<ggml_cgraph>)>();

  /// dump the graph into a file using the dot format
  void ggml_graph_dump_dot(
    ffi.Pointer<ggml_cgraph> gb,
    ffi.Pointer<ggml_cgraph> gf,
    ffi.Pointer<ffi.Char> filename,
  ) {
    return _ggml_graph_dump_dot(gb, gf, filename);
  }

  late final _ggml_graph_dump_dotPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Void Function(
            ffi.Pointer<ggml_cgraph>,
            ffi.Pointer<ggml_cgraph>,
            ffi.Pointer<ffi.Char>,
          )
        >
      >('ggml_graph_dump_dot');
  late final _ggml_graph_dump_dot = _ggml_graph_dump_dotPtr
      .asFunction<
        void Function(
          ffi.Pointer<ggml_cgraph>,
          ffi.Pointer<ggml_cgraph>,
          ffi.Pointer<ffi.Char>,
        )
      >();

  /// Set callback for all future logging events.
  /// If this is not called, or NULL is supplied, everything is output on stderr.
  void ggml_log_set(
    ggml_log_callback log_callback,
    ffi.Pointer<ffi.Void> user_data,
  ) {
    return _ggml_log_set(log_callback, user_data);
  }

  late final _ggml_log_setPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Void Function(ggml_log_callback, ffi.Pointer<ffi.Void>)
        >
      >('ggml_log_set');
  late final _ggml_log_set = _ggml_log_setPtr
      .asFunction<void Function(ggml_log_callback, ffi.Pointer<ffi.Void>)>();

  ffi.Pointer<ggml_tensor> ggml_set_zero(ffi.Pointer<ggml_tensor> tensor) {
    return _ggml_set_zero(tensor);
  }

  late final _ggml_set_zeroPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_tensor>)
        >
      >('ggml_set_zero');
  late final _ggml_set_zero = _ggml_set_zeroPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_tensor>)
      >();

  /// - ggml_quantize_init can be called multiple times with the same type
  /// it will only initialize the quantization tables for the first call or after ggml_quantize_free
  /// automatically called by ggml_quantize_chunk for convenience
  ///
  /// - ggml_quantize_free will free any memory allocated by ggml_quantize_init
  /// call this at the end of the program to avoid memory leaks
  ///
  /// note: these are thread-safe
  void ggml_quantize_init(ggml_type type) {
    return _ggml_quantize_init(type.value);
  }

  late final _ggml_quantize_initPtr =
      _lookup<ffi.NativeFunction<ffi.Void Function(ffi.UnsignedInt)>>(
        'ggml_quantize_init',
      );
  late final _ggml_quantize_init = _ggml_quantize_initPtr
      .asFunction<void Function(int)>();

  void ggml_quantize_free() {
    return _ggml_quantize_free();
  }

  late final _ggml_quantize_freePtr =
      _lookup<ffi.NativeFunction<ffi.Void Function()>>('ggml_quantize_free');
  late final _ggml_quantize_free = _ggml_quantize_freePtr
      .asFunction<void Function()>();

  /// some quantization type cannot be used without an importance matrix
  bool ggml_quantize_requires_imatrix(ggml_type type) {
    return _ggml_quantize_requires_imatrix(type.value);
  }

  late final _ggml_quantize_requires_imatrixPtr =
      _lookup<ffi.NativeFunction<ffi.Bool Function(ffi.UnsignedInt)>>(
        'ggml_quantize_requires_imatrix',
      );
  late final _ggml_quantize_requires_imatrix =
      _ggml_quantize_requires_imatrixPtr.asFunction<bool Function(int)>();

  /// calls ggml_quantize_init internally (i.e. can allocate memory)
  int ggml_quantize_chunk(
    ggml_type type,
    ffi.Pointer<ffi.Float> src,
    ffi.Pointer<ffi.Void> dst,
    int start,
    int nrows,
    int n_per_row,
    ffi.Pointer<ffi.Float> imatrix,
  ) {
    return _ggml_quantize_chunk(
      type.value,
      src,
      dst,
      start,
      nrows,
      n_per_row,
      imatrix,
    );
  }

  late final _ggml_quantize_chunkPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Size Function(
            ffi.UnsignedInt,
            ffi.Pointer<ffi.Float>,
            ffi.Pointer<ffi.Void>,
            ffi.Int64,
            ffi.Int64,
            ffi.Int64,
            ffi.Pointer<ffi.Float>,
          )
        >
      >('ggml_quantize_chunk');
  late final _ggml_quantize_chunk = _ggml_quantize_chunkPtr
      .asFunction<
        int Function(
          int,
          ffi.Pointer<ffi.Float>,
          ffi.Pointer<ffi.Void>,
          int,
          int,
          int,
          ffi.Pointer<ffi.Float>,
        )
      >();

  ffi.Pointer<ggml_type_traits> ggml_get_type_traits(ggml_type type) {
    return _ggml_get_type_traits(type.value);
  }

  late final _ggml_get_type_traitsPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_type_traits> Function(ffi.UnsignedInt)
        >
      >('ggml_get_type_traits');
  late final _ggml_get_type_traits = _ggml_get_type_traitsPtr
      .asFunction<ffi.Pointer<ggml_type_traits> Function(int)>();

  ggml_threadpool_params ggml_threadpool_params_default(int n_threads) {
    return _ggml_threadpool_params_default(n_threads);
  }

  late final _ggml_threadpool_params_defaultPtr =
      _lookup<ffi.NativeFunction<ggml_threadpool_params Function(ffi.Int)>>(
        'ggml_threadpool_params_default',
      );
  late final _ggml_threadpool_params_default =
      _ggml_threadpool_params_defaultPtr
          .asFunction<ggml_threadpool_params Function(int)>();

  void ggml_threadpool_params_init(
    ffi.Pointer<ggml_threadpool_params> p,
    int n_threads,
  ) {
    return _ggml_threadpool_params_init(p, n_threads);
  }

  late final _ggml_threadpool_params_initPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<ggml_threadpool_params>, ffi.Int)
        >
      >('ggml_threadpool_params_init');
  late final _ggml_threadpool_params_init = _ggml_threadpool_params_initPtr
      .asFunction<void Function(ffi.Pointer<ggml_threadpool_params>, int)>();

  bool ggml_threadpool_params_match(
    ffi.Pointer<ggml_threadpool_params> p0,
    ffi.Pointer<ggml_threadpool_params> p1,
  ) {
    return _ggml_threadpool_params_match(p0, p1);
  }

  late final _ggml_threadpool_params_matchPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Bool Function(
            ffi.Pointer<ggml_threadpool_params>,
            ffi.Pointer<ggml_threadpool_params>,
          )
        >
      >('ggml_threadpool_params_match');
  late final _ggml_threadpool_params_match = _ggml_threadpool_params_matchPtr
      .asFunction<
        bool Function(
          ffi.Pointer<ggml_threadpool_params>,
          ffi.Pointer<ggml_threadpool_params>,
        )
      >();

  ggml_tallocr ggml_tallocr_new(ggml_backend_buffer_t buffer) {
    return _ggml_tallocr_new(buffer);
  }

  late final _ggml_tallocr_newPtr =
      _lookup<ffi.NativeFunction<ggml_tallocr Function(ggml_backend_buffer_t)>>(
        'ggml_tallocr_new',
      );
  late final _ggml_tallocr_new = _ggml_tallocr_newPtr
      .asFunction<ggml_tallocr Function(ggml_backend_buffer_t)>();

  ggml_status ggml_tallocr_alloc(
    ffi.Pointer<ggml_tallocr> talloc,
    ffi.Pointer<ggml_tensor> tensor,
  ) {
    return ggml_status.fromValue(_ggml_tallocr_alloc(talloc, tensor));
  }

  late final _ggml_tallocr_allocPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Int Function(ffi.Pointer<ggml_tallocr>, ffi.Pointer<ggml_tensor>)
        >
      >('ggml_tallocr_alloc');
  late final _ggml_tallocr_alloc = _ggml_tallocr_allocPtr
      .asFunction<
        int Function(ffi.Pointer<ggml_tallocr>, ffi.Pointer<ggml_tensor>)
      >();

  ggml_gallocr_t ggml_gallocr_new(ggml_backend_buffer_type_t buft) {
    return _ggml_gallocr_new(buft);
  }

  late final _ggml_gallocr_newPtr =
      _lookup<
        ffi.NativeFunction<ggml_gallocr_t Function(ggml_backend_buffer_type_t)>
      >('ggml_gallocr_new');
  late final _ggml_gallocr_new = _ggml_gallocr_newPtr
      .asFunction<ggml_gallocr_t Function(ggml_backend_buffer_type_t)>();

  ggml_gallocr_t ggml_gallocr_new_n(
    ffi.Pointer<ggml_backend_buffer_type_t> bufts,
    int n_bufs,
  ) {
    return _ggml_gallocr_new_n(bufts, n_bufs);
  }

  late final _ggml_gallocr_new_nPtr =
      _lookup<
        ffi.NativeFunction<
          ggml_gallocr_t Function(
            ffi.Pointer<ggml_backend_buffer_type_t>,
            ffi.Int,
          )
        >
      >('ggml_gallocr_new_n');
  late final _ggml_gallocr_new_n = _ggml_gallocr_new_nPtr
      .asFunction<
        ggml_gallocr_t Function(ffi.Pointer<ggml_backend_buffer_type_t>, int)
      >();

  void ggml_gallocr_free(ggml_gallocr_t galloc) {
    return _ggml_gallocr_free(galloc);
  }

  late final _ggml_gallocr_freePtr =
      _lookup<ffi.NativeFunction<ffi.Void Function(ggml_gallocr_t)>>(
        'ggml_gallocr_free',
      );
  late final _ggml_gallocr_free = _ggml_gallocr_freePtr
      .asFunction<void Function(ggml_gallocr_t)>();

  /// pre-allocate buffers from a measure graph - does not allocate or modify the graph
  /// call with a worst-case graph to avoid buffer reallocations
  /// not strictly required for single buffer usage: ggml_gallocr_alloc_graph will reallocate the buffers automatically if needed
  /// returns false if the buffer allocation failed
  bool ggml_gallocr_reserve(
    ggml_gallocr_t galloc,
    ffi.Pointer<ggml_cgraph> graph,
  ) {
    return _ggml_gallocr_reserve(galloc, graph);
  }

  late final _ggml_gallocr_reservePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Bool Function(ggml_gallocr_t, ffi.Pointer<ggml_cgraph>)
        >
      >('ggml_gallocr_reserve');
  late final _ggml_gallocr_reserve = _ggml_gallocr_reservePtr
      .asFunction<bool Function(ggml_gallocr_t, ffi.Pointer<ggml_cgraph>)>();

  bool ggml_gallocr_reserve_n(
    ggml_gallocr_t galloc,
    ffi.Pointer<ggml_cgraph> graph,
    ffi.Pointer<ffi.Int> node_buffer_ids,
    ffi.Pointer<ffi.Int> leaf_buffer_ids,
  ) {
    return _ggml_gallocr_reserve_n(
      galloc,
      graph,
      node_buffer_ids,
      leaf_buffer_ids,
    );
  }

  late final _ggml_gallocr_reserve_nPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Bool Function(
            ggml_gallocr_t,
            ffi.Pointer<ggml_cgraph>,
            ffi.Pointer<ffi.Int>,
            ffi.Pointer<ffi.Int>,
          )
        >
      >('ggml_gallocr_reserve_n');
  late final _ggml_gallocr_reserve_n = _ggml_gallocr_reserve_nPtr
      .asFunction<
        bool Function(
          ggml_gallocr_t,
          ffi.Pointer<ggml_cgraph>,
          ffi.Pointer<ffi.Int>,
          ffi.Pointer<ffi.Int>,
        )
      >();

  /// automatic reallocation if the topology changes when using a single buffer
  /// returns false if using multiple buffers and a re-allocation is needed (call ggml_gallocr_reserve_n first to set the node buffers)
  bool ggml_gallocr_alloc_graph(
    ggml_gallocr_t galloc,
    ffi.Pointer<ggml_cgraph> graph,
  ) {
    return _ggml_gallocr_alloc_graph(galloc, graph);
  }

  late final _ggml_gallocr_alloc_graphPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Bool Function(ggml_gallocr_t, ffi.Pointer<ggml_cgraph>)
        >
      >('ggml_gallocr_alloc_graph');
  late final _ggml_gallocr_alloc_graph = _ggml_gallocr_alloc_graphPtr
      .asFunction<bool Function(ggml_gallocr_t, ffi.Pointer<ggml_cgraph>)>();

  int ggml_gallocr_get_buffer_size(ggml_gallocr_t galloc, int buffer_id) {
    return _ggml_gallocr_get_buffer_size(galloc, buffer_id);
  }

  late final _ggml_gallocr_get_buffer_sizePtr =
      _lookup<ffi.NativeFunction<ffi.Size Function(ggml_gallocr_t, ffi.Int)>>(
        'ggml_gallocr_get_buffer_size',
      );
  late final _ggml_gallocr_get_buffer_size = _ggml_gallocr_get_buffer_sizePtr
      .asFunction<int Function(ggml_gallocr_t, int)>();

  /// Utils
  /// Create a buffer and allocate all the tensors in a ggml_context
  ffi.Pointer<ggml_backend_buffer> ggml_backend_alloc_ctx_tensors_from_buft(
    ffi.Pointer<ggml_context> ctx,
    ggml_backend_buffer_type_t buft,
  ) {
    return _ggml_backend_alloc_ctx_tensors_from_buft(ctx, buft);
  }

  late final _ggml_backend_alloc_ctx_tensors_from_buftPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_backend_buffer> Function(
            ffi.Pointer<ggml_context>,
            ggml_backend_buffer_type_t,
          )
        >
      >('ggml_backend_alloc_ctx_tensors_from_buft');
  late final _ggml_backend_alloc_ctx_tensors_from_buft =
      _ggml_backend_alloc_ctx_tensors_from_buftPtr
          .asFunction<
            ffi.Pointer<ggml_backend_buffer> Function(
              ffi.Pointer<ggml_context>,
              ggml_backend_buffer_type_t,
            )
          >();

  ffi.Pointer<ggml_backend_buffer> ggml_backend_alloc_ctx_tensors(
    ffi.Pointer<ggml_context> ctx,
    ggml_backend_t backend,
  ) {
    return _ggml_backend_alloc_ctx_tensors(ctx, backend);
  }

  late final _ggml_backend_alloc_ctx_tensorsPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_backend_buffer> Function(
            ffi.Pointer<ggml_context>,
            ggml_backend_t,
          )
        >
      >('ggml_backend_alloc_ctx_tensors');
  late final _ggml_backend_alloc_ctx_tensors =
      _ggml_backend_alloc_ctx_tensorsPtr
          .asFunction<
            ffi.Pointer<ggml_backend_buffer> Function(
              ffi.Pointer<ggml_context>,
              ggml_backend_t,
            )
          >();

  /// Backend buffer type
  ffi.Pointer<ffi.Char> ggml_backend_buft_name(
    ggml_backend_buffer_type_t buft,
  ) {
    return _ggml_backend_buft_name(buft);
  }

  late final _ggml_backend_buft_namePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ffi.Char> Function(ggml_backend_buffer_type_t)
        >
      >('ggml_backend_buft_name');
  late final _ggml_backend_buft_name = _ggml_backend_buft_namePtr
      .asFunction<ffi.Pointer<ffi.Char> Function(ggml_backend_buffer_type_t)>();

  ggml_backend_buffer_t ggml_backend_buft_alloc_buffer(
    ggml_backend_buffer_type_t buft,
    int size,
  ) {
    return _ggml_backend_buft_alloc_buffer(buft, size);
  }

  late final _ggml_backend_buft_alloc_bufferPtr =
      _lookup<
        ffi.NativeFunction<
          ggml_backend_buffer_t Function(ggml_backend_buffer_type_t, ffi.Size)
        >
      >('ggml_backend_buft_alloc_buffer');
  late final _ggml_backend_buft_alloc_buffer =
      _ggml_backend_buft_alloc_bufferPtr
          .asFunction<
            ggml_backend_buffer_t Function(ggml_backend_buffer_type_t, int)
          >();

  int ggml_backend_buft_get_alignment(ggml_backend_buffer_type_t buft) {
    return _ggml_backend_buft_get_alignment(buft);
  }

  late final _ggml_backend_buft_get_alignmentPtr =
      _lookup<
        ffi.NativeFunction<ffi.Size Function(ggml_backend_buffer_type_t)>
      >('ggml_backend_buft_get_alignment');
  late final _ggml_backend_buft_get_alignment =
      _ggml_backend_buft_get_alignmentPtr
          .asFunction<int Function(ggml_backend_buffer_type_t)>();

  int ggml_backend_buft_get_max_size(ggml_backend_buffer_type_t buft) {
    return _ggml_backend_buft_get_max_size(buft);
  }

  late final _ggml_backend_buft_get_max_sizePtr =
      _lookup<
        ffi.NativeFunction<ffi.Size Function(ggml_backend_buffer_type_t)>
      >('ggml_backend_buft_get_max_size');
  late final _ggml_backend_buft_get_max_size =
      _ggml_backend_buft_get_max_sizePtr
          .asFunction<int Function(ggml_backend_buffer_type_t)>();

  int ggml_backend_buft_get_alloc_size(
    ggml_backend_buffer_type_t buft,
    ffi.Pointer<ggml_tensor> tensor,
  ) {
    return _ggml_backend_buft_get_alloc_size(buft, tensor);
  }

  late final _ggml_backend_buft_get_alloc_sizePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Size Function(
            ggml_backend_buffer_type_t,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_backend_buft_get_alloc_size');
  late final _ggml_backend_buft_get_alloc_size =
      _ggml_backend_buft_get_alloc_sizePtr
          .asFunction<
            int Function(ggml_backend_buffer_type_t, ffi.Pointer<ggml_tensor>)
          >();

  bool ggml_backend_buft_is_host(ggml_backend_buffer_type_t buft) {
    return _ggml_backend_buft_is_host(buft);
  }

  late final _ggml_backend_buft_is_hostPtr =
      _lookup<
        ffi.NativeFunction<ffi.Bool Function(ggml_backend_buffer_type_t)>
      >('ggml_backend_buft_is_host');
  late final _ggml_backend_buft_is_host = _ggml_backend_buft_is_hostPtr
      .asFunction<bool Function(ggml_backend_buffer_type_t)>();

  ggml_backend_dev_t ggml_backend_buft_get_device(
    ggml_backend_buffer_type_t buft,
  ) {
    return _ggml_backend_buft_get_device(buft);
  }

  late final _ggml_backend_buft_get_devicePtr =
      _lookup<
        ffi.NativeFunction<
          ggml_backend_dev_t Function(ggml_backend_buffer_type_t)
        >
      >('ggml_backend_buft_get_device');
  late final _ggml_backend_buft_get_device = _ggml_backend_buft_get_devicePtr
      .asFunction<ggml_backend_dev_t Function(ggml_backend_buffer_type_t)>();

  ffi.Pointer<ffi.Char> ggml_backend_buffer_name(ggml_backend_buffer_t buffer) {
    return _ggml_backend_buffer_name(buffer);
  }

  late final _ggml_backend_buffer_namePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ffi.Char> Function(ggml_backend_buffer_t)
        >
      >('ggml_backend_buffer_name');
  late final _ggml_backend_buffer_name = _ggml_backend_buffer_namePtr
      .asFunction<ffi.Pointer<ffi.Char> Function(ggml_backend_buffer_t)>();

  void ggml_backend_buffer_free(ggml_backend_buffer_t buffer) {
    return _ggml_backend_buffer_free(buffer);
  }

  late final _ggml_backend_buffer_freePtr =
      _lookup<ffi.NativeFunction<ffi.Void Function(ggml_backend_buffer_t)>>(
        'ggml_backend_buffer_free',
      );
  late final _ggml_backend_buffer_free = _ggml_backend_buffer_freePtr
      .asFunction<void Function(ggml_backend_buffer_t)>();

  ffi.Pointer<ffi.Void> ggml_backend_buffer_get_base(
    ggml_backend_buffer_t buffer,
  ) {
    return _ggml_backend_buffer_get_base(buffer);
  }

  late final _ggml_backend_buffer_get_basePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ffi.Void> Function(ggml_backend_buffer_t)
        >
      >('ggml_backend_buffer_get_base');
  late final _ggml_backend_buffer_get_base = _ggml_backend_buffer_get_basePtr
      .asFunction<ffi.Pointer<ffi.Void> Function(ggml_backend_buffer_t)>();

  int ggml_backend_buffer_get_size(ggml_backend_buffer_t buffer) {
    return _ggml_backend_buffer_get_size(buffer);
  }

  late final _ggml_backend_buffer_get_sizePtr =
      _lookup<ffi.NativeFunction<ffi.Size Function(ggml_backend_buffer_t)>>(
        'ggml_backend_buffer_get_size',
      );
  late final _ggml_backend_buffer_get_size = _ggml_backend_buffer_get_sizePtr
      .asFunction<int Function(ggml_backend_buffer_t)>();

  ggml_status ggml_backend_buffer_init_tensor(
    ggml_backend_buffer_t buffer,
    ffi.Pointer<ggml_tensor> tensor,
  ) {
    return ggml_status.fromValue(
      _ggml_backend_buffer_init_tensor(buffer, tensor),
    );
  }

  late final _ggml_backend_buffer_init_tensorPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Int Function(ggml_backend_buffer_t, ffi.Pointer<ggml_tensor>)
        >
      >('ggml_backend_buffer_init_tensor');
  late final _ggml_backend_buffer_init_tensor =
      _ggml_backend_buffer_init_tensorPtr
          .asFunction<
            int Function(ggml_backend_buffer_t, ffi.Pointer<ggml_tensor>)
          >();

  int ggml_backend_buffer_get_alignment(ggml_backend_buffer_t buffer) {
    return _ggml_backend_buffer_get_alignment(buffer);
  }

  late final _ggml_backend_buffer_get_alignmentPtr =
      _lookup<ffi.NativeFunction<ffi.Size Function(ggml_backend_buffer_t)>>(
        'ggml_backend_buffer_get_alignment',
      );
  late final _ggml_backend_buffer_get_alignment =
      _ggml_backend_buffer_get_alignmentPtr
          .asFunction<int Function(ggml_backend_buffer_t)>();

  int ggml_backend_buffer_get_max_size(ggml_backend_buffer_t buffer) {
    return _ggml_backend_buffer_get_max_size(buffer);
  }

  late final _ggml_backend_buffer_get_max_sizePtr =
      _lookup<ffi.NativeFunction<ffi.Size Function(ggml_backend_buffer_t)>>(
        'ggml_backend_buffer_get_max_size',
      );
  late final _ggml_backend_buffer_get_max_size =
      _ggml_backend_buffer_get_max_sizePtr
          .asFunction<int Function(ggml_backend_buffer_t)>();

  int ggml_backend_buffer_get_alloc_size(
    ggml_backend_buffer_t buffer,
    ffi.Pointer<ggml_tensor> tensor,
  ) {
    return _ggml_backend_buffer_get_alloc_size(buffer, tensor);
  }

  late final _ggml_backend_buffer_get_alloc_sizePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Size Function(ggml_backend_buffer_t, ffi.Pointer<ggml_tensor>)
        >
      >('ggml_backend_buffer_get_alloc_size');
  late final _ggml_backend_buffer_get_alloc_size =
      _ggml_backend_buffer_get_alloc_sizePtr
          .asFunction<
            int Function(ggml_backend_buffer_t, ffi.Pointer<ggml_tensor>)
          >();

  void ggml_backend_buffer_clear(ggml_backend_buffer_t buffer, int value) {
    return _ggml_backend_buffer_clear(buffer, value);
  }

  late final _ggml_backend_buffer_clearPtr =
      _lookup<
        ffi.NativeFunction<ffi.Void Function(ggml_backend_buffer_t, ffi.Uint8)>
      >('ggml_backend_buffer_clear');
  late final _ggml_backend_buffer_clear = _ggml_backend_buffer_clearPtr
      .asFunction<void Function(ggml_backend_buffer_t, int)>();

  bool ggml_backend_buffer_is_host(ggml_backend_buffer_t buffer) {
    return _ggml_backend_buffer_is_host(buffer);
  }

  late final _ggml_backend_buffer_is_hostPtr =
      _lookup<ffi.NativeFunction<ffi.Bool Function(ggml_backend_buffer_t)>>(
        'ggml_backend_buffer_is_host',
      );
  late final _ggml_backend_buffer_is_host = _ggml_backend_buffer_is_hostPtr
      .asFunction<bool Function(ggml_backend_buffer_t)>();

  void ggml_backend_buffer_set_usage(
    ggml_backend_buffer_t buffer,
    ggml_backend_buffer_usage usage,
  ) {
    return _ggml_backend_buffer_set_usage(buffer, usage.value);
  }

  late final _ggml_backend_buffer_set_usagePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Void Function(ggml_backend_buffer_t, ffi.UnsignedInt)
        >
      >('ggml_backend_buffer_set_usage');
  late final _ggml_backend_buffer_set_usage = _ggml_backend_buffer_set_usagePtr
      .asFunction<void Function(ggml_backend_buffer_t, int)>();

  ggml_backend_buffer_usage ggml_backend_buffer_get_usage(
    ggml_backend_buffer_t buffer,
  ) {
    return ggml_backend_buffer_usage.fromValue(
      _ggml_backend_buffer_get_usage(buffer),
    );
  }

  late final _ggml_backend_buffer_get_usagePtr =
      _lookup<
        ffi.NativeFunction<ffi.UnsignedInt Function(ggml_backend_buffer_t)>
      >('ggml_backend_buffer_get_usage');
  late final _ggml_backend_buffer_get_usage = _ggml_backend_buffer_get_usagePtr
      .asFunction<int Function(ggml_backend_buffer_t)>();

  ggml_backend_buffer_type_t ggml_backend_buffer_get_type(
    ggml_backend_buffer_t buffer,
  ) {
    return _ggml_backend_buffer_get_type(buffer);
  }

  late final _ggml_backend_buffer_get_typePtr =
      _lookup<
        ffi.NativeFunction<
          ggml_backend_buffer_type_t Function(ggml_backend_buffer_t)
        >
      >('ggml_backend_buffer_get_type');
  late final _ggml_backend_buffer_get_type = _ggml_backend_buffer_get_typePtr
      .asFunction<ggml_backend_buffer_type_t Function(ggml_backend_buffer_t)>();

  void ggml_backend_buffer_reset(ggml_backend_buffer_t buffer) {
    return _ggml_backend_buffer_reset(buffer);
  }

  late final _ggml_backend_buffer_resetPtr =
      _lookup<ffi.NativeFunction<ffi.Void Function(ggml_backend_buffer_t)>>(
        'ggml_backend_buffer_reset',
      );
  late final _ggml_backend_buffer_reset = _ggml_backend_buffer_resetPtr
      .asFunction<void Function(ggml_backend_buffer_t)>();

  /// tensor copy between different backends
  void ggml_backend_tensor_copy(
    ffi.Pointer<ggml_tensor> src,
    ffi.Pointer<ggml_tensor> dst,
  ) {
    return _ggml_backend_tensor_copy(src, dst);
  }

  late final _ggml_backend_tensor_copyPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<ggml_tensor>, ffi.Pointer<ggml_tensor>)
        >
      >('ggml_backend_tensor_copy');
  late final _ggml_backend_tensor_copy = _ggml_backend_tensor_copyPtr
      .asFunction<
        void Function(ffi.Pointer<ggml_tensor>, ffi.Pointer<ggml_tensor>)
      >();

  /// Backend (stream)
  ggml_guid_t ggml_backend_guid(ggml_backend_t backend) {
    return _ggml_backend_guid(backend);
  }

  late final _ggml_backend_guidPtr =
      _lookup<ffi.NativeFunction<ggml_guid_t Function(ggml_backend_t)>>(
        'ggml_backend_guid',
      );
  late final _ggml_backend_guid = _ggml_backend_guidPtr
      .asFunction<ggml_guid_t Function(ggml_backend_t)>();

  ffi.Pointer<ffi.Char> ggml_backend_name(ggml_backend_t backend) {
    return _ggml_backend_name(backend);
  }

  late final _ggml_backend_namePtr =
      _lookup<
        ffi.NativeFunction<ffi.Pointer<ffi.Char> Function(ggml_backend_t)>
      >('ggml_backend_name');
  late final _ggml_backend_name = _ggml_backend_namePtr
      .asFunction<ffi.Pointer<ffi.Char> Function(ggml_backend_t)>();

  void ggml_backend_free(ggml_backend_t backend) {
    return _ggml_backend_free(backend);
  }

  late final _ggml_backend_freePtr =
      _lookup<ffi.NativeFunction<ffi.Void Function(ggml_backend_t)>>(
        'ggml_backend_free',
      );
  late final _ggml_backend_free = _ggml_backend_freePtr
      .asFunction<void Function(ggml_backend_t)>();

  ggml_backend_buffer_type_t ggml_backend_get_default_buffer_type(
    ggml_backend_t backend,
  ) {
    return _ggml_backend_get_default_buffer_type(backend);
  }

  late final _ggml_backend_get_default_buffer_typePtr =
      _lookup<
        ffi.NativeFunction<ggml_backend_buffer_type_t Function(ggml_backend_t)>
      >('ggml_backend_get_default_buffer_type');
  late final _ggml_backend_get_default_buffer_type =
      _ggml_backend_get_default_buffer_typePtr
          .asFunction<ggml_backend_buffer_type_t Function(ggml_backend_t)>();

  ggml_backend_buffer_t ggml_backend_alloc_buffer(
    ggml_backend_t backend,
    int size,
  ) {
    return _ggml_backend_alloc_buffer(backend, size);
  }

  late final _ggml_backend_alloc_bufferPtr =
      _lookup<
        ffi.NativeFunction<
          ggml_backend_buffer_t Function(ggml_backend_t, ffi.Size)
        >
      >('ggml_backend_alloc_buffer');
  late final _ggml_backend_alloc_buffer = _ggml_backend_alloc_bufferPtr
      .asFunction<ggml_backend_buffer_t Function(ggml_backend_t, int)>();

  int ggml_backend_get_alignment(ggml_backend_t backend) {
    return _ggml_backend_get_alignment(backend);
  }

  late final _ggml_backend_get_alignmentPtr =
      _lookup<ffi.NativeFunction<ffi.Size Function(ggml_backend_t)>>(
        'ggml_backend_get_alignment',
      );
  late final _ggml_backend_get_alignment = _ggml_backend_get_alignmentPtr
      .asFunction<int Function(ggml_backend_t)>();

  int ggml_backend_get_max_size(ggml_backend_t backend) {
    return _ggml_backend_get_max_size(backend);
  }

  late final _ggml_backend_get_max_sizePtr =
      _lookup<ffi.NativeFunction<ffi.Size Function(ggml_backend_t)>>(
        'ggml_backend_get_max_size',
      );
  late final _ggml_backend_get_max_size = _ggml_backend_get_max_sizePtr
      .asFunction<int Function(ggml_backend_t)>();

  void ggml_backend_tensor_set_async(
    ggml_backend_t backend,
    ffi.Pointer<ggml_tensor> tensor,
    ffi.Pointer<ffi.Void> data,
    int offset,
    int size,
  ) {
    return _ggml_backend_tensor_set_async(backend, tensor, data, offset, size);
  }

  late final _ggml_backend_tensor_set_asyncPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Void Function(
            ggml_backend_t,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ffi.Void>,
            ffi.Size,
            ffi.Size,
          )
        >
      >('ggml_backend_tensor_set_async');
  late final _ggml_backend_tensor_set_async = _ggml_backend_tensor_set_asyncPtr
      .asFunction<
        void Function(
          ggml_backend_t,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ffi.Void>,
          int,
          int,
        )
      >();

  void ggml_backend_tensor_get_async(
    ggml_backend_t backend,
    ffi.Pointer<ggml_tensor> tensor,
    ffi.Pointer<ffi.Void> data,
    int offset,
    int size,
  ) {
    return _ggml_backend_tensor_get_async(backend, tensor, data, offset, size);
  }

  late final _ggml_backend_tensor_get_asyncPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Void Function(
            ggml_backend_t,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ffi.Void>,
            ffi.Size,
            ffi.Size,
          )
        >
      >('ggml_backend_tensor_get_async');
  late final _ggml_backend_tensor_get_async = _ggml_backend_tensor_get_asyncPtr
      .asFunction<
        void Function(
          ggml_backend_t,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ffi.Void>,
          int,
          int,
        )
      >();

  /// "offset" refers to the offset in tensor->data for setting/getting data
  void ggml_backend_tensor_set(
    ffi.Pointer<ggml_tensor> tensor,
    ffi.Pointer<ffi.Void> data,
    int offset,
    int size,
  ) {
    return _ggml_backend_tensor_set(tensor, data, offset, size);
  }

  late final _ggml_backend_tensor_setPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Void Function(
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ffi.Void>,
            ffi.Size,
            ffi.Size,
          )
        >
      >('ggml_backend_tensor_set');
  late final _ggml_backend_tensor_set = _ggml_backend_tensor_setPtr
      .asFunction<
        void Function(ffi.Pointer<ggml_tensor>, ffi.Pointer<ffi.Void>, int, int)
      >();

  void ggml_backend_tensor_get(
    ffi.Pointer<ggml_tensor> tensor,
    ffi.Pointer<ffi.Void> data,
    int offset,
    int size,
  ) {
    return _ggml_backend_tensor_get(tensor, data, offset, size);
  }

  late final _ggml_backend_tensor_getPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Void Function(
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ffi.Void>,
            ffi.Size,
            ffi.Size,
          )
        >
      >('ggml_backend_tensor_get');
  late final _ggml_backend_tensor_get = _ggml_backend_tensor_getPtr
      .asFunction<
        void Function(ffi.Pointer<ggml_tensor>, ffi.Pointer<ffi.Void>, int, int)
      >();

  void ggml_backend_tensor_memset(
    ffi.Pointer<ggml_tensor> tensor,
    int value,
    int offset,
    int size,
  ) {
    return _ggml_backend_tensor_memset(tensor, value, offset, size);
  }

  late final _ggml_backend_tensor_memsetPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Void Function(
            ffi.Pointer<ggml_tensor>,
            ffi.Uint8,
            ffi.Size,
            ffi.Size,
          )
        >
      >('ggml_backend_tensor_memset');
  late final _ggml_backend_tensor_memset = _ggml_backend_tensor_memsetPtr
      .asFunction<void Function(ffi.Pointer<ggml_tensor>, int, int, int)>();

  void ggml_backend_synchronize(ggml_backend_t backend) {
    return _ggml_backend_synchronize(backend);
  }

  late final _ggml_backend_synchronizePtr =
      _lookup<ffi.NativeFunction<ffi.Void Function(ggml_backend_t)>>(
        'ggml_backend_synchronize',
      );
  late final _ggml_backend_synchronize = _ggml_backend_synchronizePtr
      .asFunction<void Function(ggml_backend_t)>();

  ggml_backend_graph_plan_t ggml_backend_graph_plan_create(
    ggml_backend_t backend,
    ffi.Pointer<ggml_cgraph> cgraph,
  ) {
    return _ggml_backend_graph_plan_create(backend, cgraph);
  }

  late final _ggml_backend_graph_plan_createPtr =
      _lookup<
        ffi.NativeFunction<
          ggml_backend_graph_plan_t Function(
            ggml_backend_t,
            ffi.Pointer<ggml_cgraph>,
          )
        >
      >('ggml_backend_graph_plan_create');
  late final _ggml_backend_graph_plan_create =
      _ggml_backend_graph_plan_createPtr
          .asFunction<
            ggml_backend_graph_plan_t Function(
              ggml_backend_t,
              ffi.Pointer<ggml_cgraph>,
            )
          >();

  void ggml_backend_graph_plan_free(
    ggml_backend_t backend,
    ggml_backend_graph_plan_t plan,
  ) {
    return _ggml_backend_graph_plan_free(backend, plan);
  }

  late final _ggml_backend_graph_plan_freePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Void Function(ggml_backend_t, ggml_backend_graph_plan_t)
        >
      >('ggml_backend_graph_plan_free');
  late final _ggml_backend_graph_plan_free = _ggml_backend_graph_plan_freePtr
      .asFunction<void Function(ggml_backend_t, ggml_backend_graph_plan_t)>();

  ggml_status ggml_backend_graph_plan_compute(
    ggml_backend_t backend,
    ggml_backend_graph_plan_t plan,
  ) {
    return ggml_status.fromValue(
      _ggml_backend_graph_plan_compute(backend, plan),
    );
  }

  late final _ggml_backend_graph_plan_computePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Int Function(ggml_backend_t, ggml_backend_graph_plan_t)
        >
      >('ggml_backend_graph_plan_compute');
  late final _ggml_backend_graph_plan_compute =
      _ggml_backend_graph_plan_computePtr
          .asFunction<
            int Function(ggml_backend_t, ggml_backend_graph_plan_t)
          >();

  ggml_status ggml_backend_graph_compute(
    ggml_backend_t backend,
    ffi.Pointer<ggml_cgraph> cgraph,
  ) {
    return ggml_status.fromValue(_ggml_backend_graph_compute(backend, cgraph));
  }

  late final _ggml_backend_graph_computePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Int Function(ggml_backend_t, ffi.Pointer<ggml_cgraph>)
        >
      >('ggml_backend_graph_compute');
  late final _ggml_backend_graph_compute = _ggml_backend_graph_computePtr
      .asFunction<int Function(ggml_backend_t, ffi.Pointer<ggml_cgraph>)>();

  ggml_status ggml_backend_graph_compute_async(
    ggml_backend_t backend,
    ffi.Pointer<ggml_cgraph> cgraph,
  ) {
    return ggml_status.fromValue(
      _ggml_backend_graph_compute_async(backend, cgraph),
    );
  }

  late final _ggml_backend_graph_compute_asyncPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Int Function(ggml_backend_t, ffi.Pointer<ggml_cgraph>)
        >
      >('ggml_backend_graph_compute_async');
  late final _ggml_backend_graph_compute_async =
      _ggml_backend_graph_compute_asyncPtr
          .asFunction<int Function(ggml_backend_t, ffi.Pointer<ggml_cgraph>)>();

  /// NOTE: will be removed, use device version instead
  bool ggml_backend_supports_op(
    ggml_backend_t backend,
    ffi.Pointer<ggml_tensor> op,
  ) {
    return _ggml_backend_supports_op(backend, op);
  }

  late final _ggml_backend_supports_opPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Bool Function(ggml_backend_t, ffi.Pointer<ggml_tensor>)
        >
      >('ggml_backend_supports_op');
  late final _ggml_backend_supports_op = _ggml_backend_supports_opPtr
      .asFunction<bool Function(ggml_backend_t, ffi.Pointer<ggml_tensor>)>();

  bool ggml_backend_supports_buft(
    ggml_backend_t backend,
    ggml_backend_buffer_type_t buft,
  ) {
    return _ggml_backend_supports_buft(backend, buft);
  }

  late final _ggml_backend_supports_buftPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Bool Function(ggml_backend_t, ggml_backend_buffer_type_t)
        >
      >('ggml_backend_supports_buft');
  late final _ggml_backend_supports_buft = _ggml_backend_supports_buftPtr
      .asFunction<bool Function(ggml_backend_t, ggml_backend_buffer_type_t)>();

  bool ggml_backend_offload_op(
    ggml_backend_t backend,
    ffi.Pointer<ggml_tensor> op,
  ) {
    return _ggml_backend_offload_op(backend, op);
  }

  late final _ggml_backend_offload_opPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Bool Function(ggml_backend_t, ffi.Pointer<ggml_tensor>)
        >
      >('ggml_backend_offload_op');
  late final _ggml_backend_offload_op = _ggml_backend_offload_opPtr
      .asFunction<bool Function(ggml_backend_t, ffi.Pointer<ggml_tensor>)>();

  /// asynchronous copy
  /// the copy is performed after all the currently queued operations in backend_src
  /// backend_dst will wait for the copy to complete before performing other operations
  /// automatic fallback to sync copy if async is not supported
  void ggml_backend_tensor_copy_async(
    ggml_backend_t backend_src,
    ggml_backend_t backend_dst,
    ffi.Pointer<ggml_tensor> src,
    ffi.Pointer<ggml_tensor> dst,
  ) {
    return _ggml_backend_tensor_copy_async(backend_src, backend_dst, src, dst);
  }

  late final _ggml_backend_tensor_copy_asyncPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Void Function(
            ggml_backend_t,
            ggml_backend_t,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_backend_tensor_copy_async');
  late final _ggml_backend_tensor_copy_async =
      _ggml_backend_tensor_copy_asyncPtr
          .asFunction<
            void Function(
              ggml_backend_t,
              ggml_backend_t,
              ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ggml_tensor>,
            )
          >();

  ggml_backend_dev_t ggml_backend_get_device(ggml_backend_t backend) {
    return _ggml_backend_get_device(backend);
  }

  late final _ggml_backend_get_devicePtr =
      _lookup<ffi.NativeFunction<ggml_backend_dev_t Function(ggml_backend_t)>>(
        'ggml_backend_get_device',
      );
  late final _ggml_backend_get_device = _ggml_backend_get_devicePtr
      .asFunction<ggml_backend_dev_t Function(ggml_backend_t)>();

  /// Events
  ggml_backend_event_t ggml_backend_event_new(ggml_backend_dev_t device) {
    return _ggml_backend_event_new(device);
  }

  late final _ggml_backend_event_newPtr =
      _lookup<
        ffi.NativeFunction<ggml_backend_event_t Function(ggml_backend_dev_t)>
      >('ggml_backend_event_new');
  late final _ggml_backend_event_new = _ggml_backend_event_newPtr
      .asFunction<ggml_backend_event_t Function(ggml_backend_dev_t)>();

  void ggml_backend_event_free(ggml_backend_event_t event) {
    return _ggml_backend_event_free(event);
  }

  late final _ggml_backend_event_freePtr =
      _lookup<ffi.NativeFunction<ffi.Void Function(ggml_backend_event_t)>>(
        'ggml_backend_event_free',
      );
  late final _ggml_backend_event_free = _ggml_backend_event_freePtr
      .asFunction<void Function(ggml_backend_event_t)>();

  void ggml_backend_event_record(
    ggml_backend_event_t event,
    ggml_backend_t backend,
  ) {
    return _ggml_backend_event_record(event, backend);
  }

  late final _ggml_backend_event_recordPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Void Function(ggml_backend_event_t, ggml_backend_t)
        >
      >('ggml_backend_event_record');
  late final _ggml_backend_event_record = _ggml_backend_event_recordPtr
      .asFunction<void Function(ggml_backend_event_t, ggml_backend_t)>();

  void ggml_backend_event_synchronize(ggml_backend_event_t event) {
    return _ggml_backend_event_synchronize(event);
  }

  late final _ggml_backend_event_synchronizePtr =
      _lookup<ffi.NativeFunction<ffi.Void Function(ggml_backend_event_t)>>(
        'ggml_backend_event_synchronize',
      );
  late final _ggml_backend_event_synchronize =
      _ggml_backend_event_synchronizePtr
          .asFunction<void Function(ggml_backend_event_t)>();

  void ggml_backend_event_wait(
    ggml_backend_t backend,
    ggml_backend_event_t event,
  ) {
    return _ggml_backend_event_wait(backend, event);
  }

  late final _ggml_backend_event_waitPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Void Function(ggml_backend_t, ggml_backend_event_t)
        >
      >('ggml_backend_event_wait');
  late final _ggml_backend_event_wait = _ggml_backend_event_waitPtr
      .asFunction<void Function(ggml_backend_t, ggml_backend_event_t)>();

  ffi.Pointer<ffi.Char> ggml_backend_dev_name(ggml_backend_dev_t device) {
    return _ggml_backend_dev_name(device);
  }

  late final _ggml_backend_dev_namePtr =
      _lookup<
        ffi.NativeFunction<ffi.Pointer<ffi.Char> Function(ggml_backend_dev_t)>
      >('ggml_backend_dev_name');
  late final _ggml_backend_dev_name = _ggml_backend_dev_namePtr
      .asFunction<ffi.Pointer<ffi.Char> Function(ggml_backend_dev_t)>();

  ffi.Pointer<ffi.Char> ggml_backend_dev_description(
    ggml_backend_dev_t device,
  ) {
    return _ggml_backend_dev_description(device);
  }

  late final _ggml_backend_dev_descriptionPtr =
      _lookup<
        ffi.NativeFunction<ffi.Pointer<ffi.Char> Function(ggml_backend_dev_t)>
      >('ggml_backend_dev_description');
  late final _ggml_backend_dev_description = _ggml_backend_dev_descriptionPtr
      .asFunction<ffi.Pointer<ffi.Char> Function(ggml_backend_dev_t)>();

  void ggml_backend_dev_memory(
    ggml_backend_dev_t device,
    ffi.Pointer<ffi.Size> free,
    ffi.Pointer<ffi.Size> total,
  ) {
    return _ggml_backend_dev_memory(device, free, total);
  }

  late final _ggml_backend_dev_memoryPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Void Function(
            ggml_backend_dev_t,
            ffi.Pointer<ffi.Size>,
            ffi.Pointer<ffi.Size>,
          )
        >
      >('ggml_backend_dev_memory');
  late final _ggml_backend_dev_memory = _ggml_backend_dev_memoryPtr
      .asFunction<
        void Function(
          ggml_backend_dev_t,
          ffi.Pointer<ffi.Size>,
          ffi.Pointer<ffi.Size>,
        )
      >();

  ggml_backend_dev_type ggml_backend_dev_type$1(ggml_backend_dev_t device) {
    return ggml_backend_dev_type.fromValue(_ggml_backend_dev_type$1(device));
  }

  late final _ggml_backend_dev_type$1Ptr =
      _lookup<ffi.NativeFunction<ffi.UnsignedInt Function(ggml_backend_dev_t)>>(
        'ggml_backend_dev_type',
      );
  late final _ggml_backend_dev_type$1 = _ggml_backend_dev_type$1Ptr
      .asFunction<int Function(ggml_backend_dev_t)>();

  void ggml_backend_dev_get_props(
    ggml_backend_dev_t device,
    ffi.Pointer<ggml_backend_dev_props> props,
  ) {
    return _ggml_backend_dev_get_props(device, props);
  }

  late final _ggml_backend_dev_get_propsPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Void Function(
            ggml_backend_dev_t,
            ffi.Pointer<ggml_backend_dev_props>,
          )
        >
      >('ggml_backend_dev_get_props');
  late final _ggml_backend_dev_get_props = _ggml_backend_dev_get_propsPtr
      .asFunction<
        void Function(ggml_backend_dev_t, ffi.Pointer<ggml_backend_dev_props>)
      >();

  ggml_backend_reg_t ggml_backend_dev_backend_reg(ggml_backend_dev_t device) {
    return _ggml_backend_dev_backend_reg(device);
  }

  late final _ggml_backend_dev_backend_regPtr =
      _lookup<
        ffi.NativeFunction<ggml_backend_reg_t Function(ggml_backend_dev_t)>
      >('ggml_backend_dev_backend_reg');
  late final _ggml_backend_dev_backend_reg = _ggml_backend_dev_backend_regPtr
      .asFunction<ggml_backend_reg_t Function(ggml_backend_dev_t)>();

  ggml_backend_t ggml_backend_dev_init(
    ggml_backend_dev_t device,
    ffi.Pointer<ffi.Char> params,
  ) {
    return _ggml_backend_dev_init(device, params);
  }

  late final _ggml_backend_dev_initPtr =
      _lookup<
        ffi.NativeFunction<
          ggml_backend_t Function(ggml_backend_dev_t, ffi.Pointer<ffi.Char>)
        >
      >('ggml_backend_dev_init');
  late final _ggml_backend_dev_init = _ggml_backend_dev_initPtr
      .asFunction<
        ggml_backend_t Function(ggml_backend_dev_t, ffi.Pointer<ffi.Char>)
      >();

  ggml_backend_buffer_type_t ggml_backend_dev_buffer_type(
    ggml_backend_dev_t device,
  ) {
    return _ggml_backend_dev_buffer_type(device);
  }

  late final _ggml_backend_dev_buffer_typePtr =
      _lookup<
        ffi.NativeFunction<
          ggml_backend_buffer_type_t Function(ggml_backend_dev_t)
        >
      >('ggml_backend_dev_buffer_type');
  late final _ggml_backend_dev_buffer_type = _ggml_backend_dev_buffer_typePtr
      .asFunction<ggml_backend_buffer_type_t Function(ggml_backend_dev_t)>();

  ggml_backend_buffer_type_t ggml_backend_dev_host_buffer_type(
    ggml_backend_dev_t device,
  ) {
    return _ggml_backend_dev_host_buffer_type(device);
  }

  late final _ggml_backend_dev_host_buffer_typePtr =
      _lookup<
        ffi.NativeFunction<
          ggml_backend_buffer_type_t Function(ggml_backend_dev_t)
        >
      >('ggml_backend_dev_host_buffer_type');
  late final _ggml_backend_dev_host_buffer_type =
      _ggml_backend_dev_host_buffer_typePtr
          .asFunction<
            ggml_backend_buffer_type_t Function(ggml_backend_dev_t)
          >();

  ggml_backend_buffer_t ggml_backend_dev_buffer_from_host_ptr(
    ggml_backend_dev_t device,
    ffi.Pointer<ffi.Void> ptr,
    int size,
    int max_tensor_size,
  ) {
    return _ggml_backend_dev_buffer_from_host_ptr(
      device,
      ptr,
      size,
      max_tensor_size,
    );
  }

  late final _ggml_backend_dev_buffer_from_host_ptrPtr =
      _lookup<
        ffi.NativeFunction<
          ggml_backend_buffer_t Function(
            ggml_backend_dev_t,
            ffi.Pointer<ffi.Void>,
            ffi.Size,
            ffi.Size,
          )
        >
      >('ggml_backend_dev_buffer_from_host_ptr');
  late final _ggml_backend_dev_buffer_from_host_ptr =
      _ggml_backend_dev_buffer_from_host_ptrPtr
          .asFunction<
            ggml_backend_buffer_t Function(
              ggml_backend_dev_t,
              ffi.Pointer<ffi.Void>,
              int,
              int,
            )
          >();

  bool ggml_backend_dev_supports_op(
    ggml_backend_dev_t device,
    ffi.Pointer<ggml_tensor> op,
  ) {
    return _ggml_backend_dev_supports_op(device, op);
  }

  late final _ggml_backend_dev_supports_opPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Bool Function(ggml_backend_dev_t, ffi.Pointer<ggml_tensor>)
        >
      >('ggml_backend_dev_supports_op');
  late final _ggml_backend_dev_supports_op = _ggml_backend_dev_supports_opPtr
      .asFunction<
        bool Function(ggml_backend_dev_t, ffi.Pointer<ggml_tensor>)
      >();

  bool ggml_backend_dev_supports_buft(
    ggml_backend_dev_t device,
    ggml_backend_buffer_type_t buft,
  ) {
    return _ggml_backend_dev_supports_buft(device, buft);
  }

  late final _ggml_backend_dev_supports_buftPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Bool Function(ggml_backend_dev_t, ggml_backend_buffer_type_t)
        >
      >('ggml_backend_dev_supports_buft');
  late final _ggml_backend_dev_supports_buft =
      _ggml_backend_dev_supports_buftPtr
          .asFunction<
            bool Function(ggml_backend_dev_t, ggml_backend_buffer_type_t)
          >();

  bool ggml_backend_dev_offload_op(
    ggml_backend_dev_t device,
    ffi.Pointer<ggml_tensor> op,
  ) {
    return _ggml_backend_dev_offload_op(device, op);
  }

  late final _ggml_backend_dev_offload_opPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Bool Function(ggml_backend_dev_t, ffi.Pointer<ggml_tensor>)
        >
      >('ggml_backend_dev_offload_op');
  late final _ggml_backend_dev_offload_op = _ggml_backend_dev_offload_opPtr
      .asFunction<
        bool Function(ggml_backend_dev_t, ffi.Pointer<ggml_tensor>)
      >();

  /// Backend (reg)
  ffi.Pointer<ffi.Char> ggml_backend_reg_name(ggml_backend_reg_t reg) {
    return _ggml_backend_reg_name(reg);
  }

  late final _ggml_backend_reg_namePtr =
      _lookup<
        ffi.NativeFunction<ffi.Pointer<ffi.Char> Function(ggml_backend_reg_t)>
      >('ggml_backend_reg_name');
  late final _ggml_backend_reg_name = _ggml_backend_reg_namePtr
      .asFunction<ffi.Pointer<ffi.Char> Function(ggml_backend_reg_t)>();

  int ggml_backend_reg_dev_count(ggml_backend_reg_t reg) {
    return _ggml_backend_reg_dev_count(reg);
  }

  late final _ggml_backend_reg_dev_countPtr =
      _lookup<ffi.NativeFunction<ffi.Size Function(ggml_backend_reg_t)>>(
        'ggml_backend_reg_dev_count',
      );
  late final _ggml_backend_reg_dev_count = _ggml_backend_reg_dev_countPtr
      .asFunction<int Function(ggml_backend_reg_t)>();

  ggml_backend_dev_t ggml_backend_reg_dev_get(
    ggml_backend_reg_t reg,
    int index,
  ) {
    return _ggml_backend_reg_dev_get(reg, index);
  }

  late final _ggml_backend_reg_dev_getPtr =
      _lookup<
        ffi.NativeFunction<
          ggml_backend_dev_t Function(ggml_backend_reg_t, ffi.Size)
        >
      >('ggml_backend_reg_dev_get');
  late final _ggml_backend_reg_dev_get = _ggml_backend_reg_dev_getPtr
      .asFunction<ggml_backend_dev_t Function(ggml_backend_reg_t, int)>();

  ffi.Pointer<ffi.Void> ggml_backend_reg_get_proc_address(
    ggml_backend_reg_t reg,
    ffi.Pointer<ffi.Char> name,
  ) {
    return _ggml_backend_reg_get_proc_address(reg, name);
  }

  late final _ggml_backend_reg_get_proc_addressPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ffi.Void> Function(
            ggml_backend_reg_t,
            ffi.Pointer<ffi.Char>,
          )
        >
      >('ggml_backend_reg_get_proc_address');
  late final _ggml_backend_reg_get_proc_address =
      _ggml_backend_reg_get_proc_addressPtr
          .asFunction<
            ffi.Pointer<ffi.Void> Function(
              ggml_backend_reg_t,
              ffi.Pointer<ffi.Char>,
            )
          >();

  /// Backend registry
  void ggml_backend_register(ggml_backend_reg_t reg) {
    return _ggml_backend_register(reg);
  }

  late final _ggml_backend_registerPtr =
      _lookup<ffi.NativeFunction<ffi.Void Function(ggml_backend_reg_t)>>(
        'ggml_backend_register',
      );
  late final _ggml_backend_register = _ggml_backend_registerPtr
      .asFunction<void Function(ggml_backend_reg_t)>();

  void ggml_backend_device_register(ggml_backend_dev_t device) {
    return _ggml_backend_device_register(device);
  }

  late final _ggml_backend_device_registerPtr =
      _lookup<ffi.NativeFunction<ffi.Void Function(ggml_backend_dev_t)>>(
        'ggml_backend_device_register',
      );
  late final _ggml_backend_device_register = _ggml_backend_device_registerPtr
      .asFunction<void Function(ggml_backend_dev_t)>();

  /// Backend (reg) enumeration
  int ggml_backend_reg_count() {
    return _ggml_backend_reg_count();
  }

  late final _ggml_backend_reg_countPtr =
      _lookup<ffi.NativeFunction<ffi.Size Function()>>(
        'ggml_backend_reg_count',
      );
  late final _ggml_backend_reg_count = _ggml_backend_reg_countPtr
      .asFunction<int Function()>();

  ggml_backend_reg_t ggml_backend_reg_get(int index) {
    return _ggml_backend_reg_get(index);
  }

  late final _ggml_backend_reg_getPtr =
      _lookup<ffi.NativeFunction<ggml_backend_reg_t Function(ffi.Size)>>(
        'ggml_backend_reg_get',
      );
  late final _ggml_backend_reg_get = _ggml_backend_reg_getPtr
      .asFunction<ggml_backend_reg_t Function(int)>();

  ggml_backend_reg_t ggml_backend_reg_by_name(ffi.Pointer<ffi.Char> name) {
    return _ggml_backend_reg_by_name(name);
  }

  late final _ggml_backend_reg_by_namePtr =
      _lookup<
        ffi.NativeFunction<ggml_backend_reg_t Function(ffi.Pointer<ffi.Char>)>
      >('ggml_backend_reg_by_name');
  late final _ggml_backend_reg_by_name = _ggml_backend_reg_by_namePtr
      .asFunction<ggml_backend_reg_t Function(ffi.Pointer<ffi.Char>)>();

  /// Device enumeration
  int ggml_backend_dev_count() {
    return _ggml_backend_dev_count();
  }

  late final _ggml_backend_dev_countPtr =
      _lookup<ffi.NativeFunction<ffi.Size Function()>>(
        'ggml_backend_dev_count',
      );
  late final _ggml_backend_dev_count = _ggml_backend_dev_countPtr
      .asFunction<int Function()>();

  ggml_backend_dev_t ggml_backend_dev_get(int index) {
    return _ggml_backend_dev_get(index);
  }

  late final _ggml_backend_dev_getPtr =
      _lookup<ffi.NativeFunction<ggml_backend_dev_t Function(ffi.Size)>>(
        'ggml_backend_dev_get',
      );
  late final _ggml_backend_dev_get = _ggml_backend_dev_getPtr
      .asFunction<ggml_backend_dev_t Function(int)>();

  ggml_backend_dev_t ggml_backend_dev_by_name(ffi.Pointer<ffi.Char> name) {
    return _ggml_backend_dev_by_name(name);
  }

  late final _ggml_backend_dev_by_namePtr =
      _lookup<
        ffi.NativeFunction<ggml_backend_dev_t Function(ffi.Pointer<ffi.Char>)>
      >('ggml_backend_dev_by_name');
  late final _ggml_backend_dev_by_name = _ggml_backend_dev_by_namePtr
      .asFunction<ggml_backend_dev_t Function(ffi.Pointer<ffi.Char>)>();

  ggml_backend_dev_t ggml_backend_dev_by_type(ggml_backend_dev_type type) {
    return _ggml_backend_dev_by_type(type.value);
  }

  late final _ggml_backend_dev_by_typePtr =
      _lookup<ffi.NativeFunction<ggml_backend_dev_t Function(ffi.UnsignedInt)>>(
        'ggml_backend_dev_by_type',
      );
  late final _ggml_backend_dev_by_type = _ggml_backend_dev_by_typePtr
      .asFunction<ggml_backend_dev_t Function(int)>();

  /// Direct backend (stream) initialization
  /// = ggml_backend_dev_init(ggml_backend_dev_by_name(name), params)
  ggml_backend_t ggml_backend_init_by_name(
    ffi.Pointer<ffi.Char> name,
    ffi.Pointer<ffi.Char> params,
  ) {
    return _ggml_backend_init_by_name(name, params);
  }

  late final _ggml_backend_init_by_namePtr =
      _lookup<
        ffi.NativeFunction<
          ggml_backend_t Function(ffi.Pointer<ffi.Char>, ffi.Pointer<ffi.Char>)
        >
      >('ggml_backend_init_by_name');
  late final _ggml_backend_init_by_name = _ggml_backend_init_by_namePtr
      .asFunction<
        ggml_backend_t Function(ffi.Pointer<ffi.Char>, ffi.Pointer<ffi.Char>)
      >();

  /// = ggml_backend_dev_init(ggml_backend_dev_by_type(type), params)
  ggml_backend_t ggml_backend_init_by_type(
    ggml_backend_dev_type type,
    ffi.Pointer<ffi.Char> params,
  ) {
    return _ggml_backend_init_by_type(type.value, params);
  }

  late final _ggml_backend_init_by_typePtr =
      _lookup<
        ffi.NativeFunction<
          ggml_backend_t Function(ffi.UnsignedInt, ffi.Pointer<ffi.Char>)
        >
      >('ggml_backend_init_by_type');
  late final _ggml_backend_init_by_type = _ggml_backend_init_by_typePtr
      .asFunction<ggml_backend_t Function(int, ffi.Pointer<ffi.Char>)>();

  /// = ggml_backend_dev_init(ggml_backend_dev_by_type(GPU) OR ggml_backend_dev_by_type(CPU), NULL)
  ggml_backend_t ggml_backend_init_best() {
    return _ggml_backend_init_best();
  }

  late final _ggml_backend_init_bestPtr =
      _lookup<ffi.NativeFunction<ggml_backend_t Function()>>(
        'ggml_backend_init_best',
      );
  late final _ggml_backend_init_best = _ggml_backend_init_bestPtr
      .asFunction<ggml_backend_t Function()>();

  /// Load a backend from a dynamic library and register it
  ggml_backend_reg_t ggml_backend_load(ffi.Pointer<ffi.Char> path) {
    return _ggml_backend_load(path);
  }

  late final _ggml_backend_loadPtr =
      _lookup<
        ffi.NativeFunction<ggml_backend_reg_t Function(ffi.Pointer<ffi.Char>)>
      >('ggml_backend_load');
  late final _ggml_backend_load = _ggml_backend_loadPtr
      .asFunction<ggml_backend_reg_t Function(ffi.Pointer<ffi.Char>)>();

  /// Unload a backend if loaded dynamically and unregister it
  void ggml_backend_unload(ggml_backend_reg_t reg) {
    return _ggml_backend_unload(reg);
  }

  late final _ggml_backend_unloadPtr =
      _lookup<ffi.NativeFunction<ffi.Void Function(ggml_backend_reg_t)>>(
        'ggml_backend_unload',
      );
  late final _ggml_backend_unload = _ggml_backend_unloadPtr
      .asFunction<void Function(ggml_backend_reg_t)>();

  /// Load all known backends from dynamic libraries
  void ggml_backend_load_all() {
    return _ggml_backend_load_all();
  }

  late final _ggml_backend_load_allPtr =
      _lookup<ffi.NativeFunction<ffi.Void Function()>>('ggml_backend_load_all');
  late final _ggml_backend_load_all = _ggml_backend_load_allPtr
      .asFunction<void Function()>();

  void ggml_backend_load_all_from_path(ffi.Pointer<ffi.Char> dir_path) {
    return _ggml_backend_load_all_from_path(dir_path);
  }

  late final _ggml_backend_load_all_from_pathPtr =
      _lookup<ffi.NativeFunction<ffi.Void Function(ffi.Pointer<ffi.Char>)>>(
        'ggml_backend_load_all_from_path',
      );
  late final _ggml_backend_load_all_from_path =
      _ggml_backend_load_all_from_pathPtr
          .asFunction<void Function(ffi.Pointer<ffi.Char>)>();

  /// Initialize a backend scheduler, backends with low index are given priority over backends with high index
  ggml_backend_sched_t ggml_backend_sched_new(
    ffi.Pointer<ggml_backend_t> backends,
    ffi.Pointer<ggml_backend_buffer_type_t> bufts,
    int n_backends,
    int graph_size,
    bool parallel,
    bool op_offload,
  ) {
    return _ggml_backend_sched_new(
      backends,
      bufts,
      n_backends,
      graph_size,
      parallel,
      op_offload,
    );
  }

  late final _ggml_backend_sched_newPtr =
      _lookup<
        ffi.NativeFunction<
          ggml_backend_sched_t Function(
            ffi.Pointer<ggml_backend_t>,
            ffi.Pointer<ggml_backend_buffer_type_t>,
            ffi.Int,
            ffi.Size,
            ffi.Bool,
            ffi.Bool,
          )
        >
      >('ggml_backend_sched_new');
  late final _ggml_backend_sched_new = _ggml_backend_sched_newPtr
      .asFunction<
        ggml_backend_sched_t Function(
          ffi.Pointer<ggml_backend_t>,
          ffi.Pointer<ggml_backend_buffer_type_t>,
          int,
          int,
          bool,
          bool,
        )
      >();

  void ggml_backend_sched_free(ggml_backend_sched_t sched) {
    return _ggml_backend_sched_free(sched);
  }

  late final _ggml_backend_sched_freePtr =
      _lookup<ffi.NativeFunction<ffi.Void Function(ggml_backend_sched_t)>>(
        'ggml_backend_sched_free',
      );
  late final _ggml_backend_sched_free = _ggml_backend_sched_freePtr
      .asFunction<void Function(ggml_backend_sched_t)>();

  /// Initialize backend buffers from a measure graph
  bool ggml_backend_sched_reserve(
    ggml_backend_sched_t sched,
    ffi.Pointer<ggml_cgraph> measure_graph,
  ) {
    return _ggml_backend_sched_reserve(sched, measure_graph);
  }

  late final _ggml_backend_sched_reservePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Bool Function(ggml_backend_sched_t, ffi.Pointer<ggml_cgraph>)
        >
      >('ggml_backend_sched_reserve');
  late final _ggml_backend_sched_reserve = _ggml_backend_sched_reservePtr
      .asFunction<
        bool Function(ggml_backend_sched_t, ffi.Pointer<ggml_cgraph>)
      >();

  int ggml_backend_sched_get_n_backends(ggml_backend_sched_t sched) {
    return _ggml_backend_sched_get_n_backends(sched);
  }

  late final _ggml_backend_sched_get_n_backendsPtr =
      _lookup<ffi.NativeFunction<ffi.Int Function(ggml_backend_sched_t)>>(
        'ggml_backend_sched_get_n_backends',
      );
  late final _ggml_backend_sched_get_n_backends =
      _ggml_backend_sched_get_n_backendsPtr
          .asFunction<int Function(ggml_backend_sched_t)>();

  ggml_backend_t ggml_backend_sched_get_backend(
    ggml_backend_sched_t sched,
    int i,
  ) {
    return _ggml_backend_sched_get_backend(sched, i);
  }

  late final _ggml_backend_sched_get_backendPtr =
      _lookup<
        ffi.NativeFunction<
          ggml_backend_t Function(ggml_backend_sched_t, ffi.Int)
        >
      >('ggml_backend_sched_get_backend');
  late final _ggml_backend_sched_get_backend =
      _ggml_backend_sched_get_backendPtr
          .asFunction<ggml_backend_t Function(ggml_backend_sched_t, int)>();

  /// Get the number of splits of the last graph
  int ggml_backend_sched_get_n_splits(ggml_backend_sched_t sched) {
    return _ggml_backend_sched_get_n_splits(sched);
  }

  late final _ggml_backend_sched_get_n_splitsPtr =
      _lookup<ffi.NativeFunction<ffi.Int Function(ggml_backend_sched_t)>>(
        'ggml_backend_sched_get_n_splits',
      );
  late final _ggml_backend_sched_get_n_splits =
      _ggml_backend_sched_get_n_splitsPtr
          .asFunction<int Function(ggml_backend_sched_t)>();

  int ggml_backend_sched_get_n_copies(ggml_backend_sched_t sched) {
    return _ggml_backend_sched_get_n_copies(sched);
  }

  late final _ggml_backend_sched_get_n_copiesPtr =
      _lookup<ffi.NativeFunction<ffi.Int Function(ggml_backend_sched_t)>>(
        'ggml_backend_sched_get_n_copies',
      );
  late final _ggml_backend_sched_get_n_copies =
      _ggml_backend_sched_get_n_copiesPtr
          .asFunction<int Function(ggml_backend_sched_t)>();

  ggml_backend_buffer_type_t ggml_backend_sched_get_buffer_type(
    ggml_backend_sched_t sched,
    ggml_backend_t backend,
  ) {
    return _ggml_backend_sched_get_buffer_type(sched, backend);
  }

  late final _ggml_backend_sched_get_buffer_typePtr =
      _lookup<
        ffi.NativeFunction<
          ggml_backend_buffer_type_t Function(
            ggml_backend_sched_t,
            ggml_backend_t,
          )
        >
      >('ggml_backend_sched_get_buffer_type');
  late final _ggml_backend_sched_get_buffer_type =
      _ggml_backend_sched_get_buffer_typePtr
          .asFunction<
            ggml_backend_buffer_type_t Function(
              ggml_backend_sched_t,
              ggml_backend_t,
            )
          >();

  int ggml_backend_sched_get_buffer_size(
    ggml_backend_sched_t sched,
    ggml_backend_t backend,
  ) {
    return _ggml_backend_sched_get_buffer_size(sched, backend);
  }

  late final _ggml_backend_sched_get_buffer_sizePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Size Function(ggml_backend_sched_t, ggml_backend_t)
        >
      >('ggml_backend_sched_get_buffer_size');
  late final _ggml_backend_sched_get_buffer_size =
      _ggml_backend_sched_get_buffer_sizePtr
          .asFunction<int Function(ggml_backend_sched_t, ggml_backend_t)>();

  void ggml_backend_sched_set_tensor_backend(
    ggml_backend_sched_t sched,
    ffi.Pointer<ggml_tensor> node,
    ggml_backend_t backend,
  ) {
    return _ggml_backend_sched_set_tensor_backend(sched, node, backend);
  }

  late final _ggml_backend_sched_set_tensor_backendPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Void Function(
            ggml_backend_sched_t,
            ffi.Pointer<ggml_tensor>,
            ggml_backend_t,
          )
        >
      >('ggml_backend_sched_set_tensor_backend');
  late final _ggml_backend_sched_set_tensor_backend =
      _ggml_backend_sched_set_tensor_backendPtr
          .asFunction<
            void Function(
              ggml_backend_sched_t,
              ffi.Pointer<ggml_tensor>,
              ggml_backend_t,
            )
          >();

  ggml_backend_t ggml_backend_sched_get_tensor_backend(
    ggml_backend_sched_t sched,
    ffi.Pointer<ggml_tensor> node,
  ) {
    return _ggml_backend_sched_get_tensor_backend(sched, node);
  }

  late final _ggml_backend_sched_get_tensor_backendPtr =
      _lookup<
        ffi.NativeFunction<
          ggml_backend_t Function(
            ggml_backend_sched_t,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_backend_sched_get_tensor_backend');
  late final _ggml_backend_sched_get_tensor_backend =
      _ggml_backend_sched_get_tensor_backendPtr
          .asFunction<
            ggml_backend_t Function(
              ggml_backend_sched_t,
              ffi.Pointer<ggml_tensor>,
            )
          >();

  /// Split graph without allocating it
  void ggml_backend_sched_split_graph(
    ggml_backend_sched_t sched,
    ffi.Pointer<ggml_cgraph> graph,
  ) {
    return _ggml_backend_sched_split_graph(sched, graph);
  }

  late final _ggml_backend_sched_split_graphPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Void Function(ggml_backend_sched_t, ffi.Pointer<ggml_cgraph>)
        >
      >('ggml_backend_sched_split_graph');
  late final _ggml_backend_sched_split_graph =
      _ggml_backend_sched_split_graphPtr
          .asFunction<
            void Function(ggml_backend_sched_t, ffi.Pointer<ggml_cgraph>)
          >();

  /// Allocate and compute graph on the backend scheduler
  bool ggml_backend_sched_alloc_graph(
    ggml_backend_sched_t sched,
    ffi.Pointer<ggml_cgraph> graph,
  ) {
    return _ggml_backend_sched_alloc_graph(sched, graph);
  }

  late final _ggml_backend_sched_alloc_graphPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Bool Function(ggml_backend_sched_t, ffi.Pointer<ggml_cgraph>)
        >
      >('ggml_backend_sched_alloc_graph');
  late final _ggml_backend_sched_alloc_graph =
      _ggml_backend_sched_alloc_graphPtr
          .asFunction<
            bool Function(ggml_backend_sched_t, ffi.Pointer<ggml_cgraph>)
          >();

  ggml_status ggml_backend_sched_graph_compute(
    ggml_backend_sched_t sched,
    ffi.Pointer<ggml_cgraph> graph,
  ) {
    return ggml_status.fromValue(
      _ggml_backend_sched_graph_compute(sched, graph),
    );
  }

  late final _ggml_backend_sched_graph_computePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Int Function(ggml_backend_sched_t, ffi.Pointer<ggml_cgraph>)
        >
      >('ggml_backend_sched_graph_compute');
  late final _ggml_backend_sched_graph_compute =
      _ggml_backend_sched_graph_computePtr
          .asFunction<
            int Function(ggml_backend_sched_t, ffi.Pointer<ggml_cgraph>)
          >();

  ggml_status ggml_backend_sched_graph_compute_async(
    ggml_backend_sched_t sched,
    ffi.Pointer<ggml_cgraph> graph,
  ) {
    return ggml_status.fromValue(
      _ggml_backend_sched_graph_compute_async(sched, graph),
    );
  }

  late final _ggml_backend_sched_graph_compute_asyncPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Int Function(ggml_backend_sched_t, ffi.Pointer<ggml_cgraph>)
        >
      >('ggml_backend_sched_graph_compute_async');
  late final _ggml_backend_sched_graph_compute_async =
      _ggml_backend_sched_graph_compute_asyncPtr
          .asFunction<
            int Function(ggml_backend_sched_t, ffi.Pointer<ggml_cgraph>)
          >();

  void ggml_backend_sched_synchronize(ggml_backend_sched_t sched) {
    return _ggml_backend_sched_synchronize(sched);
  }

  late final _ggml_backend_sched_synchronizePtr =
      _lookup<ffi.NativeFunction<ffi.Void Function(ggml_backend_sched_t)>>(
        'ggml_backend_sched_synchronize',
      );
  late final _ggml_backend_sched_synchronize =
      _ggml_backend_sched_synchronizePtr
          .asFunction<void Function(ggml_backend_sched_t)>();

  /// Reset all assignments and allocators - must be called before changing the node backends or allocating a new graph.
  /// This in effect deallocates all tensors that were previously allocated and leaves them with dangling pointers.
  /// The correct way to use this API is to discard the deallocated tensors and create new ones.
  void ggml_backend_sched_reset(ggml_backend_sched_t sched) {
    return _ggml_backend_sched_reset(sched);
  }

  late final _ggml_backend_sched_resetPtr =
      _lookup<ffi.NativeFunction<ffi.Void Function(ggml_backend_sched_t)>>(
        'ggml_backend_sched_reset',
      );
  late final _ggml_backend_sched_reset = _ggml_backend_sched_resetPtr
      .asFunction<void Function(ggml_backend_sched_t)>();

  /// Set a callback to be called for each resulting node during graph compute
  void ggml_backend_sched_set_eval_callback(
    ggml_backend_sched_t sched,
    ggml_backend_sched_eval_callback callback,
    ffi.Pointer<ffi.Void> user_data,
  ) {
    return _ggml_backend_sched_set_eval_callback(sched, callback, user_data);
  }

  late final _ggml_backend_sched_set_eval_callbackPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Void Function(
            ggml_backend_sched_t,
            ggml_backend_sched_eval_callback,
            ffi.Pointer<ffi.Void>,
          )
        >
      >('ggml_backend_sched_set_eval_callback');
  late final _ggml_backend_sched_set_eval_callback =
      _ggml_backend_sched_set_eval_callbackPtr
          .asFunction<
            void Function(
              ggml_backend_sched_t,
              ggml_backend_sched_eval_callback,
              ffi.Pointer<ffi.Void>,
            )
          >();

  /// Copy a graph to a different backend
  ggml_backend_graph_copy$1 ggml_backend_graph_copy(
    ggml_backend_t backend,
    ffi.Pointer<ggml_cgraph> graph,
  ) {
    return _ggml_backend_graph_copy(backend, graph);
  }

  late final _ggml_backend_graph_copyPtr =
      _lookup<
        ffi.NativeFunction<
          ggml_backend_graph_copy$1 Function(
            ggml_backend_t,
            ffi.Pointer<ggml_cgraph>,
          )
        >
      >('ggml_backend_graph_copy');
  late final _ggml_backend_graph_copy = _ggml_backend_graph_copyPtr
      .asFunction<
        ggml_backend_graph_copy$1 Function(
          ggml_backend_t,
          ffi.Pointer<ggml_cgraph>,
        )
      >();

  void ggml_backend_graph_copy_free(ggml_backend_graph_copy$1 copy) {
    return _ggml_backend_graph_copy_free(copy);
  }

  late final _ggml_backend_graph_copy_freePtr =
      _lookup<ffi.NativeFunction<ffi.Void Function(ggml_backend_graph_copy$1)>>(
        'ggml_backend_graph_copy_free',
      );
  late final _ggml_backend_graph_copy_free = _ggml_backend_graph_copy_freePtr
      .asFunction<void Function(ggml_backend_graph_copy$1)>();

  /// Compare the output of two backends
  bool ggml_backend_compare_graph_backend(
    ggml_backend_t backend1,
    ggml_backend_t backend2,
    ffi.Pointer<ggml_cgraph> graph,
    ggml_backend_eval_callback callback,
    ffi.Pointer<ffi.Void> user_data,
    ffi.Pointer<ggml_tensor> test_node,
  ) {
    return _ggml_backend_compare_graph_backend(
      backend1,
      backend2,
      graph,
      callback,
      user_data,
      test_node,
    );
  }

  late final _ggml_backend_compare_graph_backendPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Bool Function(
            ggml_backend_t,
            ggml_backend_t,
            ffi.Pointer<ggml_cgraph>,
            ggml_backend_eval_callback,
            ffi.Pointer<ffi.Void>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_backend_compare_graph_backend');
  late final _ggml_backend_compare_graph_backend =
      _ggml_backend_compare_graph_backendPtr
          .asFunction<
            bool Function(
              ggml_backend_t,
              ggml_backend_t,
              ffi.Pointer<ggml_cgraph>,
              ggml_backend_eval_callback,
              ffi.Pointer<ffi.Void>,
              ffi.Pointer<ggml_tensor>,
            )
          >();

  /// Tensor initialization
  ggml_status ggml_backend_tensor_alloc(
    ggml_backend_buffer_t buffer,
    ffi.Pointer<ggml_tensor> tensor,
    ffi.Pointer<ffi.Void> addr,
  ) {
    return ggml_status.fromValue(
      _ggml_backend_tensor_alloc(buffer, tensor, addr),
    );
  }

  late final _ggml_backend_tensor_allocPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Int Function(
            ggml_backend_buffer_t,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ffi.Void>,
          )
        >
      >('ggml_backend_tensor_alloc');
  late final _ggml_backend_tensor_alloc = _ggml_backend_tensor_allocPtr
      .asFunction<
        int Function(
          ggml_backend_buffer_t,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ffi.Void>,
        )
      >();

  ggml_status ggml_backend_view_init(ffi.Pointer<ggml_tensor> tensor) {
    return ggml_status.fromValue(_ggml_backend_view_init(tensor));
  }

  late final _ggml_backend_view_initPtr =
      _lookup<ffi.NativeFunction<ffi.Int Function(ffi.Pointer<ggml_tensor>)>>(
        'ggml_backend_view_init',
      );
  late final _ggml_backend_view_init = _ggml_backend_view_initPtr
      .asFunction<int Function(ffi.Pointer<ggml_tensor>)>();

  /// CPU buffer types are always available
  ggml_backend_buffer_t ggml_backend_cpu_buffer_from_ptr(
    ffi.Pointer<ffi.Void> ptr,
    int size,
  ) {
    return _ggml_backend_cpu_buffer_from_ptr(ptr, size);
  }

  late final _ggml_backend_cpu_buffer_from_ptrPtr =
      _lookup<
        ffi.NativeFunction<
          ggml_backend_buffer_t Function(ffi.Pointer<ffi.Void>, ffi.Size)
        >
      >('ggml_backend_cpu_buffer_from_ptr');
  late final _ggml_backend_cpu_buffer_from_ptr =
      _ggml_backend_cpu_buffer_from_ptrPtr
          .asFunction<
            ggml_backend_buffer_t Function(ffi.Pointer<ffi.Void>, int)
          >();

  ggml_backend_buffer_type_t ggml_backend_cpu_buffer_type() {
    return _ggml_backend_cpu_buffer_type();
  }

  late final _ggml_backend_cpu_buffer_typePtr =
      _lookup<ffi.NativeFunction<ggml_backend_buffer_type_t Function()>>(
        'ggml_backend_cpu_buffer_type',
      );
  late final _ggml_backend_cpu_buffer_type = _ggml_backend_cpu_buffer_typePtr
      .asFunction<ggml_backend_buffer_type_t Function()>();

  void ggml_numa_init(ggml_numa_strategy numa) {
    return _ggml_numa_init(numa.value);
  }

  late final _ggml_numa_initPtr =
      _lookup<ffi.NativeFunction<ffi.Void Function(ffi.UnsignedInt)>>(
        'ggml_numa_init',
      );
  late final _ggml_numa_init = _ggml_numa_initPtr
      .asFunction<void Function(int)>();

  bool ggml_is_numa() {
    return _ggml_is_numa();
  }

  late final _ggml_is_numaPtr =
      _lookup<ffi.NativeFunction<ffi.Bool Function()>>('ggml_is_numa');
  late final _ggml_is_numa = _ggml_is_numaPtr.asFunction<bool Function()>();

  ffi.Pointer<ggml_tensor> ggml_new_i32(
    ffi.Pointer<ggml_context> ctx,
    int value,
  ) {
    return _ggml_new_i32(ctx, value);
  }

  late final _ggml_new_i32Ptr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Int32,
          )
        >
      >('ggml_new_i32');
  late final _ggml_new_i32 = _ggml_new_i32Ptr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>, int)
      >();

  ffi.Pointer<ggml_tensor> ggml_new_f32(
    ffi.Pointer<ggml_context> ctx,
    double value,
  ) {
    return _ggml_new_f32(ctx, value);
  }

  late final _ggml_new_f32Ptr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ffi.Pointer<ggml_context>,
            ffi.Float,
          )
        >
      >('ggml_new_f32');
  late final _ggml_new_f32 = _ggml_new_f32Ptr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>, double)
      >();

  ffi.Pointer<ggml_tensor> ggml_set_i32(
    ffi.Pointer<ggml_tensor> tensor,
    int value,
  ) {
    return _ggml_set_i32(tensor, value);
  }

  late final _ggml_set_i32Ptr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_tensor>, ffi.Int32)
        >
      >('ggml_set_i32');
  late final _ggml_set_i32 = _ggml_set_i32Ptr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_tensor>, int)
      >();

  ffi.Pointer<ggml_tensor> ggml_set_f32(
    ffi.Pointer<ggml_tensor> tensor,
    double value,
  ) {
    return _ggml_set_f32(tensor, value);
  }

  late final _ggml_set_f32Ptr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_tensor>, ffi.Float)
        >
      >('ggml_set_f32');
  late final _ggml_set_f32 = _ggml_set_f32Ptr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_tensor>, double)
      >();

  int ggml_get_i32_1d(ffi.Pointer<ggml_tensor> tensor, int i) {
    return _ggml_get_i32_1d(tensor, i);
  }

  late final _ggml_get_i32_1dPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Int32 Function(ffi.Pointer<ggml_tensor>, ffi.Int)
        >
      >('ggml_get_i32_1d');
  late final _ggml_get_i32_1d = _ggml_get_i32_1dPtr
      .asFunction<int Function(ffi.Pointer<ggml_tensor>, int)>();

  void ggml_set_i32_1d(ffi.Pointer<ggml_tensor> tensor, int i, int value) {
    return _ggml_set_i32_1d(tensor, i, value);
  }

  late final _ggml_set_i32_1dPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<ggml_tensor>, ffi.Int, ffi.Int32)
        >
      >('ggml_set_i32_1d');
  late final _ggml_set_i32_1d = _ggml_set_i32_1dPtr
      .asFunction<void Function(ffi.Pointer<ggml_tensor>, int, int)>();

  int ggml_get_i32_nd(
    ffi.Pointer<ggml_tensor> tensor,
    int i0,
    int i1,
    int i2,
    int i3,
  ) {
    return _ggml_get_i32_nd(tensor, i0, i1, i2, i3);
  }

  late final _ggml_get_i32_ndPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Int32 Function(
            ffi.Pointer<ggml_tensor>,
            ffi.Int,
            ffi.Int,
            ffi.Int,
            ffi.Int,
          )
        >
      >('ggml_get_i32_nd');
  late final _ggml_get_i32_nd = _ggml_get_i32_ndPtr
      .asFunction<int Function(ffi.Pointer<ggml_tensor>, int, int, int, int)>();

  void ggml_set_i32_nd(
    ffi.Pointer<ggml_tensor> tensor,
    int i0,
    int i1,
    int i2,
    int i3,
    int value,
  ) {
    return _ggml_set_i32_nd(tensor, i0, i1, i2, i3, value);
  }

  late final _ggml_set_i32_ndPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Void Function(
            ffi.Pointer<ggml_tensor>,
            ffi.Int,
            ffi.Int,
            ffi.Int,
            ffi.Int,
            ffi.Int32,
          )
        >
      >('ggml_set_i32_nd');
  late final _ggml_set_i32_nd = _ggml_set_i32_ndPtr
      .asFunction<
        void Function(ffi.Pointer<ggml_tensor>, int, int, int, int, int)
      >();

  double ggml_get_f32_1d(ffi.Pointer<ggml_tensor> tensor, int i) {
    return _ggml_get_f32_1d(tensor, i);
  }

  late final _ggml_get_f32_1dPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Float Function(ffi.Pointer<ggml_tensor>, ffi.Int)
        >
      >('ggml_get_f32_1d');
  late final _ggml_get_f32_1d = _ggml_get_f32_1dPtr
      .asFunction<double Function(ffi.Pointer<ggml_tensor>, int)>();

  void ggml_set_f32_1d(ffi.Pointer<ggml_tensor> tensor, int i, double value) {
    return _ggml_set_f32_1d(tensor, i, value);
  }

  late final _ggml_set_f32_1dPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<ggml_tensor>, ffi.Int, ffi.Float)
        >
      >('ggml_set_f32_1d');
  late final _ggml_set_f32_1d = _ggml_set_f32_1dPtr
      .asFunction<void Function(ffi.Pointer<ggml_tensor>, int, double)>();

  double ggml_get_f32_nd(
    ffi.Pointer<ggml_tensor> tensor,
    int i0,
    int i1,
    int i2,
    int i3,
  ) {
    return _ggml_get_f32_nd(tensor, i0, i1, i2, i3);
  }

  late final _ggml_get_f32_ndPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Float Function(
            ffi.Pointer<ggml_tensor>,
            ffi.Int,
            ffi.Int,
            ffi.Int,
            ffi.Int,
          )
        >
      >('ggml_get_f32_nd');
  late final _ggml_get_f32_nd = _ggml_get_f32_ndPtr
      .asFunction<
        double Function(ffi.Pointer<ggml_tensor>, int, int, int, int)
      >();

  void ggml_set_f32_nd(
    ffi.Pointer<ggml_tensor> tensor,
    int i0,
    int i1,
    int i2,
    int i3,
    double value,
  ) {
    return _ggml_set_f32_nd(tensor, i0, i1, i2, i3, value);
  }

  late final _ggml_set_f32_ndPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Void Function(
            ffi.Pointer<ggml_tensor>,
            ffi.Int,
            ffi.Int,
            ffi.Int,
            ffi.Int,
            ffi.Float,
          )
        >
      >('ggml_set_f32_nd');
  late final _ggml_set_f32_nd = _ggml_set_f32_ndPtr
      .asFunction<
        void Function(ffi.Pointer<ggml_tensor>, int, int, int, int, double)
      >();

  ffi.Pointer<ggml_threadpool> ggml_threadpool_new(
    ffi.Pointer<ggml_threadpool_params> params,
  ) {
    return _ggml_threadpool_new(params);
  }

  late final _ggml_threadpool_newPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_threadpool> Function(
            ffi.Pointer<ggml_threadpool_params>,
          )
        >
      >('ggml_threadpool_new');
  late final _ggml_threadpool_new = _ggml_threadpool_newPtr
      .asFunction<
        ffi.Pointer<ggml_threadpool> Function(
          ffi.Pointer<ggml_threadpool_params>,
        )
      >();

  void ggml_threadpool_free(ffi.Pointer<ggml_threadpool> threadpool) {
    return _ggml_threadpool_free(threadpool);
  }

  late final _ggml_threadpool_freePtr =
      _lookup<
        ffi.NativeFunction<ffi.Void Function(ffi.Pointer<ggml_threadpool>)>
      >('ggml_threadpool_free');
  late final _ggml_threadpool_free = _ggml_threadpool_freePtr
      .asFunction<void Function(ffi.Pointer<ggml_threadpool>)>();

  int ggml_threadpool_get_n_threads(ffi.Pointer<ggml_threadpool> threadpool) {
    return _ggml_threadpool_get_n_threads(threadpool);
  }

  late final _ggml_threadpool_get_n_threadsPtr =
      _lookup<
        ffi.NativeFunction<ffi.Int Function(ffi.Pointer<ggml_threadpool>)>
      >('ggml_threadpool_get_n_threads');
  late final _ggml_threadpool_get_n_threads = _ggml_threadpool_get_n_threadsPtr
      .asFunction<int Function(ffi.Pointer<ggml_threadpool>)>();

  void ggml_threadpool_pause(ffi.Pointer<ggml_threadpool> threadpool) {
    return _ggml_threadpool_pause(threadpool);
  }

  late final _ggml_threadpool_pausePtr =
      _lookup<
        ffi.NativeFunction<ffi.Void Function(ffi.Pointer<ggml_threadpool>)>
      >('ggml_threadpool_pause');
  late final _ggml_threadpool_pause = _ggml_threadpool_pausePtr
      .asFunction<void Function(ffi.Pointer<ggml_threadpool>)>();

  void ggml_threadpool_resume(ffi.Pointer<ggml_threadpool> threadpool) {
    return _ggml_threadpool_resume(threadpool);
  }

  late final _ggml_threadpool_resumePtr =
      _lookup<
        ffi.NativeFunction<ffi.Void Function(ffi.Pointer<ggml_threadpool>)>
      >('ggml_threadpool_resume');
  late final _ggml_threadpool_resume = _ggml_threadpool_resumePtr
      .asFunction<void Function(ffi.Pointer<ggml_threadpool>)>();

  /// ggml_graph_plan() has to be called before ggml_graph_compute()
  /// when plan.work_size > 0, caller must allocate memory for plan.work_data
  ggml_cplan ggml_graph_plan(
    ffi.Pointer<ggml_cgraph> cgraph,
    int n_threads,
    ffi.Pointer<ggml_threadpool> threadpool,
  ) {
    return _ggml_graph_plan(cgraph, n_threads, threadpool);
  }

  late final _ggml_graph_planPtr =
      _lookup<
        ffi.NativeFunction<
          ggml_cplan Function(
            ffi.Pointer<ggml_cgraph>,
            ffi.Int,
            ffi.Pointer<ggml_threadpool>,
          )
        >
      >('ggml_graph_plan');
  late final _ggml_graph_plan = _ggml_graph_planPtr
      .asFunction<
        ggml_cplan Function(
          ffi.Pointer<ggml_cgraph>,
          int,
          ffi.Pointer<ggml_threadpool>,
        )
      >();

  ggml_status ggml_graph_compute(
    ffi.Pointer<ggml_cgraph> cgraph,
    ffi.Pointer<ggml_cplan> cplan,
  ) {
    return ggml_status.fromValue(_ggml_graph_compute(cgraph, cplan));
  }

  late final _ggml_graph_computePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Int Function(ffi.Pointer<ggml_cgraph>, ffi.Pointer<ggml_cplan>)
        >
      >('ggml_graph_compute');
  late final _ggml_graph_compute = _ggml_graph_computePtr
      .asFunction<
        int Function(ffi.Pointer<ggml_cgraph>, ffi.Pointer<ggml_cplan>)
      >();

  /// same as ggml_graph_compute() but the work data is allocated as a part of the context
  /// note: the drawback of this API is that you must have ensured that the context has enough memory for the work data
  ggml_status ggml_graph_compute_with_ctx(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_cgraph> cgraph,
    int n_threads,
  ) {
    return ggml_status.fromValue(
      _ggml_graph_compute_with_ctx(ctx, cgraph, n_threads),
    );
  }

  late final _ggml_graph_compute_with_ctxPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Int Function(
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_cgraph>,
            ffi.Int,
          )
        >
      >('ggml_graph_compute_with_ctx');
  late final _ggml_graph_compute_with_ctx = _ggml_graph_compute_with_ctxPtr
      .asFunction<
        int Function(ffi.Pointer<ggml_context>, ffi.Pointer<ggml_cgraph>, int)
      >();

  /// x86
  int ggml_cpu_has_sse3() {
    return _ggml_cpu_has_sse3();
  }

  late final _ggml_cpu_has_sse3Ptr =
      _lookup<ffi.NativeFunction<ffi.Int Function()>>('ggml_cpu_has_sse3');
  late final _ggml_cpu_has_sse3 = _ggml_cpu_has_sse3Ptr
      .asFunction<int Function()>();

  int ggml_cpu_has_ssse3() {
    return _ggml_cpu_has_ssse3();
  }

  late final _ggml_cpu_has_ssse3Ptr =
      _lookup<ffi.NativeFunction<ffi.Int Function()>>('ggml_cpu_has_ssse3');
  late final _ggml_cpu_has_ssse3 = _ggml_cpu_has_ssse3Ptr
      .asFunction<int Function()>();

  int ggml_cpu_has_avx() {
    return _ggml_cpu_has_avx();
  }

  late final _ggml_cpu_has_avxPtr =
      _lookup<ffi.NativeFunction<ffi.Int Function()>>('ggml_cpu_has_avx');
  late final _ggml_cpu_has_avx = _ggml_cpu_has_avxPtr
      .asFunction<int Function()>();

  int ggml_cpu_has_avx_vnni() {
    return _ggml_cpu_has_avx_vnni();
  }

  late final _ggml_cpu_has_avx_vnniPtr =
      _lookup<ffi.NativeFunction<ffi.Int Function()>>('ggml_cpu_has_avx_vnni');
  late final _ggml_cpu_has_avx_vnni = _ggml_cpu_has_avx_vnniPtr
      .asFunction<int Function()>();

  int ggml_cpu_has_avx2() {
    return _ggml_cpu_has_avx2();
  }

  late final _ggml_cpu_has_avx2Ptr =
      _lookup<ffi.NativeFunction<ffi.Int Function()>>('ggml_cpu_has_avx2');
  late final _ggml_cpu_has_avx2 = _ggml_cpu_has_avx2Ptr
      .asFunction<int Function()>();

  int ggml_cpu_has_bmi2() {
    return _ggml_cpu_has_bmi2();
  }

  late final _ggml_cpu_has_bmi2Ptr =
      _lookup<ffi.NativeFunction<ffi.Int Function()>>('ggml_cpu_has_bmi2');
  late final _ggml_cpu_has_bmi2 = _ggml_cpu_has_bmi2Ptr
      .asFunction<int Function()>();

  int ggml_cpu_has_f16c() {
    return _ggml_cpu_has_f16c();
  }

  late final _ggml_cpu_has_f16cPtr =
      _lookup<ffi.NativeFunction<ffi.Int Function()>>('ggml_cpu_has_f16c');
  late final _ggml_cpu_has_f16c = _ggml_cpu_has_f16cPtr
      .asFunction<int Function()>();

  int ggml_cpu_has_fma() {
    return _ggml_cpu_has_fma();
  }

  late final _ggml_cpu_has_fmaPtr =
      _lookup<ffi.NativeFunction<ffi.Int Function()>>('ggml_cpu_has_fma');
  late final _ggml_cpu_has_fma = _ggml_cpu_has_fmaPtr
      .asFunction<int Function()>();

  int ggml_cpu_has_avx512() {
    return _ggml_cpu_has_avx512();
  }

  late final _ggml_cpu_has_avx512Ptr =
      _lookup<ffi.NativeFunction<ffi.Int Function()>>('ggml_cpu_has_avx512');
  late final _ggml_cpu_has_avx512 = _ggml_cpu_has_avx512Ptr
      .asFunction<int Function()>();

  int ggml_cpu_has_avx512_vbmi() {
    return _ggml_cpu_has_avx512_vbmi();
  }

  late final _ggml_cpu_has_avx512_vbmiPtr =
      _lookup<ffi.NativeFunction<ffi.Int Function()>>(
        'ggml_cpu_has_avx512_vbmi',
      );
  late final _ggml_cpu_has_avx512_vbmi = _ggml_cpu_has_avx512_vbmiPtr
      .asFunction<int Function()>();

  int ggml_cpu_has_avx512_vnni() {
    return _ggml_cpu_has_avx512_vnni();
  }

  late final _ggml_cpu_has_avx512_vnniPtr =
      _lookup<ffi.NativeFunction<ffi.Int Function()>>(
        'ggml_cpu_has_avx512_vnni',
      );
  late final _ggml_cpu_has_avx512_vnni = _ggml_cpu_has_avx512_vnniPtr
      .asFunction<int Function()>();

  int ggml_cpu_has_avx512_bf16() {
    return _ggml_cpu_has_avx512_bf16();
  }

  late final _ggml_cpu_has_avx512_bf16Ptr =
      _lookup<ffi.NativeFunction<ffi.Int Function()>>(
        'ggml_cpu_has_avx512_bf16',
      );
  late final _ggml_cpu_has_avx512_bf16 = _ggml_cpu_has_avx512_bf16Ptr
      .asFunction<int Function()>();

  int ggml_cpu_has_amx_int8() {
    return _ggml_cpu_has_amx_int8();
  }

  late final _ggml_cpu_has_amx_int8Ptr =
      _lookup<ffi.NativeFunction<ffi.Int Function()>>('ggml_cpu_has_amx_int8');
  late final _ggml_cpu_has_amx_int8 = _ggml_cpu_has_amx_int8Ptr
      .asFunction<int Function()>();

  /// ARM
  int ggml_cpu_has_neon() {
    return _ggml_cpu_has_neon();
  }

  late final _ggml_cpu_has_neonPtr =
      _lookup<ffi.NativeFunction<ffi.Int Function()>>('ggml_cpu_has_neon');
  late final _ggml_cpu_has_neon = _ggml_cpu_has_neonPtr
      .asFunction<int Function()>();

  int ggml_cpu_has_arm_fma() {
    return _ggml_cpu_has_arm_fma();
  }

  late final _ggml_cpu_has_arm_fmaPtr =
      _lookup<ffi.NativeFunction<ffi.Int Function()>>('ggml_cpu_has_arm_fma');
  late final _ggml_cpu_has_arm_fma = _ggml_cpu_has_arm_fmaPtr
      .asFunction<int Function()>();

  int ggml_cpu_has_fp16_va() {
    return _ggml_cpu_has_fp16_va();
  }

  late final _ggml_cpu_has_fp16_vaPtr =
      _lookup<ffi.NativeFunction<ffi.Int Function()>>('ggml_cpu_has_fp16_va');
  late final _ggml_cpu_has_fp16_va = _ggml_cpu_has_fp16_vaPtr
      .asFunction<int Function()>();

  int ggml_cpu_has_dotprod() {
    return _ggml_cpu_has_dotprod();
  }

  late final _ggml_cpu_has_dotprodPtr =
      _lookup<ffi.NativeFunction<ffi.Int Function()>>('ggml_cpu_has_dotprod');
  late final _ggml_cpu_has_dotprod = _ggml_cpu_has_dotprodPtr
      .asFunction<int Function()>();

  int ggml_cpu_has_matmul_int8() {
    return _ggml_cpu_has_matmul_int8();
  }

  late final _ggml_cpu_has_matmul_int8Ptr =
      _lookup<ffi.NativeFunction<ffi.Int Function()>>(
        'ggml_cpu_has_matmul_int8',
      );
  late final _ggml_cpu_has_matmul_int8 = _ggml_cpu_has_matmul_int8Ptr
      .asFunction<int Function()>();

  int ggml_cpu_has_sve() {
    return _ggml_cpu_has_sve();
  }

  late final _ggml_cpu_has_svePtr =
      _lookup<ffi.NativeFunction<ffi.Int Function()>>('ggml_cpu_has_sve');
  late final _ggml_cpu_has_sve = _ggml_cpu_has_svePtr
      .asFunction<int Function()>();

  int ggml_cpu_get_sve_cnt() {
    return _ggml_cpu_get_sve_cnt();
  }

  late final _ggml_cpu_get_sve_cntPtr =
      _lookup<ffi.NativeFunction<ffi.Int Function()>>('ggml_cpu_get_sve_cnt');
  late final _ggml_cpu_get_sve_cnt = _ggml_cpu_get_sve_cntPtr
      .asFunction<int Function()>();

  int ggml_cpu_has_sme() {
    return _ggml_cpu_has_sme();
  }

  late final _ggml_cpu_has_smePtr =
      _lookup<ffi.NativeFunction<ffi.Int Function()>>('ggml_cpu_has_sme');
  late final _ggml_cpu_has_sme = _ggml_cpu_has_smePtr
      .asFunction<int Function()>();

  /// other
  int ggml_cpu_has_riscv_v() {
    return _ggml_cpu_has_riscv_v();
  }

  late final _ggml_cpu_has_riscv_vPtr =
      _lookup<ffi.NativeFunction<ffi.Int Function()>>('ggml_cpu_has_riscv_v');
  late final _ggml_cpu_has_riscv_v = _ggml_cpu_has_riscv_vPtr
      .asFunction<int Function()>();

  int ggml_cpu_has_vsx() {
    return _ggml_cpu_has_vsx();
  }

  late final _ggml_cpu_has_vsxPtr =
      _lookup<ffi.NativeFunction<ffi.Int Function()>>('ggml_cpu_has_vsx');
  late final _ggml_cpu_has_vsx = _ggml_cpu_has_vsxPtr
      .asFunction<int Function()>();

  int ggml_cpu_has_vxe() {
    return _ggml_cpu_has_vxe();
  }

  late final _ggml_cpu_has_vxePtr =
      _lookup<ffi.NativeFunction<ffi.Int Function()>>('ggml_cpu_has_vxe');
  late final _ggml_cpu_has_vxe = _ggml_cpu_has_vxePtr
      .asFunction<int Function()>();

  int ggml_cpu_has_wasm_simd() {
    return _ggml_cpu_has_wasm_simd();
  }

  late final _ggml_cpu_has_wasm_simdPtr =
      _lookup<ffi.NativeFunction<ffi.Int Function()>>('ggml_cpu_has_wasm_simd');
  late final _ggml_cpu_has_wasm_simd = _ggml_cpu_has_wasm_simdPtr
      .asFunction<int Function()>();

  int ggml_cpu_has_llamafile() {
    return _ggml_cpu_has_llamafile();
  }

  late final _ggml_cpu_has_llamafilePtr =
      _lookup<ffi.NativeFunction<ffi.Int Function()>>('ggml_cpu_has_llamafile');
  late final _ggml_cpu_has_llamafile = _ggml_cpu_has_llamafilePtr
      .asFunction<int Function()>();

  ffi.Pointer<ggml_type_traits_cpu> ggml_get_type_traits_cpu(ggml_type type) {
    return _ggml_get_type_traits_cpu(type.value);
  }

  late final _ggml_get_type_traits_cpuPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_type_traits_cpu> Function(ffi.UnsignedInt)
        >
      >('ggml_get_type_traits_cpu');
  late final _ggml_get_type_traits_cpu = _ggml_get_type_traits_cpuPtr
      .asFunction<ffi.Pointer<ggml_type_traits_cpu> Function(int)>();

  void ggml_cpu_init() {
    return _ggml_cpu_init();
  }

  late final _ggml_cpu_initPtr =
      _lookup<ffi.NativeFunction<ffi.Void Function()>>('ggml_cpu_init');
  late final _ggml_cpu_init = _ggml_cpu_initPtr.asFunction<void Function()>();

  /// CPU backend
  ggml_backend_t ggml_backend_cpu_init() {
    return _ggml_backend_cpu_init();
  }

  late final _ggml_backend_cpu_initPtr =
      _lookup<ffi.NativeFunction<ggml_backend_t Function()>>(
        'ggml_backend_cpu_init',
      );
  late final _ggml_backend_cpu_init = _ggml_backend_cpu_initPtr
      .asFunction<ggml_backend_t Function()>();

  bool ggml_backend_is_cpu(ggml_backend_t backend) {
    return _ggml_backend_is_cpu(backend);
  }

  late final _ggml_backend_is_cpuPtr =
      _lookup<ffi.NativeFunction<ffi.Bool Function(ggml_backend_t)>>(
        'ggml_backend_is_cpu',
      );
  late final _ggml_backend_is_cpu = _ggml_backend_is_cpuPtr
      .asFunction<bool Function(ggml_backend_t)>();

  void ggml_backend_cpu_set_n_threads(
    ggml_backend_t backend_cpu,
    int n_threads,
  ) {
    return _ggml_backend_cpu_set_n_threads(backend_cpu, n_threads);
  }

  late final _ggml_backend_cpu_set_n_threadsPtr =
      _lookup<ffi.NativeFunction<ffi.Void Function(ggml_backend_t, ffi.Int)>>(
        'ggml_backend_cpu_set_n_threads',
      );
  late final _ggml_backend_cpu_set_n_threads =
      _ggml_backend_cpu_set_n_threadsPtr
          .asFunction<void Function(ggml_backend_t, int)>();

  void ggml_backend_cpu_set_threadpool(
    ggml_backend_t backend_cpu,
    ggml_threadpool_t threadpool,
  ) {
    return _ggml_backend_cpu_set_threadpool(backend_cpu, threadpool);
  }

  late final _ggml_backend_cpu_set_threadpoolPtr =
      _lookup<
        ffi.NativeFunction<ffi.Void Function(ggml_backend_t, ggml_threadpool_t)>
      >('ggml_backend_cpu_set_threadpool');
  late final _ggml_backend_cpu_set_threadpool =
      _ggml_backend_cpu_set_threadpoolPtr
          .asFunction<void Function(ggml_backend_t, ggml_threadpool_t)>();

  void ggml_backend_cpu_set_abort_callback(
    ggml_backend_t backend_cpu,
    ggml_abort_callback abort_callback,
    ffi.Pointer<ffi.Void> abort_callback_data,
  ) {
    return _ggml_backend_cpu_set_abort_callback(
      backend_cpu,
      abort_callback,
      abort_callback_data,
    );
  }

  late final _ggml_backend_cpu_set_abort_callbackPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Void Function(
            ggml_backend_t,
            ggml_abort_callback,
            ffi.Pointer<ffi.Void>,
          )
        >
      >('ggml_backend_cpu_set_abort_callback');
  late final _ggml_backend_cpu_set_abort_callback =
      _ggml_backend_cpu_set_abort_callbackPtr
          .asFunction<
            void Function(
              ggml_backend_t,
              ggml_abort_callback,
              ffi.Pointer<ffi.Void>,
            )
          >();

  ggml_backend_reg_t ggml_backend_cpu_reg() {
    return _ggml_backend_cpu_reg();
  }

  late final _ggml_backend_cpu_regPtr =
      _lookup<ffi.NativeFunction<ggml_backend_reg_t Function()>>(
        'ggml_backend_cpu_reg',
      );
  late final _ggml_backend_cpu_reg = _ggml_backend_cpu_regPtr
      .asFunction<ggml_backend_reg_t Function()>();

  void ggml_cpu_fp32_to_fp32(
    ffi.Pointer<ffi.Float> arg0,
    ffi.Pointer<ffi.Float> arg1,
    int arg2,
  ) {
    return _ggml_cpu_fp32_to_fp32(arg0, arg1, arg2);
  }

  late final _ggml_cpu_fp32_to_fp32Ptr =
      _lookup<
        ffi.NativeFunction<
          ffi.Void Function(
            ffi.Pointer<ffi.Float>,
            ffi.Pointer<ffi.Float>,
            ffi.Int64,
          )
        >
      >('ggml_cpu_fp32_to_fp32');
  late final _ggml_cpu_fp32_to_fp32 = _ggml_cpu_fp32_to_fp32Ptr
      .asFunction<
        void Function(ffi.Pointer<ffi.Float>, ffi.Pointer<ffi.Float>, int)
      >();

  void ggml_cpu_fp32_to_i32(
    ffi.Pointer<ffi.Float> arg0,
    ffi.Pointer<ffi.Int32> arg1,
    int arg2,
  ) {
    return _ggml_cpu_fp32_to_i32(arg0, arg1, arg2);
  }

  late final _ggml_cpu_fp32_to_i32Ptr =
      _lookup<
        ffi.NativeFunction<
          ffi.Void Function(
            ffi.Pointer<ffi.Float>,
            ffi.Pointer<ffi.Int32>,
            ffi.Int64,
          )
        >
      >('ggml_cpu_fp32_to_i32');
  late final _ggml_cpu_fp32_to_i32 = _ggml_cpu_fp32_to_i32Ptr
      .asFunction<
        void Function(ffi.Pointer<ffi.Float>, ffi.Pointer<ffi.Int32>, int)
      >();

  void ggml_cpu_fp32_to_fp16(
    ffi.Pointer<ffi.Float> arg0,
    ffi.Pointer<ggml_fp16_t> arg1,
    int arg2,
  ) {
    return _ggml_cpu_fp32_to_fp16(arg0, arg1, arg2);
  }

  late final _ggml_cpu_fp32_to_fp16Ptr =
      _lookup<
        ffi.NativeFunction<
          ffi.Void Function(
            ffi.Pointer<ffi.Float>,
            ffi.Pointer<ggml_fp16_t>,
            ffi.Int64,
          )
        >
      >('ggml_cpu_fp32_to_fp16');
  late final _ggml_cpu_fp32_to_fp16 = _ggml_cpu_fp32_to_fp16Ptr
      .asFunction<
        void Function(ffi.Pointer<ffi.Float>, ffi.Pointer<ggml_fp16_t>, int)
      >();

  void ggml_cpu_fp16_to_fp32(
    ffi.Pointer<ggml_fp16_t> arg0,
    ffi.Pointer<ffi.Float> arg1,
    int arg2,
  ) {
    return _ggml_cpu_fp16_to_fp32(arg0, arg1, arg2);
  }

  late final _ggml_cpu_fp16_to_fp32Ptr =
      _lookup<
        ffi.NativeFunction<
          ffi.Void Function(
            ffi.Pointer<ggml_fp16_t>,
            ffi.Pointer<ffi.Float>,
            ffi.Int64,
          )
        >
      >('ggml_cpu_fp16_to_fp32');
  late final _ggml_cpu_fp16_to_fp32 = _ggml_cpu_fp16_to_fp32Ptr
      .asFunction<
        void Function(ffi.Pointer<ggml_fp16_t>, ffi.Pointer<ffi.Float>, int)
      >();

  void ggml_cpu_fp32_to_bf16(
    ffi.Pointer<ffi.Float> arg0,
    ffi.Pointer<ggml_bf16_t> arg1,
    int arg2,
  ) {
    return _ggml_cpu_fp32_to_bf16(arg0, arg1, arg2);
  }

  late final _ggml_cpu_fp32_to_bf16Ptr =
      _lookup<
        ffi.NativeFunction<
          ffi.Void Function(
            ffi.Pointer<ffi.Float>,
            ffi.Pointer<ggml_bf16_t>,
            ffi.Int64,
          )
        >
      >('ggml_cpu_fp32_to_bf16');
  late final _ggml_cpu_fp32_to_bf16 = _ggml_cpu_fp32_to_bf16Ptr
      .asFunction<
        void Function(ffi.Pointer<ffi.Float>, ffi.Pointer<ggml_bf16_t>, int)
      >();

  void ggml_cpu_bf16_to_fp32(
    ffi.Pointer<ggml_bf16_t> arg0,
    ffi.Pointer<ffi.Float> arg1,
    int arg2,
  ) {
    return _ggml_cpu_bf16_to_fp32(arg0, arg1, arg2);
  }

  late final _ggml_cpu_bf16_to_fp32Ptr =
      _lookup<
        ffi.NativeFunction<
          ffi.Void Function(
            ffi.Pointer<ggml_bf16_t>,
            ffi.Pointer<ffi.Float>,
            ffi.Int64,
          )
        >
      >('ggml_cpu_bf16_to_fp32');
  late final _ggml_cpu_bf16_to_fp32 = _ggml_cpu_bf16_to_fp32Ptr
      .asFunction<
        void Function(ffi.Pointer<ggml_bf16_t>, ffi.Pointer<ffi.Float>, int)
      >();

  /// ====== Dataset ======
  ggml_opt_dataset_t ggml_opt_dataset_init(
    ggml_type type_data,
    ggml_type type_label,
    int ne_datapoint,
    int ne_label,
    int ndata,
    int ndata_shard,
  ) {
    return _ggml_opt_dataset_init(
      type_data.value,
      type_label.value,
      ne_datapoint,
      ne_label,
      ndata,
      ndata_shard,
    );
  }

  late final _ggml_opt_dataset_initPtr =
      _lookup<
        ffi.NativeFunction<
          ggml_opt_dataset_t Function(
            ffi.UnsignedInt,
            ffi.UnsignedInt,
            ffi.Int64,
            ffi.Int64,
            ffi.Int64,
            ffi.Int64,
          )
        >
      >('ggml_opt_dataset_init');
  late final _ggml_opt_dataset_init = _ggml_opt_dataset_initPtr
      .asFunction<ggml_opt_dataset_t Function(int, int, int, int, int, int)>();

  void ggml_opt_dataset_free(ggml_opt_dataset_t dataset) {
    return _ggml_opt_dataset_free(dataset);
  }

  late final _ggml_opt_dataset_freePtr =
      _lookup<ffi.NativeFunction<ffi.Void Function(ggml_opt_dataset_t)>>(
        'ggml_opt_dataset_free',
      );
  late final _ggml_opt_dataset_free = _ggml_opt_dataset_freePtr
      .asFunction<void Function(ggml_opt_dataset_t)>();

  /// get underlying tensors that store the data
  int ggml_opt_dataset_ndata(ggml_opt_dataset_t dataset) {
    return _ggml_opt_dataset_ndata(dataset);
  }

  late final _ggml_opt_dataset_ndataPtr =
      _lookup<ffi.NativeFunction<ffi.Int64 Function(ggml_opt_dataset_t)>>(
        'ggml_opt_dataset_ndata',
      );
  late final _ggml_opt_dataset_ndata = _ggml_opt_dataset_ndataPtr
      .asFunction<int Function(ggml_opt_dataset_t)>();

  ffi.Pointer<ggml_tensor> ggml_opt_dataset_data(ggml_opt_dataset_t dataset) {
    return _ggml_opt_dataset_data(dataset);
  }

  late final _ggml_opt_dataset_dataPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(ggml_opt_dataset_t)
        >
      >('ggml_opt_dataset_data');
  late final _ggml_opt_dataset_data = _ggml_opt_dataset_dataPtr
      .asFunction<ffi.Pointer<ggml_tensor> Function(ggml_opt_dataset_t)>();

  ffi.Pointer<ggml_tensor> ggml_opt_dataset_labels(ggml_opt_dataset_t dataset) {
    return _ggml_opt_dataset_labels(dataset);
  }

  late final _ggml_opt_dataset_labelsPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(ggml_opt_dataset_t)
        >
      >('ggml_opt_dataset_labels');
  late final _ggml_opt_dataset_labels = _ggml_opt_dataset_labelsPtr
      .asFunction<ffi.Pointer<ggml_tensor> Function(ggml_opt_dataset_t)>();

  /// shuffle idata first datapoints from dataset with RNG from opt_ctx, shuffle all datapoints if idata is negative
  void ggml_opt_dataset_shuffle(
    ggml_opt_context_t opt_ctx,
    ggml_opt_dataset_t dataset,
    int idata,
  ) {
    return _ggml_opt_dataset_shuffle(opt_ctx, dataset, idata);
  }

  late final _ggml_opt_dataset_shufflePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Void Function(ggml_opt_context_t, ggml_opt_dataset_t, ffi.Int64)
        >
      >('ggml_opt_dataset_shuffle');
  late final _ggml_opt_dataset_shuffle = _ggml_opt_dataset_shufflePtr
      .asFunction<void Function(ggml_opt_context_t, ggml_opt_dataset_t, int)>();

  /// get batch at position ibatch from dataset and copy the data to data_batch and labels_batch
  void ggml_opt_dataset_get_batch(
    ggml_opt_dataset_t dataset,
    ffi.Pointer<ggml_tensor> data_batch,
    ffi.Pointer<ggml_tensor> labels_batch,
    int ibatch,
  ) {
    return _ggml_opt_dataset_get_batch(
      dataset,
      data_batch,
      labels_batch,
      ibatch,
    );
  }

  late final _ggml_opt_dataset_get_batchPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Void Function(
            ggml_opt_dataset_t,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
            ffi.Int64,
          )
        >
      >('ggml_opt_dataset_get_batch');
  late final _ggml_opt_dataset_get_batch = _ggml_opt_dataset_get_batchPtr
      .asFunction<
        void Function(
          ggml_opt_dataset_t,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          int,
        )
      >();

  void ggml_opt_dataset_get_batch_host(
    ggml_opt_dataset_t dataset,
    ffi.Pointer<ffi.Void> data_batch,
    int nb_data_batch,
    ffi.Pointer<ffi.Void> labels_batch,
    int ibatch,
  ) {
    return _ggml_opt_dataset_get_batch_host(
      dataset,
      data_batch,
      nb_data_batch,
      labels_batch,
      ibatch,
    );
  }

  late final _ggml_opt_dataset_get_batch_hostPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Void Function(
            ggml_opt_dataset_t,
            ffi.Pointer<ffi.Void>,
            ffi.Size,
            ffi.Pointer<ffi.Void>,
            ffi.Int64,
          )
        >
      >('ggml_opt_dataset_get_batch_host');
  late final _ggml_opt_dataset_get_batch_host =
      _ggml_opt_dataset_get_batch_hostPtr
          .asFunction<
            void Function(
              ggml_opt_dataset_t,
              ffi.Pointer<ffi.Void>,
              int,
              ffi.Pointer<ffi.Void>,
              int,
            )
          >();

  /// returns the default optimizer params (constant, hard-coded values)
  /// userdata is not used
  ggml_opt_optimizer_params ggml_opt_get_default_optimizer_params(
    ffi.Pointer<ffi.Void> userdata,
  ) {
    return _ggml_opt_get_default_optimizer_params(userdata);
  }

  late final _ggml_opt_get_default_optimizer_paramsPtr =
      _lookup<
        ffi.NativeFunction<
          ggml_opt_optimizer_params Function(ffi.Pointer<ffi.Void>)
        >
      >('ggml_opt_get_default_optimizer_params');
  late final _ggml_opt_get_default_optimizer_params =
      _ggml_opt_get_default_optimizer_paramsPtr
          .asFunction<
            ggml_opt_optimizer_params Function(ffi.Pointer<ffi.Void>)
          >();

  /// casts userdata to ggml_opt_optimizer_params and returns it
  ggml_opt_optimizer_params ggml_opt_get_constant_optimizer_params(
    ffi.Pointer<ffi.Void> userdata,
  ) {
    return _ggml_opt_get_constant_optimizer_params(userdata);
  }

  late final _ggml_opt_get_constant_optimizer_paramsPtr =
      _lookup<
        ffi.NativeFunction<
          ggml_opt_optimizer_params Function(ffi.Pointer<ffi.Void>)
        >
      >('ggml_opt_get_constant_optimizer_params');
  late final _ggml_opt_get_constant_optimizer_params =
      _ggml_opt_get_constant_optimizer_paramsPtr
          .asFunction<
            ggml_opt_optimizer_params Function(ffi.Pointer<ffi.Void>)
          >();

  /// get parameters for an optimization context with defaults set where possible
  /// parameters for which no sensible defaults exist are supplied as arguments to this function
  ggml_opt_params ggml_opt_default_params(
    ggml_backend_sched_t backend_sched,
    ggml_opt_loss_type loss_type,
  ) {
    return _ggml_opt_default_params(backend_sched, loss_type.value);
  }

  late final _ggml_opt_default_paramsPtr =
      _lookup<
        ffi.NativeFunction<
          ggml_opt_params Function(ggml_backend_sched_t, ffi.UnsignedInt)
        >
      >('ggml_opt_default_params');
  late final _ggml_opt_default_params = _ggml_opt_default_paramsPtr
      .asFunction<ggml_opt_params Function(ggml_backend_sched_t, int)>();

  ggml_opt_context_t ggml_opt_init(ggml_opt_params params) {
    return _ggml_opt_init(params);
  }

  late final _ggml_opt_initPtr =
      _lookup<ffi.NativeFunction<ggml_opt_context_t Function(ggml_opt_params)>>(
        'ggml_opt_init',
      );
  late final _ggml_opt_init = _ggml_opt_initPtr
      .asFunction<ggml_opt_context_t Function(ggml_opt_params)>();

  void ggml_opt_free(ggml_opt_context_t opt_ctx) {
    return _ggml_opt_free(opt_ctx);
  }

  late final _ggml_opt_freePtr =
      _lookup<ffi.NativeFunction<ffi.Void Function(ggml_opt_context_t)>>(
        'ggml_opt_free',
      );
  late final _ggml_opt_free = _ggml_opt_freePtr
      .asFunction<void Function(ggml_opt_context_t)>();

  /// set gradients to zero, initilize loss, and optionally reset the optimizer
  void ggml_opt_reset(ggml_opt_context_t opt_ctx, bool optimizer) {
    return _ggml_opt_reset(opt_ctx, optimizer);
  }

  late final _ggml_opt_resetPtr =
      _lookup<
        ffi.NativeFunction<ffi.Void Function(ggml_opt_context_t, ffi.Bool)>
      >('ggml_opt_reset');
  late final _ggml_opt_reset = _ggml_opt_resetPtr
      .asFunction<void Function(ggml_opt_context_t, bool)>();

  bool ggml_opt_static_graphs(ggml_opt_context_t opt_ctx) {
    return _ggml_opt_static_graphs(opt_ctx);
  }

  late final _ggml_opt_static_graphsPtr =
      _lookup<ffi.NativeFunction<ffi.Bool Function(ggml_opt_context_t)>>(
        'ggml_opt_static_graphs',
      );
  late final _ggml_opt_static_graphs = _ggml_opt_static_graphsPtr
      .asFunction<bool Function(ggml_opt_context_t)>();

  /// get underlying tensors that store data
  /// if not using static graphs these pointers become invalid with the next call to ggml_opt_alloc
  ffi.Pointer<ggml_tensor> ggml_opt_inputs(ggml_opt_context_t opt_ctx) {
    return _ggml_opt_inputs(opt_ctx);
  }

  late final _ggml_opt_inputsPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(ggml_opt_context_t)
        >
      >('ggml_opt_inputs');
  late final _ggml_opt_inputs = _ggml_opt_inputsPtr
      .asFunction<ffi.Pointer<ggml_tensor> Function(ggml_opt_context_t)>();

  ffi.Pointer<ggml_tensor> ggml_opt_outputs(ggml_opt_context_t opt_ctx) {
    return _ggml_opt_outputs(opt_ctx);
  }

  late final _ggml_opt_outputsPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(ggml_opt_context_t)
        >
      >('ggml_opt_outputs');
  late final _ggml_opt_outputs = _ggml_opt_outputsPtr
      .asFunction<ffi.Pointer<ggml_tensor> Function(ggml_opt_context_t)>();

  ffi.Pointer<ggml_tensor> ggml_opt_labels(ggml_opt_context_t opt_ctx) {
    return _ggml_opt_labels(opt_ctx);
  }

  late final _ggml_opt_labelsPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(ggml_opt_context_t)
        >
      >('ggml_opt_labels');
  late final _ggml_opt_labels = _ggml_opt_labelsPtr
      .asFunction<ffi.Pointer<ggml_tensor> Function(ggml_opt_context_t)>();

  ffi.Pointer<ggml_tensor> ggml_opt_loss(ggml_opt_context_t opt_ctx) {
    return _ggml_opt_loss(opt_ctx);
  }

  late final _ggml_opt_lossPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(ggml_opt_context_t)
        >
      >('ggml_opt_loss');
  late final _ggml_opt_loss = _ggml_opt_lossPtr
      .asFunction<ffi.Pointer<ggml_tensor> Function(ggml_opt_context_t)>();

  ffi.Pointer<ggml_tensor> ggml_opt_pred(ggml_opt_context_t opt_ctx) {
    return _ggml_opt_pred(opt_ctx);
  }

  late final _ggml_opt_predPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(ggml_opt_context_t)
        >
      >('ggml_opt_pred');
  late final _ggml_opt_pred = _ggml_opt_predPtr
      .asFunction<ffi.Pointer<ggml_tensor> Function(ggml_opt_context_t)>();

  ffi.Pointer<ggml_tensor> ggml_opt_ncorrect(ggml_opt_context_t opt_ctx) {
    return _ggml_opt_ncorrect(opt_ctx);
  }

  late final _ggml_opt_ncorrectPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(ggml_opt_context_t)
        >
      >('ggml_opt_ncorrect');
  late final _ggml_opt_ncorrect = _ggml_opt_ncorrectPtr
      .asFunction<ffi.Pointer<ggml_tensor> Function(ggml_opt_context_t)>();

  /// get the gradient accumulator for a node from the forward graph
  ffi.Pointer<ggml_tensor> ggml_opt_grad_acc(
    ggml_opt_context_t opt_ctx,
    ffi.Pointer<ggml_tensor> node,
  ) {
    return _ggml_opt_grad_acc(opt_ctx, node);
  }

  late final _ggml_opt_grad_accPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
            ggml_opt_context_t,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_opt_grad_acc');
  late final _ggml_opt_grad_acc = _ggml_opt_grad_accPtr
      .asFunction<
        ffi.Pointer<ggml_tensor> Function(
          ggml_opt_context_t,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  ggml_opt_optimizer_type ggml_opt_context_optimizer_type(
    ggml_opt_context_t arg0,
  ) {
    return ggml_opt_optimizer_type.fromValue(
      _ggml_opt_context_optimizer_type(arg0),
    );
  }

  late final _ggml_opt_context_optimizer_typePtr =
      _lookup<ffi.NativeFunction<ffi.UnsignedInt Function(ggml_opt_context_t)>>(
        'ggml_opt_context_optimizer_type',
      );
  late final _ggml_opt_context_optimizer_type =
      _ggml_opt_context_optimizer_typePtr
          .asFunction<int Function(ggml_opt_context_t)>();

  ffi.Pointer<ffi.Char> ggml_opt_optimizer_name(ggml_opt_optimizer_type arg0) {
    return _ggml_opt_optimizer_name(arg0.value);
  }

  late final _ggml_opt_optimizer_namePtr =
      _lookup<
        ffi.NativeFunction<ffi.Pointer<ffi.Char> Function(ffi.UnsignedInt)>
      >('ggml_opt_optimizer_name');
  late final _ggml_opt_optimizer_name = _ggml_opt_optimizer_namePtr
      .asFunction<ffi.Pointer<ffi.Char> Function(int)>();

  /// ====== Optimization Result ======
  ggml_opt_result_t ggml_opt_result_init() {
    return _ggml_opt_result_init();
  }

  late final _ggml_opt_result_initPtr =
      _lookup<ffi.NativeFunction<ggml_opt_result_t Function()>>(
        'ggml_opt_result_init',
      );
  late final _ggml_opt_result_init = _ggml_opt_result_initPtr
      .asFunction<ggml_opt_result_t Function()>();

  void ggml_opt_result_free(ggml_opt_result_t result) {
    return _ggml_opt_result_free(result);
  }

  late final _ggml_opt_result_freePtr =
      _lookup<ffi.NativeFunction<ffi.Void Function(ggml_opt_result_t)>>(
        'ggml_opt_result_free',
      );
  late final _ggml_opt_result_free = _ggml_opt_result_freePtr
      .asFunction<void Function(ggml_opt_result_t)>();

  void ggml_opt_result_reset(ggml_opt_result_t result) {
    return _ggml_opt_result_reset(result);
  }

  late final _ggml_opt_result_resetPtr =
      _lookup<ffi.NativeFunction<ffi.Void Function(ggml_opt_result_t)>>(
        'ggml_opt_result_reset',
      );
  late final _ggml_opt_result_reset = _ggml_opt_result_resetPtr
      .asFunction<void Function(ggml_opt_result_t)>();

  /// get data from result, uncertainties are optional and can be ignored by passing NULL
  void ggml_opt_result_ndata(
    ggml_opt_result_t result,
    ffi.Pointer<ffi.Int64> ndata,
  ) {
    return _ggml_opt_result_ndata(result, ndata);
  }

  late final _ggml_opt_result_ndataPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Void Function(ggml_opt_result_t, ffi.Pointer<ffi.Int64>)
        >
      >('ggml_opt_result_ndata');
  late final _ggml_opt_result_ndata = _ggml_opt_result_ndataPtr
      .asFunction<void Function(ggml_opt_result_t, ffi.Pointer<ffi.Int64>)>();

  void ggml_opt_result_loss(
    ggml_opt_result_t result,
    ffi.Pointer<ffi.Double> loss,
    ffi.Pointer<ffi.Double> unc,
  ) {
    return _ggml_opt_result_loss(result, loss, unc);
  }

  late final _ggml_opt_result_lossPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Void Function(
            ggml_opt_result_t,
            ffi.Pointer<ffi.Double>,
            ffi.Pointer<ffi.Double>,
          )
        >
      >('ggml_opt_result_loss');
  late final _ggml_opt_result_loss = _ggml_opt_result_lossPtr
      .asFunction<
        void Function(
          ggml_opt_result_t,
          ffi.Pointer<ffi.Double>,
          ffi.Pointer<ffi.Double>,
        )
      >();

  void ggml_opt_result_pred(
    ggml_opt_result_t result,
    ffi.Pointer<ffi.Int32> pred,
  ) {
    return _ggml_opt_result_pred(result, pred);
  }

  late final _ggml_opt_result_predPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Void Function(ggml_opt_result_t, ffi.Pointer<ffi.Int32>)
        >
      >('ggml_opt_result_pred');
  late final _ggml_opt_result_pred = _ggml_opt_result_predPtr
      .asFunction<void Function(ggml_opt_result_t, ffi.Pointer<ffi.Int32>)>();

  void ggml_opt_result_accuracy(
    ggml_opt_result_t result,
    ffi.Pointer<ffi.Double> accuracy,
    ffi.Pointer<ffi.Double> unc,
  ) {
    return _ggml_opt_result_accuracy(result, accuracy, unc);
  }

  late final _ggml_opt_result_accuracyPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Void Function(
            ggml_opt_result_t,
            ffi.Pointer<ffi.Double>,
            ffi.Pointer<ffi.Double>,
          )
        >
      >('ggml_opt_result_accuracy');
  late final _ggml_opt_result_accuracy = _ggml_opt_result_accuracyPtr
      .asFunction<
        void Function(
          ggml_opt_result_t,
          ffi.Pointer<ffi.Double>,
          ffi.Pointer<ffi.Double>,
        )
      >();

  /// if not using static graphs, this function must be called prior to ggml_opt_alloc
  void ggml_opt_prepare_alloc(
    ggml_opt_context_t opt_ctx,
    ffi.Pointer<ggml_context> ctx_compute,
    ffi.Pointer<ggml_cgraph> gf,
    ffi.Pointer<ggml_tensor> inputs,
    ffi.Pointer<ggml_tensor> outputs,
  ) {
    return _ggml_opt_prepare_alloc(opt_ctx, ctx_compute, gf, inputs, outputs);
  }

  late final _ggml_opt_prepare_allocPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Void Function(
            ggml_opt_context_t,
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_cgraph>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
          )
        >
      >('ggml_opt_prepare_alloc');
  late final _ggml_opt_prepare_alloc = _ggml_opt_prepare_allocPtr
      .asFunction<
        void Function(
          ggml_opt_context_t,
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_cgraph>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
        )
      >();

  /// allocate the next graph for evaluation, either forward or forward + backward
  /// must be called exactly once prior to calling ggml_opt_eval
  void ggml_opt_alloc(ggml_opt_context_t opt_ctx, bool backward) {
    return _ggml_opt_alloc(opt_ctx, backward);
  }

  late final _ggml_opt_allocPtr =
      _lookup<
        ffi.NativeFunction<ffi.Void Function(ggml_opt_context_t, ffi.Bool)>
      >('ggml_opt_alloc');
  late final _ggml_opt_alloc = _ggml_opt_allocPtr
      .asFunction<void Function(ggml_opt_context_t, bool)>();

  /// do forward pass, increment result if not NULL, do backward pass if allocated
  void ggml_opt_eval(ggml_opt_context_t opt_ctx, ggml_opt_result_t result) {
    return _ggml_opt_eval(opt_ctx, result);
  }

  late final _ggml_opt_evalPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Void Function(ggml_opt_context_t, ggml_opt_result_t)
        >
      >('ggml_opt_eval');
  late final _ggml_opt_eval = _ggml_opt_evalPtr
      .asFunction<void Function(ggml_opt_context_t, ggml_opt_result_t)>();

  /// do training on front of dataset, do evaluation only on back of dataset
  void ggml_opt_epoch(
    ggml_opt_context_t opt_ctx,
    ggml_opt_dataset_t dataset,
    ggml_opt_result_t result_train,
    ggml_opt_result_t result_eval,
    int idata_split,
    ggml_opt_epoch_callback callback_train,
    ggml_opt_epoch_callback callback_eval,
  ) {
    return _ggml_opt_epoch(
      opt_ctx,
      dataset,
      result_train,
      result_eval,
      idata_split,
      callback_train,
      callback_eval,
    );
  }

  late final _ggml_opt_epochPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Void Function(
            ggml_opt_context_t,
            ggml_opt_dataset_t,
            ggml_opt_result_t,
            ggml_opt_result_t,
            ffi.Int64,
            ggml_opt_epoch_callback,
            ggml_opt_epoch_callback,
          )
        >
      >('ggml_opt_epoch');
  late final _ggml_opt_epoch = _ggml_opt_epochPtr
      .asFunction<
        void Function(
          ggml_opt_context_t,
          ggml_opt_dataset_t,
          ggml_opt_result_t,
          ggml_opt_result_t,
          int,
          ggml_opt_epoch_callback,
          ggml_opt_epoch_callback,
        )
      >();

  /// callback that prints a progress bar on stderr
  void ggml_opt_epoch_callback_progress_bar(
    bool train,
    ggml_opt_context_t opt_ctx,
    ggml_opt_dataset_t dataset,
    ggml_opt_result_t result,
    int ibatch,
    int ibatch_max,
    int t_start_us,
  ) {
    return _ggml_opt_epoch_callback_progress_bar(
      train,
      opt_ctx,
      dataset,
      result,
      ibatch,
      ibatch_max,
      t_start_us,
    );
  }

  late final _ggml_opt_epoch_callback_progress_barPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Void Function(
            ffi.Bool,
            ggml_opt_context_t,
            ggml_opt_dataset_t,
            ggml_opt_result_t,
            ffi.Int64,
            ffi.Int64,
            ffi.Int64,
          )
        >
      >('ggml_opt_epoch_callback_progress_bar');
  late final _ggml_opt_epoch_callback_progress_bar =
      _ggml_opt_epoch_callback_progress_barPtr
          .asFunction<
            void Function(
              bool,
              ggml_opt_context_t,
              ggml_opt_dataset_t,
              ggml_opt_result_t,
              int,
              int,
              int,
            )
          >();

  /// fit model defined by inputs and outputs to dataset
  void ggml_opt_fit(
    ggml_backend_sched_t backend_sched,
    ffi.Pointer<ggml_context> ctx_compute,
    ffi.Pointer<ggml_tensor> inputs,
    ffi.Pointer<ggml_tensor> outputs,
    ggml_opt_dataset_t dataset,
    ggml_opt_loss_type loss_type,
    ggml_opt_optimizer_type optimizer,
    ggml_opt_get_optimizer_params get_opt_pars,
    int nepoch,
    int nbatch_logical,
    double val_split,
    bool silent,
  ) {
    return _ggml_opt_fit(
      backend_sched,
      ctx_compute,
      inputs,
      outputs,
      dataset,
      loss_type.value,
      optimizer.value,
      get_opt_pars,
      nepoch,
      nbatch_logical,
      val_split,
      silent,
    );
  }

  late final _ggml_opt_fitPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Void Function(
            ggml_backend_sched_t,
            ffi.Pointer<ggml_context>,
            ffi.Pointer<ggml_tensor>,
            ffi.Pointer<ggml_tensor>,
            ggml_opt_dataset_t,
            ffi.UnsignedInt,
            ffi.UnsignedInt,
            ggml_opt_get_optimizer_params,
            ffi.Int64,
            ffi.Int64,
            ffi.Float,
            ffi.Bool,
          )
        >
      >('ggml_opt_fit');
  late final _ggml_opt_fit = _ggml_opt_fitPtr
      .asFunction<
        void Function(
          ggml_backend_sched_t,
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          ggml_opt_dataset_t,
          int,
          int,
          ggml_opt_get_optimizer_params,
          int,
          int,
          double,
          bool,
        )
      >();

  ffi.Pointer<ffi.Char> llama_flash_attn_type_name(
    llama_flash_attn_type flash_attn_type,
  ) {
    return _llama_flash_attn_type_name(flash_attn_type.value);
  }

  late final _llama_flash_attn_type_namePtr =
      _lookup<ffi.NativeFunction<ffi.Pointer<ffi.Char> Function(ffi.Int)>>(
        'llama_flash_attn_type_name',
      );
  late final _llama_flash_attn_type_name = _llama_flash_attn_type_namePtr
      .asFunction<ffi.Pointer<ffi.Char> Function(int)>();

  /// Helpers for getting default parameters
  /// TODO: update API to start accepting pointers to params structs (https://github.com/ggml-org/llama.cpp/discussions/9172)
  llama_model_params llama_model_default_params() {
    return _llama_model_default_params();
  }

  late final _llama_model_default_paramsPtr =
      _lookup<ffi.NativeFunction<llama_model_params Function()>>(
        'llama_model_default_params',
      );
  late final _llama_model_default_params = _llama_model_default_paramsPtr
      .asFunction<llama_model_params Function()>();

  llama_context_params llama_context_default_params() {
    return _llama_context_default_params();
  }

  late final _llama_context_default_paramsPtr =
      _lookup<ffi.NativeFunction<llama_context_params Function()>>(
        'llama_context_default_params',
      );
  late final _llama_context_default_params = _llama_context_default_paramsPtr
      .asFunction<llama_context_params Function()>();

  llama_sampler_chain_params llama_sampler_chain_default_params() {
    return _llama_sampler_chain_default_params();
  }

  late final _llama_sampler_chain_default_paramsPtr =
      _lookup<ffi.NativeFunction<llama_sampler_chain_params Function()>>(
        'llama_sampler_chain_default_params',
      );
  late final _llama_sampler_chain_default_params =
      _llama_sampler_chain_default_paramsPtr
          .asFunction<llama_sampler_chain_params Function()>();

  llama_model_quantize_params llama_model_quantize_default_params() {
    return _llama_model_quantize_default_params();
  }

  late final _llama_model_quantize_default_paramsPtr =
      _lookup<ffi.NativeFunction<llama_model_quantize_params Function()>>(
        'llama_model_quantize_default_params',
      );
  late final _llama_model_quantize_default_params =
      _llama_model_quantize_default_paramsPtr
          .asFunction<llama_model_quantize_params Function()>();

  /// Initialize the llama + ggml backend
  /// If numa is true, use NUMA optimizations
  /// Call once at the start of the program
  void llama_backend_init() {
    return _llama_backend_init();
  }

  late final _llama_backend_initPtr =
      _lookup<ffi.NativeFunction<ffi.Void Function()>>('llama_backend_init');
  late final _llama_backend_init = _llama_backend_initPtr
      .asFunction<void Function()>();

  /// Call once at the end of the program - currently only used for MPI
  void llama_backend_free() {
    return _llama_backend_free();
  }

  late final _llama_backend_freePtr =
      _lookup<ffi.NativeFunction<ffi.Void Function()>>('llama_backend_free');
  late final _llama_backend_free = _llama_backend_freePtr
      .asFunction<void Function()>();

  /// optional:
  void llama_numa_init(ggml_numa_strategy numa) {
    return _llama_numa_init(numa.value);
  }

  late final _llama_numa_initPtr =
      _lookup<ffi.NativeFunction<ffi.Void Function(ffi.UnsignedInt)>>(
        'llama_numa_init',
      );
  late final _llama_numa_init = _llama_numa_initPtr
      .asFunction<void Function(int)>();

  /// Optional: an auto threadpool gets created in ggml if not passed explicitly
  void llama_attach_threadpool(
    ffi.Pointer<llama_context> ctx,
    ggml_threadpool_t threadpool,
    ggml_threadpool_t threadpool_batch,
  ) {
    return _llama_attach_threadpool(ctx, threadpool, threadpool_batch);
  }

  late final _llama_attach_threadpoolPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Void Function(
            ffi.Pointer<llama_context>,
            ggml_threadpool_t,
            ggml_threadpool_t,
          )
        >
      >('llama_attach_threadpool');
  late final _llama_attach_threadpool = _llama_attach_threadpoolPtr
      .asFunction<
        void Function(
          ffi.Pointer<llama_context>,
          ggml_threadpool_t,
          ggml_threadpool_t,
        )
      >();

  void llama_detach_threadpool(ffi.Pointer<llama_context> ctx) {
    return _llama_detach_threadpool(ctx);
  }

  late final _llama_detach_threadpoolPtr =
      _lookup<
        ffi.NativeFunction<ffi.Void Function(ffi.Pointer<llama_context>)>
      >('llama_detach_threadpool');
  late final _llama_detach_threadpool = _llama_detach_threadpoolPtr
      .asFunction<void Function(ffi.Pointer<llama_context>)>();

  ffi.Pointer<llama_model> llama_load_model_from_file(
    ffi.Pointer<ffi.Char> path_model,
    llama_model_params params,
  ) {
    return _llama_load_model_from_file(path_model, params);
  }

  late final _llama_load_model_from_filePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<llama_model> Function(
            ffi.Pointer<ffi.Char>,
            llama_model_params,
          )
        >
      >('llama_load_model_from_file');
  late final _llama_load_model_from_file = _llama_load_model_from_filePtr
      .asFunction<
        ffi.Pointer<llama_model> Function(
          ffi.Pointer<ffi.Char>,
          llama_model_params,
        )
      >();

  /// Load the model from a file
  /// If the file is split into multiple parts, the file name must follow this pattern: <name>-%05d-of-%05d.gguf
  /// If the split file name does not follow this pattern, use llama_model_load_from_splits
  ffi.Pointer<llama_model> llama_model_load_from_file(
    ffi.Pointer<ffi.Char> path_model,
    llama_model_params params,
  ) {
    return _llama_model_load_from_file(path_model, params);
  }

  late final _llama_model_load_from_filePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<llama_model> Function(
            ffi.Pointer<ffi.Char>,
            llama_model_params,
          )
        >
      >('llama_model_load_from_file');
  late final _llama_model_load_from_file = _llama_model_load_from_filePtr
      .asFunction<
        ffi.Pointer<llama_model> Function(
          ffi.Pointer<ffi.Char>,
          llama_model_params,
        )
      >();

  /// Load the model from multiple splits (support custom naming scheme)
  /// The paths must be in the correct order
  ffi.Pointer<llama_model> llama_model_load_from_splits(
    ffi.Pointer<ffi.Pointer<ffi.Char>> paths,
    int n_paths,
    llama_model_params params,
  ) {
    return _llama_model_load_from_splits(paths, n_paths, params);
  }

  late final _llama_model_load_from_splitsPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<llama_model> Function(
            ffi.Pointer<ffi.Pointer<ffi.Char>>,
            ffi.Size,
            llama_model_params,
          )
        >
      >('llama_model_load_from_splits');
  late final _llama_model_load_from_splits = _llama_model_load_from_splitsPtr
      .asFunction<
        ffi.Pointer<llama_model> Function(
          ffi.Pointer<ffi.Pointer<ffi.Char>>,
          int,
          llama_model_params,
        )
      >();

  void llama_model_save_to_file(
    ffi.Pointer<llama_model> model,
    ffi.Pointer<ffi.Char> path_model,
  ) {
    return _llama_model_save_to_file(model, path_model);
  }

  late final _llama_model_save_to_filePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<llama_model>, ffi.Pointer<ffi.Char>)
        >
      >('llama_model_save_to_file');
  late final _llama_model_save_to_file = _llama_model_save_to_filePtr
      .asFunction<
        void Function(ffi.Pointer<llama_model>, ffi.Pointer<ffi.Char>)
      >();

  void llama_free_model(ffi.Pointer<llama_model> model) {
    return _llama_free_model(model);
  }

  late final _llama_free_modelPtr =
      _lookup<ffi.NativeFunction<ffi.Void Function(ffi.Pointer<llama_model>)>>(
        'llama_free_model',
      );
  late final _llama_free_model = _llama_free_modelPtr
      .asFunction<void Function(ffi.Pointer<llama_model>)>();

  void llama_model_free(ffi.Pointer<llama_model> model) {
    return _llama_model_free(model);
  }

  late final _llama_model_freePtr =
      _lookup<ffi.NativeFunction<ffi.Void Function(ffi.Pointer<llama_model>)>>(
        'llama_model_free',
      );
  late final _llama_model_free = _llama_model_freePtr
      .asFunction<void Function(ffi.Pointer<llama_model>)>();

  ffi.Pointer<llama_context> llama_init_from_model(
    ffi.Pointer<llama_model> model,
    llama_context_params params,
  ) {
    return _llama_init_from_model(model, params);
  }

  late final _llama_init_from_modelPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<llama_context> Function(
            ffi.Pointer<llama_model>,
            llama_context_params,
          )
        >
      >('llama_init_from_model');
  late final _llama_init_from_model = _llama_init_from_modelPtr
      .asFunction<
        ffi.Pointer<llama_context> Function(
          ffi.Pointer<llama_model>,
          llama_context_params,
        )
      >();

  ffi.Pointer<llama_context> llama_new_context_with_model(
    ffi.Pointer<llama_model> model,
    llama_context_params params,
  ) {
    return _llama_new_context_with_model(model, params);
  }

  late final _llama_new_context_with_modelPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<llama_context> Function(
            ffi.Pointer<llama_model>,
            llama_context_params,
          )
        >
      >('llama_new_context_with_model');
  late final _llama_new_context_with_model = _llama_new_context_with_modelPtr
      .asFunction<
        ffi.Pointer<llama_context> Function(
          ffi.Pointer<llama_model>,
          llama_context_params,
        )
      >();

  /// Frees all allocated memory
  void llama_free(ffi.Pointer<llama_context> ctx) {
    return _llama_free(ctx);
  }

  late final _llama_freePtr =
      _lookup<
        ffi.NativeFunction<ffi.Void Function(ffi.Pointer<llama_context>)>
      >('llama_free');
  late final _llama_free = _llama_freePtr
      .asFunction<void Function(ffi.Pointer<llama_context>)>();

  int llama_time_us() {
    return _llama_time_us();
  }

  late final _llama_time_usPtr =
      _lookup<ffi.NativeFunction<ffi.Int64 Function()>>('llama_time_us');
  late final _llama_time_us = _llama_time_usPtr.asFunction<int Function()>();

  int llama_max_devices() {
    return _llama_max_devices();
  }

  late final _llama_max_devicesPtr =
      _lookup<ffi.NativeFunction<ffi.Size Function()>>('llama_max_devices');
  late final _llama_max_devices = _llama_max_devicesPtr
      .asFunction<int Function()>();

  int llama_max_parallel_sequences() {
    return _llama_max_parallel_sequences();
  }

  late final _llama_max_parallel_sequencesPtr =
      _lookup<ffi.NativeFunction<ffi.Size Function()>>(
        'llama_max_parallel_sequences',
      );
  late final _llama_max_parallel_sequences = _llama_max_parallel_sequencesPtr
      .asFunction<int Function()>();

  bool llama_supports_mmap() {
    return _llama_supports_mmap();
  }

  late final _llama_supports_mmapPtr =
      _lookup<ffi.NativeFunction<ffi.Bool Function()>>('llama_supports_mmap');
  late final _llama_supports_mmap = _llama_supports_mmapPtr
      .asFunction<bool Function()>();

  bool llama_supports_mlock() {
    return _llama_supports_mlock();
  }

  late final _llama_supports_mlockPtr =
      _lookup<ffi.NativeFunction<ffi.Bool Function()>>('llama_supports_mlock');
  late final _llama_supports_mlock = _llama_supports_mlockPtr
      .asFunction<bool Function()>();

  bool llama_supports_gpu_offload() {
    return _llama_supports_gpu_offload();
  }

  late final _llama_supports_gpu_offloadPtr =
      _lookup<ffi.NativeFunction<ffi.Bool Function()>>(
        'llama_supports_gpu_offload',
      );
  late final _llama_supports_gpu_offload = _llama_supports_gpu_offloadPtr
      .asFunction<bool Function()>();

  bool llama_supports_rpc() {
    return _llama_supports_rpc();
  }

  late final _llama_supports_rpcPtr =
      _lookup<ffi.NativeFunction<ffi.Bool Function()>>('llama_supports_rpc');
  late final _llama_supports_rpc = _llama_supports_rpcPtr
      .asFunction<bool Function()>();

  /// NOTE: After creating a llama_context, it is recommended to query the actual values using these functions
  /// In some cases the requested values via llama_context_params may differ from the actual values used by the context
  /// ref: https://github.com/ggml-org/llama.cpp/pull/17046#discussion_r2503085732
  int llama_n_ctx(ffi.Pointer<llama_context> ctx) {
    return _llama_n_ctx(ctx);
  }

  late final _llama_n_ctxPtr =
      _lookup<
        ffi.NativeFunction<ffi.Uint32 Function(ffi.Pointer<llama_context>)>
      >('llama_n_ctx');
  late final _llama_n_ctx = _llama_n_ctxPtr
      .asFunction<int Function(ffi.Pointer<llama_context>)>();

  int llama_n_ctx_seq(ffi.Pointer<llama_context> ctx) {
    return _llama_n_ctx_seq(ctx);
  }

  late final _llama_n_ctx_seqPtr =
      _lookup<
        ffi.NativeFunction<ffi.Uint32 Function(ffi.Pointer<llama_context>)>
      >('llama_n_ctx_seq');
  late final _llama_n_ctx_seq = _llama_n_ctx_seqPtr
      .asFunction<int Function(ffi.Pointer<llama_context>)>();

  int llama_n_batch(ffi.Pointer<llama_context> ctx) {
    return _llama_n_batch(ctx);
  }

  late final _llama_n_batchPtr =
      _lookup<
        ffi.NativeFunction<ffi.Uint32 Function(ffi.Pointer<llama_context>)>
      >('llama_n_batch');
  late final _llama_n_batch = _llama_n_batchPtr
      .asFunction<int Function(ffi.Pointer<llama_context>)>();

  int llama_n_ubatch(ffi.Pointer<llama_context> ctx) {
    return _llama_n_ubatch(ctx);
  }

  late final _llama_n_ubatchPtr =
      _lookup<
        ffi.NativeFunction<ffi.Uint32 Function(ffi.Pointer<llama_context>)>
      >('llama_n_ubatch');
  late final _llama_n_ubatch = _llama_n_ubatchPtr
      .asFunction<int Function(ffi.Pointer<llama_context>)>();

  int llama_n_seq_max(ffi.Pointer<llama_context> ctx) {
    return _llama_n_seq_max(ctx);
  }

  late final _llama_n_seq_maxPtr =
      _lookup<
        ffi.NativeFunction<ffi.Uint32 Function(ffi.Pointer<llama_context>)>
      >('llama_n_seq_max');
  late final _llama_n_seq_max = _llama_n_seq_maxPtr
      .asFunction<int Function(ffi.Pointer<llama_context>)>();

  int llama_n_ctx_train(ffi.Pointer<llama_model> model) {
    return _llama_n_ctx_train(model);
  }

  late final _llama_n_ctx_trainPtr =
      _lookup<ffi.NativeFunction<ffi.Int32 Function(ffi.Pointer<llama_model>)>>(
        'llama_n_ctx_train',
      );
  late final _llama_n_ctx_train = _llama_n_ctx_trainPtr
      .asFunction<int Function(ffi.Pointer<llama_model>)>();

  int llama_n_embd(ffi.Pointer<llama_model> model) {
    return _llama_n_embd(model);
  }

  late final _llama_n_embdPtr =
      _lookup<ffi.NativeFunction<ffi.Int32 Function(ffi.Pointer<llama_model>)>>(
        'llama_n_embd',
      );
  late final _llama_n_embd = _llama_n_embdPtr
      .asFunction<int Function(ffi.Pointer<llama_model>)>();

  int llama_n_layer(ffi.Pointer<llama_model> model) {
    return _llama_n_layer(model);
  }

  late final _llama_n_layerPtr =
      _lookup<ffi.NativeFunction<ffi.Int32 Function(ffi.Pointer<llama_model>)>>(
        'llama_n_layer',
      );
  late final _llama_n_layer = _llama_n_layerPtr
      .asFunction<int Function(ffi.Pointer<llama_model>)>();

  int llama_n_head(ffi.Pointer<llama_model> model) {
    return _llama_n_head(model);
  }

  late final _llama_n_headPtr =
      _lookup<ffi.NativeFunction<ffi.Int32 Function(ffi.Pointer<llama_model>)>>(
        'llama_n_head',
      );
  late final _llama_n_head = _llama_n_headPtr
      .asFunction<int Function(ffi.Pointer<llama_model>)>();

  int llama_n_vocab(ffi.Pointer<llama_vocab> vocab) {
    return _llama_n_vocab(vocab);
  }

  late final _llama_n_vocabPtr =
      _lookup<ffi.NativeFunction<ffi.Int32 Function(ffi.Pointer<llama_vocab>)>>(
        'llama_n_vocab',
      );
  late final _llama_n_vocab = _llama_n_vocabPtr
      .asFunction<int Function(ffi.Pointer<llama_vocab>)>();

  ffi.Pointer<llama_model> llama_get_model(ffi.Pointer<llama_context> ctx) {
    return _llama_get_model(ctx);
  }

  late final _llama_get_modelPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<llama_model> Function(ffi.Pointer<llama_context>)
        >
      >('llama_get_model');
  late final _llama_get_model = _llama_get_modelPtr
      .asFunction<
        ffi.Pointer<llama_model> Function(ffi.Pointer<llama_context>)
      >();

  llama_memory_t llama_get_memory(ffi.Pointer<llama_context> ctx) {
    return _llama_get_memory(ctx);
  }

  late final _llama_get_memoryPtr =
      _lookup<
        ffi.NativeFunction<llama_memory_t Function(ffi.Pointer<llama_context>)>
      >('llama_get_memory');
  late final _llama_get_memory = _llama_get_memoryPtr
      .asFunction<llama_memory_t Function(ffi.Pointer<llama_context>)>();

  llama_pooling_type llama_pooling_type$1(ffi.Pointer<llama_context> ctx) {
    return llama_pooling_type.fromValue(_llama_pooling_type$1(ctx));
  }

  late final _llama_pooling_type$1Ptr =
      _lookup<ffi.NativeFunction<ffi.Int Function(ffi.Pointer<llama_context>)>>(
        'llama_pooling_type',
      );
  late final _llama_pooling_type$1 = _llama_pooling_type$1Ptr
      .asFunction<int Function(ffi.Pointer<llama_context>)>();

  ffi.Pointer<llama_vocab> llama_model_get_vocab(
    ffi.Pointer<llama_model> model,
  ) {
    return _llama_model_get_vocab(model);
  }

  late final _llama_model_get_vocabPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<llama_vocab> Function(ffi.Pointer<llama_model>)
        >
      >('llama_model_get_vocab');
  late final _llama_model_get_vocab = _llama_model_get_vocabPtr
      .asFunction<
        ffi.Pointer<llama_vocab> Function(ffi.Pointer<llama_model>)
      >();

  llama_rope_type llama_model_rope_type(ffi.Pointer<llama_model> model) {
    return llama_rope_type.fromValue(_llama_model_rope_type(model));
  }

  late final _llama_model_rope_typePtr =
      _lookup<ffi.NativeFunction<ffi.Int Function(ffi.Pointer<llama_model>)>>(
        'llama_model_rope_type',
      );
  late final _llama_model_rope_type = _llama_model_rope_typePtr
      .asFunction<int Function(ffi.Pointer<llama_model>)>();

  int llama_model_n_ctx_train(ffi.Pointer<llama_model> model) {
    return _llama_model_n_ctx_train(model);
  }

  late final _llama_model_n_ctx_trainPtr =
      _lookup<ffi.NativeFunction<ffi.Int32 Function(ffi.Pointer<llama_model>)>>(
        'llama_model_n_ctx_train',
      );
  late final _llama_model_n_ctx_train = _llama_model_n_ctx_trainPtr
      .asFunction<int Function(ffi.Pointer<llama_model>)>();

  int llama_model_n_embd(ffi.Pointer<llama_model> model) {
    return _llama_model_n_embd(model);
  }

  late final _llama_model_n_embdPtr =
      _lookup<ffi.NativeFunction<ffi.Int32 Function(ffi.Pointer<llama_model>)>>(
        'llama_model_n_embd',
      );
  late final _llama_model_n_embd = _llama_model_n_embdPtr
      .asFunction<int Function(ffi.Pointer<llama_model>)>();

  int llama_model_n_embd_inp(ffi.Pointer<llama_model> model) {
    return _llama_model_n_embd_inp(model);
  }

  late final _llama_model_n_embd_inpPtr =
      _lookup<ffi.NativeFunction<ffi.Int32 Function(ffi.Pointer<llama_model>)>>(
        'llama_model_n_embd_inp',
      );
  late final _llama_model_n_embd_inp = _llama_model_n_embd_inpPtr
      .asFunction<int Function(ffi.Pointer<llama_model>)>();

  int llama_model_n_layer(ffi.Pointer<llama_model> model) {
    return _llama_model_n_layer(model);
  }

  late final _llama_model_n_layerPtr =
      _lookup<ffi.NativeFunction<ffi.Int32 Function(ffi.Pointer<llama_model>)>>(
        'llama_model_n_layer',
      );
  late final _llama_model_n_layer = _llama_model_n_layerPtr
      .asFunction<int Function(ffi.Pointer<llama_model>)>();

  int llama_model_n_head(ffi.Pointer<llama_model> model) {
    return _llama_model_n_head(model);
  }

  late final _llama_model_n_headPtr =
      _lookup<ffi.NativeFunction<ffi.Int32 Function(ffi.Pointer<llama_model>)>>(
        'llama_model_n_head',
      );
  late final _llama_model_n_head = _llama_model_n_headPtr
      .asFunction<int Function(ffi.Pointer<llama_model>)>();

  int llama_model_n_head_kv(ffi.Pointer<llama_model> model) {
    return _llama_model_n_head_kv(model);
  }

  late final _llama_model_n_head_kvPtr =
      _lookup<ffi.NativeFunction<ffi.Int32 Function(ffi.Pointer<llama_model>)>>(
        'llama_model_n_head_kv',
      );
  late final _llama_model_n_head_kv = _llama_model_n_head_kvPtr
      .asFunction<int Function(ffi.Pointer<llama_model>)>();

  int llama_model_n_swa(ffi.Pointer<llama_model> model) {
    return _llama_model_n_swa(model);
  }

  late final _llama_model_n_swaPtr =
      _lookup<ffi.NativeFunction<ffi.Int32 Function(ffi.Pointer<llama_model>)>>(
        'llama_model_n_swa',
      );
  late final _llama_model_n_swa = _llama_model_n_swaPtr
      .asFunction<int Function(ffi.Pointer<llama_model>)>();

  /// Get the model's RoPE frequency scaling factor
  double llama_model_rope_freq_scale_train(ffi.Pointer<llama_model> model) {
    return _llama_model_rope_freq_scale_train(model);
  }

  late final _llama_model_rope_freq_scale_trainPtr =
      _lookup<ffi.NativeFunction<ffi.Float Function(ffi.Pointer<llama_model>)>>(
        'llama_model_rope_freq_scale_train',
      );
  late final _llama_model_rope_freq_scale_train =
      _llama_model_rope_freq_scale_trainPtr
          .asFunction<double Function(ffi.Pointer<llama_model>)>();

  /// Returns the number of classifier outputs (only valid for classifier models)
  /// Undefined behavior for non-classifier models
  int llama_model_n_cls_out(ffi.Pointer<llama_model> model) {
    return _llama_model_n_cls_out(model);
  }

  late final _llama_model_n_cls_outPtr =
      _lookup<
        ffi.NativeFunction<ffi.Uint32 Function(ffi.Pointer<llama_model>)>
      >('llama_model_n_cls_out');
  late final _llama_model_n_cls_out = _llama_model_n_cls_outPtr
      .asFunction<int Function(ffi.Pointer<llama_model>)>();

  /// Returns label of classifier output by index (<n_cls_out). Returns nullptr if no label provided
  ffi.Pointer<ffi.Char> llama_model_cls_label(
    ffi.Pointer<llama_model> model,
    int i,
  ) {
    return _llama_model_cls_label(model, i);
  }

  late final _llama_model_cls_labelPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ffi.Char> Function(ffi.Pointer<llama_model>, ffi.Uint32)
        >
      >('llama_model_cls_label');
  late final _llama_model_cls_label = _llama_model_cls_labelPtr
      .asFunction<
        ffi.Pointer<ffi.Char> Function(ffi.Pointer<llama_model>, int)
      >();

  llama_vocab_type llama_vocab_type$1(ffi.Pointer<llama_vocab> vocab) {
    return llama_vocab_type.fromValue(_llama_vocab_type$1(vocab));
  }

  late final _llama_vocab_type$1Ptr =
      _lookup<
        ffi.NativeFunction<ffi.UnsignedInt Function(ffi.Pointer<llama_vocab>)>
      >('llama_vocab_type');
  late final _llama_vocab_type$1 = _llama_vocab_type$1Ptr
      .asFunction<int Function(ffi.Pointer<llama_vocab>)>();

  int llama_vocab_n_tokens(ffi.Pointer<llama_vocab> vocab) {
    return _llama_vocab_n_tokens(vocab);
  }

  late final _llama_vocab_n_tokensPtr =
      _lookup<ffi.NativeFunction<ffi.Int32 Function(ffi.Pointer<llama_vocab>)>>(
        'llama_vocab_n_tokens',
      );
  late final _llama_vocab_n_tokens = _llama_vocab_n_tokensPtr
      .asFunction<int Function(ffi.Pointer<llama_vocab>)>();

  /// Get metadata value as a string by key name
  int llama_model_meta_val_str(
    ffi.Pointer<llama_model> model,
    ffi.Pointer<ffi.Char> key,
    ffi.Pointer<ffi.Char> buf,
    int buf_size,
  ) {
    return _llama_model_meta_val_str(model, key, buf, buf_size);
  }

  late final _llama_model_meta_val_strPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Int32 Function(
            ffi.Pointer<llama_model>,
            ffi.Pointer<ffi.Char>,
            ffi.Pointer<ffi.Char>,
            ffi.Size,
          )
        >
      >('llama_model_meta_val_str');
  late final _llama_model_meta_val_str = _llama_model_meta_val_strPtr
      .asFunction<
        int Function(
          ffi.Pointer<llama_model>,
          ffi.Pointer<ffi.Char>,
          ffi.Pointer<ffi.Char>,
          int,
        )
      >();

  /// Get the number of metadata key/value pairs
  int llama_model_meta_count(ffi.Pointer<llama_model> model) {
    return _llama_model_meta_count(model);
  }

  late final _llama_model_meta_countPtr =
      _lookup<ffi.NativeFunction<ffi.Int32 Function(ffi.Pointer<llama_model>)>>(
        'llama_model_meta_count',
      );
  late final _llama_model_meta_count = _llama_model_meta_countPtr
      .asFunction<int Function(ffi.Pointer<llama_model>)>();

  /// Get sampling metadata key name. Returns nullptr if the key is invalid
  ffi.Pointer<ffi.Char> llama_model_meta_key_str(llama_model_meta_key key) {
    return _llama_model_meta_key_str(key.value);
  }

  late final _llama_model_meta_key_strPtr =
      _lookup<
        ffi.NativeFunction<ffi.Pointer<ffi.Char> Function(ffi.UnsignedInt)>
      >('llama_model_meta_key_str');
  late final _llama_model_meta_key_str = _llama_model_meta_key_strPtr
      .asFunction<ffi.Pointer<ffi.Char> Function(int)>();

  /// Get metadata key name by index
  int llama_model_meta_key_by_index(
    ffi.Pointer<llama_model> model,
    int i,
    ffi.Pointer<ffi.Char> buf,
    int buf_size,
  ) {
    return _llama_model_meta_key_by_index(model, i, buf, buf_size);
  }

  late final _llama_model_meta_key_by_indexPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Int32 Function(
            ffi.Pointer<llama_model>,
            ffi.Int32,
            ffi.Pointer<ffi.Char>,
            ffi.Size,
          )
        >
      >('llama_model_meta_key_by_index');
  late final _llama_model_meta_key_by_index = _llama_model_meta_key_by_indexPtr
      .asFunction<
        int Function(ffi.Pointer<llama_model>, int, ffi.Pointer<ffi.Char>, int)
      >();

  /// Get metadata value as a string by index
  int llama_model_meta_val_str_by_index(
    ffi.Pointer<llama_model> model,
    int i,
    ffi.Pointer<ffi.Char> buf,
    int buf_size,
  ) {
    return _llama_model_meta_val_str_by_index(model, i, buf, buf_size);
  }

  late final _llama_model_meta_val_str_by_indexPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Int32 Function(
            ffi.Pointer<llama_model>,
            ffi.Int32,
            ffi.Pointer<ffi.Char>,
            ffi.Size,
          )
        >
      >('llama_model_meta_val_str_by_index');
  late final _llama_model_meta_val_str_by_index =
      _llama_model_meta_val_str_by_indexPtr
          .asFunction<
            int Function(
              ffi.Pointer<llama_model>,
              int,
              ffi.Pointer<ffi.Char>,
              int,
            )
          >();

  /// Get a string describing the model type
  int llama_model_desc(
    ffi.Pointer<llama_model> model,
    ffi.Pointer<ffi.Char> buf,
    int buf_size,
  ) {
    return _llama_model_desc(model, buf, buf_size);
  }

  late final _llama_model_descPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Int32 Function(
            ffi.Pointer<llama_model>,
            ffi.Pointer<ffi.Char>,
            ffi.Size,
          )
        >
      >('llama_model_desc');
  late final _llama_model_desc = _llama_model_descPtr
      .asFunction<
        int Function(ffi.Pointer<llama_model>, ffi.Pointer<ffi.Char>, int)
      >();

  /// Returns the total size of all the tensors in the model in bytes
  int llama_model_size(ffi.Pointer<llama_model> model) {
    return _llama_model_size(model);
  }

  late final _llama_model_sizePtr =
      _lookup<
        ffi.NativeFunction<ffi.Uint64 Function(ffi.Pointer<llama_model>)>
      >('llama_model_size');
  late final _llama_model_size = _llama_model_sizePtr
      .asFunction<int Function(ffi.Pointer<llama_model>)>();

  /// Get the default chat template. Returns nullptr if not available
  /// If name is NULL, returns the default chat template
  ffi.Pointer<ffi.Char> llama_model_chat_template(
    ffi.Pointer<llama_model> model,
    ffi.Pointer<ffi.Char> name,
  ) {
    return _llama_model_chat_template(model, name);
  }

  late final _llama_model_chat_templatePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ffi.Char> Function(
            ffi.Pointer<llama_model>,
            ffi.Pointer<ffi.Char>,
          )
        >
      >('llama_model_chat_template');
  late final _llama_model_chat_template = _llama_model_chat_templatePtr
      .asFunction<
        ffi.Pointer<ffi.Char> Function(
          ffi.Pointer<llama_model>,
          ffi.Pointer<ffi.Char>,
        )
      >();

  /// Returns the total number of parameters in the model
  int llama_model_n_params(ffi.Pointer<llama_model> model) {
    return _llama_model_n_params(model);
  }

  late final _llama_model_n_paramsPtr =
      _lookup<
        ffi.NativeFunction<ffi.Uint64 Function(ffi.Pointer<llama_model>)>
      >('llama_model_n_params');
  late final _llama_model_n_params = _llama_model_n_paramsPtr
      .asFunction<int Function(ffi.Pointer<llama_model>)>();

  /// Returns true if the model contains an encoder that requires llama_encode() call
  bool llama_model_has_encoder(ffi.Pointer<llama_model> model) {
    return _llama_model_has_encoder(model);
  }

  late final _llama_model_has_encoderPtr =
      _lookup<ffi.NativeFunction<ffi.Bool Function(ffi.Pointer<llama_model>)>>(
        'llama_model_has_encoder',
      );
  late final _llama_model_has_encoder = _llama_model_has_encoderPtr
      .asFunction<bool Function(ffi.Pointer<llama_model>)>();

  /// Returns true if the model contains a decoder that requires llama_decode() call
  bool llama_model_has_decoder(ffi.Pointer<llama_model> model) {
    return _llama_model_has_decoder(model);
  }

  late final _llama_model_has_decoderPtr =
      _lookup<ffi.NativeFunction<ffi.Bool Function(ffi.Pointer<llama_model>)>>(
        'llama_model_has_decoder',
      );
  late final _llama_model_has_decoder = _llama_model_has_decoderPtr
      .asFunction<bool Function(ffi.Pointer<llama_model>)>();

  /// For encoder-decoder models, this function returns id of the token that must be provided
  /// to the decoder to start generating output sequence. For other models, it returns -1.
  int llama_model_decoder_start_token(ffi.Pointer<llama_model> model) {
    return _llama_model_decoder_start_token(model);
  }

  late final _llama_model_decoder_start_tokenPtr =
      _lookup<
        ffi.NativeFunction<llama_token Function(ffi.Pointer<llama_model>)>
      >('llama_model_decoder_start_token');
  late final _llama_model_decoder_start_token =
      _llama_model_decoder_start_tokenPtr
          .asFunction<int Function(ffi.Pointer<llama_model>)>();

  /// Returns true if the model is recurrent (like Mamba, RWKV, etc.)
  bool llama_model_is_recurrent(ffi.Pointer<llama_model> model) {
    return _llama_model_is_recurrent(model);
  }

  late final _llama_model_is_recurrentPtr =
      _lookup<ffi.NativeFunction<ffi.Bool Function(ffi.Pointer<llama_model>)>>(
        'llama_model_is_recurrent',
      );
  late final _llama_model_is_recurrent = _llama_model_is_recurrentPtr
      .asFunction<bool Function(ffi.Pointer<llama_model>)>();

  /// Returns true if the model is hybrid (like Jamba, Granite, etc.)
  bool llama_model_is_hybrid(ffi.Pointer<llama_model> model) {
    return _llama_model_is_hybrid(model);
  }

  late final _llama_model_is_hybridPtr =
      _lookup<ffi.NativeFunction<ffi.Bool Function(ffi.Pointer<llama_model>)>>(
        'llama_model_is_hybrid',
      );
  late final _llama_model_is_hybrid = _llama_model_is_hybridPtr
      .asFunction<bool Function(ffi.Pointer<llama_model>)>();

  /// Returns true if the model is diffusion-based (like LLaDA, Dream, etc.)
  bool llama_model_is_diffusion(ffi.Pointer<llama_model> model) {
    return _llama_model_is_diffusion(model);
  }

  late final _llama_model_is_diffusionPtr =
      _lookup<ffi.NativeFunction<ffi.Bool Function(ffi.Pointer<llama_model>)>>(
        'llama_model_is_diffusion',
      );
  late final _llama_model_is_diffusion = _llama_model_is_diffusionPtr
      .asFunction<bool Function(ffi.Pointer<llama_model>)>();

  /// Returns 0 on success
  int llama_model_quantize(
    ffi.Pointer<ffi.Char> fname_inp,
    ffi.Pointer<ffi.Char> fname_out,
    ffi.Pointer<llama_model_quantize_params> params,
  ) {
    return _llama_model_quantize(fname_inp, fname_out, params);
  }

  late final _llama_model_quantizePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Uint32 Function(
            ffi.Pointer<ffi.Char>,
            ffi.Pointer<ffi.Char>,
            ffi.Pointer<llama_model_quantize_params>,
          )
        >
      >('llama_model_quantize');
  late final _llama_model_quantize = _llama_model_quantizePtr
      .asFunction<
        int Function(
          ffi.Pointer<ffi.Char>,
          ffi.Pointer<ffi.Char>,
          ffi.Pointer<llama_model_quantize_params>,
        )
      >();

  /// Load a LoRA adapter from file
  ffi.Pointer<llama_adapter_lora> llama_adapter_lora_init(
    ffi.Pointer<llama_model> model,
    ffi.Pointer<ffi.Char> path_lora,
  ) {
    return _llama_adapter_lora_init(model, path_lora);
  }

  late final _llama_adapter_lora_initPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<llama_adapter_lora> Function(
            ffi.Pointer<llama_model>,
            ffi.Pointer<ffi.Char>,
          )
        >
      >('llama_adapter_lora_init');
  late final _llama_adapter_lora_init = _llama_adapter_lora_initPtr
      .asFunction<
        ffi.Pointer<llama_adapter_lora> Function(
          ffi.Pointer<llama_model>,
          ffi.Pointer<ffi.Char>,
        )
      >();

  /// Get metadata value as a string by key name
  int llama_adapter_meta_val_str(
    ffi.Pointer<llama_adapter_lora> adapter,
    ffi.Pointer<ffi.Char> key,
    ffi.Pointer<ffi.Char> buf,
    int buf_size,
  ) {
    return _llama_adapter_meta_val_str(adapter, key, buf, buf_size);
  }

  late final _llama_adapter_meta_val_strPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Int32 Function(
            ffi.Pointer<llama_adapter_lora>,
            ffi.Pointer<ffi.Char>,
            ffi.Pointer<ffi.Char>,
            ffi.Size,
          )
        >
      >('llama_adapter_meta_val_str');
  late final _llama_adapter_meta_val_str = _llama_adapter_meta_val_strPtr
      .asFunction<
        int Function(
          ffi.Pointer<llama_adapter_lora>,
          ffi.Pointer<ffi.Char>,
          ffi.Pointer<ffi.Char>,
          int,
        )
      >();

  /// Get the number of metadata key/value pairs
  int llama_adapter_meta_count(ffi.Pointer<llama_adapter_lora> adapter) {
    return _llama_adapter_meta_count(adapter);
  }

  late final _llama_adapter_meta_countPtr =
      _lookup<
        ffi.NativeFunction<ffi.Int32 Function(ffi.Pointer<llama_adapter_lora>)>
      >('llama_adapter_meta_count');
  late final _llama_adapter_meta_count = _llama_adapter_meta_countPtr
      .asFunction<int Function(ffi.Pointer<llama_adapter_lora>)>();

  /// Get metadata key name by index
  int llama_adapter_meta_key_by_index(
    ffi.Pointer<llama_adapter_lora> adapter,
    int i,
    ffi.Pointer<ffi.Char> buf,
    int buf_size,
  ) {
    return _llama_adapter_meta_key_by_index(adapter, i, buf, buf_size);
  }

  late final _llama_adapter_meta_key_by_indexPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Int32 Function(
            ffi.Pointer<llama_adapter_lora>,
            ffi.Int32,
            ffi.Pointer<ffi.Char>,
            ffi.Size,
          )
        >
      >('llama_adapter_meta_key_by_index');
  late final _llama_adapter_meta_key_by_index =
      _llama_adapter_meta_key_by_indexPtr
          .asFunction<
            int Function(
              ffi.Pointer<llama_adapter_lora>,
              int,
              ffi.Pointer<ffi.Char>,
              int,
            )
          >();

  /// Get metadata value as a string by index
  int llama_adapter_meta_val_str_by_index(
    ffi.Pointer<llama_adapter_lora> adapter,
    int i,
    ffi.Pointer<ffi.Char> buf,
    int buf_size,
  ) {
    return _llama_adapter_meta_val_str_by_index(adapter, i, buf, buf_size);
  }

  late final _llama_adapter_meta_val_str_by_indexPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Int32 Function(
            ffi.Pointer<llama_adapter_lora>,
            ffi.Int32,
            ffi.Pointer<ffi.Char>,
            ffi.Size,
          )
        >
      >('llama_adapter_meta_val_str_by_index');
  late final _llama_adapter_meta_val_str_by_index =
      _llama_adapter_meta_val_str_by_indexPtr
          .asFunction<
            int Function(
              ffi.Pointer<llama_adapter_lora>,
              int,
              ffi.Pointer<ffi.Char>,
              int,
            )
          >();

  /// Manually free a LoRA adapter
  /// NOTE: loaded adapters will be free when the associated model is deleted
  void llama_adapter_lora_free(ffi.Pointer<llama_adapter_lora> adapter) {
    return _llama_adapter_lora_free(adapter);
  }

  late final _llama_adapter_lora_freePtr =
      _lookup<
        ffi.NativeFunction<ffi.Void Function(ffi.Pointer<llama_adapter_lora>)>
      >('llama_adapter_lora_free');
  late final _llama_adapter_lora_free = _llama_adapter_lora_freePtr
      .asFunction<void Function(ffi.Pointer<llama_adapter_lora>)>();

  /// Get the invocation tokens if the current lora is an alora
  int llama_adapter_get_alora_n_invocation_tokens(
    ffi.Pointer<llama_adapter_lora> adapter,
  ) {
    return _llama_adapter_get_alora_n_invocation_tokens(adapter);
  }

  late final _llama_adapter_get_alora_n_invocation_tokensPtr =
      _lookup<
        ffi.NativeFunction<ffi.Uint64 Function(ffi.Pointer<llama_adapter_lora>)>
      >('llama_adapter_get_alora_n_invocation_tokens');
  late final _llama_adapter_get_alora_n_invocation_tokens =
      _llama_adapter_get_alora_n_invocation_tokensPtr
          .asFunction<int Function(ffi.Pointer<llama_adapter_lora>)>();

  ffi.Pointer<llama_token> llama_adapter_get_alora_invocation_tokens(
    ffi.Pointer<llama_adapter_lora> adapter,
  ) {
    return _llama_adapter_get_alora_invocation_tokens(adapter);
  }

  late final _llama_adapter_get_alora_invocation_tokensPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<llama_token> Function(ffi.Pointer<llama_adapter_lora>)
        >
      >('llama_adapter_get_alora_invocation_tokens');
  late final _llama_adapter_get_alora_invocation_tokens =
      _llama_adapter_get_alora_invocation_tokensPtr
          .asFunction<
            ffi.Pointer<llama_token> Function(ffi.Pointer<llama_adapter_lora>)
          >();

  /// Add a loaded LoRA adapter to given context
  /// This will not modify model's weight
  int llama_set_adapter_lora(
    ffi.Pointer<llama_context> ctx,
    ffi.Pointer<llama_adapter_lora> adapter,
    double scale,
  ) {
    return _llama_set_adapter_lora(ctx, adapter, scale);
  }

  late final _llama_set_adapter_loraPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Int32 Function(
            ffi.Pointer<llama_context>,
            ffi.Pointer<llama_adapter_lora>,
            ffi.Float,
          )
        >
      >('llama_set_adapter_lora');
  late final _llama_set_adapter_lora = _llama_set_adapter_loraPtr
      .asFunction<
        int Function(
          ffi.Pointer<llama_context>,
          ffi.Pointer<llama_adapter_lora>,
          double,
        )
      >();

  /// Remove a specific LoRA adapter from given context
  /// Return -1 if the adapter is not present in the context
  int llama_rm_adapter_lora(
    ffi.Pointer<llama_context> ctx,
    ffi.Pointer<llama_adapter_lora> adapter,
  ) {
    return _llama_rm_adapter_lora(ctx, adapter);
  }

  late final _llama_rm_adapter_loraPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Int32 Function(
            ffi.Pointer<llama_context>,
            ffi.Pointer<llama_adapter_lora>,
          )
        >
      >('llama_rm_adapter_lora');
  late final _llama_rm_adapter_lora = _llama_rm_adapter_loraPtr
      .asFunction<
        int Function(
          ffi.Pointer<llama_context>,
          ffi.Pointer<llama_adapter_lora>,
        )
      >();

  /// Remove all LoRA adapters from given context
  void llama_clear_adapter_lora(ffi.Pointer<llama_context> ctx) {
    return _llama_clear_adapter_lora(ctx);
  }

  late final _llama_clear_adapter_loraPtr =
      _lookup<
        ffi.NativeFunction<ffi.Void Function(ffi.Pointer<llama_context>)>
      >('llama_clear_adapter_lora');
  late final _llama_clear_adapter_lora = _llama_clear_adapter_loraPtr
      .asFunction<void Function(ffi.Pointer<llama_context>)>();

  /// Apply a loaded control vector to a llama_context, or if data is NULL, clear
  /// the currently loaded vector.
  /// n_embd should be the size of a single layer's control, and data should point
  /// to an n_embd x n_layers buffer starting from layer 1.
  /// il_start and il_end are the layer range the vector should apply to (both inclusive)
  /// See llama_control_vector_load in common to load a control vector.
  int llama_apply_adapter_cvec(
    ffi.Pointer<llama_context> ctx,
    ffi.Pointer<ffi.Float> data,
    int len,
    int n_embd,
    int il_start,
    int il_end,
  ) {
    return _llama_apply_adapter_cvec(ctx, data, len, n_embd, il_start, il_end);
  }

  late final _llama_apply_adapter_cvecPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Int32 Function(
            ffi.Pointer<llama_context>,
            ffi.Pointer<ffi.Float>,
            ffi.Size,
            ffi.Int32,
            ffi.Int32,
            ffi.Int32,
          )
        >
      >('llama_apply_adapter_cvec');
  late final _llama_apply_adapter_cvec = _llama_apply_adapter_cvecPtr
      .asFunction<
        int Function(
          ffi.Pointer<llama_context>,
          ffi.Pointer<ffi.Float>,
          int,
          int,
          int,
          int,
        )
      >();

  /// Clear the memory contents
  /// If data == true, the data buffers will also be cleared together with the metadata
  void llama_memory_clear(llama_memory_t mem, bool data) {
    return _llama_memory_clear(mem, data);
  }

  late final _llama_memory_clearPtr =
      _lookup<ffi.NativeFunction<ffi.Void Function(llama_memory_t, ffi.Bool)>>(
        'llama_memory_clear',
      );
  late final _llama_memory_clear = _llama_memory_clearPtr
      .asFunction<void Function(llama_memory_t, bool)>();

  /// Removes all tokens that belong to the specified sequence and have positions in [p0, p1)
  /// Returns false if a partial sequence cannot be removed. Removing a whole sequence never fails
  /// seq_id < 0 : match any sequence
  /// p0 < 0     : [0,  p1]
  /// p1 < 0     : [p0, inf)
  bool llama_memory_seq_rm(llama_memory_t mem, int seq_id, int p0, int p1) {
    return _llama_memory_seq_rm(mem, seq_id, p0, p1);
  }

  late final _llama_memory_seq_rmPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Bool Function(llama_memory_t, llama_seq_id, llama_pos, llama_pos)
        >
      >('llama_memory_seq_rm');
  late final _llama_memory_seq_rm = _llama_memory_seq_rmPtr
      .asFunction<bool Function(llama_memory_t, int, int, int)>();

  /// Copy all tokens that belong to the specified sequence to another sequence
  /// p0 < 0 : [0,  p1]
  /// p1 < 0 : [p0, inf)
  void llama_memory_seq_cp(
    llama_memory_t mem,
    int seq_id_src,
    int seq_id_dst,
    int p0,
    int p1,
  ) {
    return _llama_memory_seq_cp(mem, seq_id_src, seq_id_dst, p0, p1);
  }

  late final _llama_memory_seq_cpPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Void Function(
            llama_memory_t,
            llama_seq_id,
            llama_seq_id,
            llama_pos,
            llama_pos,
          )
        >
      >('llama_memory_seq_cp');
  late final _llama_memory_seq_cp = _llama_memory_seq_cpPtr
      .asFunction<void Function(llama_memory_t, int, int, int, int)>();

  /// Removes all tokens that do not belong to the specified sequence
  void llama_memory_seq_keep(llama_memory_t mem, int seq_id) {
    return _llama_memory_seq_keep(mem, seq_id);
  }

  late final _llama_memory_seq_keepPtr =
      _lookup<
        ffi.NativeFunction<ffi.Void Function(llama_memory_t, llama_seq_id)>
      >('llama_memory_seq_keep');
  late final _llama_memory_seq_keep = _llama_memory_seq_keepPtr
      .asFunction<void Function(llama_memory_t, int)>();

  /// Adds relative position "delta" to all tokens that belong to the specified sequence and have positions in [p0, p1)
  /// p0 < 0 : [0,  p1]
  /// p1 < 0 : [p0, inf)
  void llama_memory_seq_add(
    llama_memory_t mem,
    int seq_id,
    int p0,
    int p1,
    int delta,
  ) {
    return _llama_memory_seq_add(mem, seq_id, p0, p1, delta);
  }

  late final _llama_memory_seq_addPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Void Function(
            llama_memory_t,
            llama_seq_id,
            llama_pos,
            llama_pos,
            llama_pos,
          )
        >
      >('llama_memory_seq_add');
  late final _llama_memory_seq_add = _llama_memory_seq_addPtr
      .asFunction<void Function(llama_memory_t, int, int, int, int)>();

  /// Integer division of the positions by factor of `d > 1`
  /// p0 < 0 : [0,  p1]
  /// p1 < 0 : [p0, inf)
  void llama_memory_seq_div(
    llama_memory_t mem,
    int seq_id,
    int p0,
    int p1,
    int d,
  ) {
    return _llama_memory_seq_div(mem, seq_id, p0, p1, d);
  }

  late final _llama_memory_seq_divPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Void Function(
            llama_memory_t,
            llama_seq_id,
            llama_pos,
            llama_pos,
            ffi.Int,
          )
        >
      >('llama_memory_seq_div');
  late final _llama_memory_seq_div = _llama_memory_seq_divPtr
      .asFunction<void Function(llama_memory_t, int, int, int, int)>();

  /// Returns the smallest position present in the memory for the specified sequence
  /// This is typically non-zero only for SWA caches
  /// Note that all positions in the range [pos_min, pos_max] are guaranteed to be present in the memory
  /// Return -1 if the sequence is empty
  int llama_memory_seq_pos_min(llama_memory_t mem, int seq_id) {
    return _llama_memory_seq_pos_min(mem, seq_id);
  }

  late final _llama_memory_seq_pos_minPtr =
      _lookup<
        ffi.NativeFunction<llama_pos Function(llama_memory_t, llama_seq_id)>
      >('llama_memory_seq_pos_min');
  late final _llama_memory_seq_pos_min = _llama_memory_seq_pos_minPtr
      .asFunction<int Function(llama_memory_t, int)>();

  /// Returns the largest position present in the memory for the specified sequence
  /// Note that all positions in the range [pos_min, pos_max] are guaranteed to be present in the memory
  /// Return -1 if the sequence is empty
  int llama_memory_seq_pos_max(llama_memory_t mem, int seq_id) {
    return _llama_memory_seq_pos_max(mem, seq_id);
  }

  late final _llama_memory_seq_pos_maxPtr =
      _lookup<
        ffi.NativeFunction<llama_pos Function(llama_memory_t, llama_seq_id)>
      >('llama_memory_seq_pos_max');
  late final _llama_memory_seq_pos_max = _llama_memory_seq_pos_maxPtr
      .asFunction<int Function(llama_memory_t, int)>();

  /// Check if the memory supports shifting
  bool llama_memory_can_shift(llama_memory_t mem) {
    return _llama_memory_can_shift(mem);
  }

  late final _llama_memory_can_shiftPtr =
      _lookup<ffi.NativeFunction<ffi.Bool Function(llama_memory_t)>>(
        'llama_memory_can_shift',
      );
  late final _llama_memory_can_shift = _llama_memory_can_shiftPtr
      .asFunction<bool Function(llama_memory_t)>();

  /// Returns the *actual* size in bytes of the state
  /// (logits, embedding and memory)
  /// Only use when saving the state, not when restoring it, otherwise the size may be too small.
  int llama_state_get_size(ffi.Pointer<llama_context> ctx) {
    return _llama_state_get_size(ctx);
  }

  late final _llama_state_get_sizePtr =
      _lookup<
        ffi.NativeFunction<ffi.Size Function(ffi.Pointer<llama_context>)>
      >('llama_state_get_size');
  late final _llama_state_get_size = _llama_state_get_sizePtr
      .asFunction<int Function(ffi.Pointer<llama_context>)>();

  int llama_get_state_size(ffi.Pointer<llama_context> ctx) {
    return _llama_get_state_size(ctx);
  }

  late final _llama_get_state_sizePtr =
      _lookup<
        ffi.NativeFunction<ffi.Size Function(ffi.Pointer<llama_context>)>
      >('llama_get_state_size');
  late final _llama_get_state_size = _llama_get_state_sizePtr
      .asFunction<int Function(ffi.Pointer<llama_context>)>();

  /// Copies the state to the specified destination address.
  /// Destination needs to have allocated enough memory.
  /// Returns the number of bytes copied
  int llama_state_get_data(
    ffi.Pointer<llama_context> ctx,
    ffi.Pointer<ffi.Uint8> dst,
    int size,
  ) {
    return _llama_state_get_data(ctx, dst, size);
  }

  late final _llama_state_get_dataPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Size Function(
            ffi.Pointer<llama_context>,
            ffi.Pointer<ffi.Uint8>,
            ffi.Size,
          )
        >
      >('llama_state_get_data');
  late final _llama_state_get_data = _llama_state_get_dataPtr
      .asFunction<
        int Function(ffi.Pointer<llama_context>, ffi.Pointer<ffi.Uint8>, int)
      >();

  int llama_copy_state_data(
    ffi.Pointer<llama_context> ctx,
    ffi.Pointer<ffi.Uint8> dst,
  ) {
    return _llama_copy_state_data(ctx, dst);
  }

  late final _llama_copy_state_dataPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Size Function(ffi.Pointer<llama_context>, ffi.Pointer<ffi.Uint8>)
        >
      >('llama_copy_state_data');
  late final _llama_copy_state_data = _llama_copy_state_dataPtr
      .asFunction<
        int Function(ffi.Pointer<llama_context>, ffi.Pointer<ffi.Uint8>)
      >();

  /// Set the state reading from the specified address
  /// Returns the number of bytes read
  int llama_state_set_data(
    ffi.Pointer<llama_context> ctx,
    ffi.Pointer<ffi.Uint8> src,
    int size,
  ) {
    return _llama_state_set_data(ctx, src, size);
  }

  late final _llama_state_set_dataPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Size Function(
            ffi.Pointer<llama_context>,
            ffi.Pointer<ffi.Uint8>,
            ffi.Size,
          )
        >
      >('llama_state_set_data');
  late final _llama_state_set_data = _llama_state_set_dataPtr
      .asFunction<
        int Function(ffi.Pointer<llama_context>, ffi.Pointer<ffi.Uint8>, int)
      >();

  int llama_set_state_data(
    ffi.Pointer<llama_context> ctx,
    ffi.Pointer<ffi.Uint8> src,
  ) {
    return _llama_set_state_data(ctx, src);
  }

  late final _llama_set_state_dataPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Size Function(ffi.Pointer<llama_context>, ffi.Pointer<ffi.Uint8>)
        >
      >('llama_set_state_data');
  late final _llama_set_state_data = _llama_set_state_dataPtr
      .asFunction<
        int Function(ffi.Pointer<llama_context>, ffi.Pointer<ffi.Uint8>)
      >();

  /// Save/load session file
  bool llama_state_load_file(
    ffi.Pointer<llama_context> ctx,
    ffi.Pointer<ffi.Char> path_session,
    ffi.Pointer<llama_token> tokens_out,
    int n_token_capacity,
    ffi.Pointer<ffi.Size> n_token_count_out,
  ) {
    return _llama_state_load_file(
      ctx,
      path_session,
      tokens_out,
      n_token_capacity,
      n_token_count_out,
    );
  }

  late final _llama_state_load_filePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Bool Function(
            ffi.Pointer<llama_context>,
            ffi.Pointer<ffi.Char>,
            ffi.Pointer<llama_token>,
            ffi.Size,
            ffi.Pointer<ffi.Size>,
          )
        >
      >('llama_state_load_file');
  late final _llama_state_load_file = _llama_state_load_filePtr
      .asFunction<
        bool Function(
          ffi.Pointer<llama_context>,
          ffi.Pointer<ffi.Char>,
          ffi.Pointer<llama_token>,
          int,
          ffi.Pointer<ffi.Size>,
        )
      >();

  bool llama_load_session_file(
    ffi.Pointer<llama_context> ctx,
    ffi.Pointer<ffi.Char> path_session,
    ffi.Pointer<llama_token> tokens_out,
    int n_token_capacity,
    ffi.Pointer<ffi.Size> n_token_count_out,
  ) {
    return _llama_load_session_file(
      ctx,
      path_session,
      tokens_out,
      n_token_capacity,
      n_token_count_out,
    );
  }

  late final _llama_load_session_filePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Bool Function(
            ffi.Pointer<llama_context>,
            ffi.Pointer<ffi.Char>,
            ffi.Pointer<llama_token>,
            ffi.Size,
            ffi.Pointer<ffi.Size>,
          )
        >
      >('llama_load_session_file');
  late final _llama_load_session_file = _llama_load_session_filePtr
      .asFunction<
        bool Function(
          ffi.Pointer<llama_context>,
          ffi.Pointer<ffi.Char>,
          ffi.Pointer<llama_token>,
          int,
          ffi.Pointer<ffi.Size>,
        )
      >();

  bool llama_state_save_file(
    ffi.Pointer<llama_context> ctx,
    ffi.Pointer<ffi.Char> path_session,
    ffi.Pointer<llama_token> tokens,
    int n_token_count,
  ) {
    return _llama_state_save_file(ctx, path_session, tokens, n_token_count);
  }

  late final _llama_state_save_filePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Bool Function(
            ffi.Pointer<llama_context>,
            ffi.Pointer<ffi.Char>,
            ffi.Pointer<llama_token>,
            ffi.Size,
          )
        >
      >('llama_state_save_file');
  late final _llama_state_save_file = _llama_state_save_filePtr
      .asFunction<
        bool Function(
          ffi.Pointer<llama_context>,
          ffi.Pointer<ffi.Char>,
          ffi.Pointer<llama_token>,
          int,
        )
      >();

  bool llama_save_session_file(
    ffi.Pointer<llama_context> ctx,
    ffi.Pointer<ffi.Char> path_session,
    ffi.Pointer<llama_token> tokens,
    int n_token_count,
  ) {
    return _llama_save_session_file(ctx, path_session, tokens, n_token_count);
  }

  late final _llama_save_session_filePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Bool Function(
            ffi.Pointer<llama_context>,
            ffi.Pointer<ffi.Char>,
            ffi.Pointer<llama_token>,
            ffi.Size,
          )
        >
      >('llama_save_session_file');
  late final _llama_save_session_file = _llama_save_session_filePtr
      .asFunction<
        bool Function(
          ffi.Pointer<llama_context>,
          ffi.Pointer<ffi.Char>,
          ffi.Pointer<llama_token>,
          int,
        )
      >();

  /// Get the exact size needed to copy the state of a single sequence
  int llama_state_seq_get_size(ffi.Pointer<llama_context> ctx, int seq_id) {
    return _llama_state_seq_get_size(ctx, seq_id);
  }

  late final _llama_state_seq_get_sizePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Size Function(ffi.Pointer<llama_context>, llama_seq_id)
        >
      >('llama_state_seq_get_size');
  late final _llama_state_seq_get_size = _llama_state_seq_get_sizePtr
      .asFunction<int Function(ffi.Pointer<llama_context>, int)>();

  /// Copy the state of a single sequence into the specified buffer
  int llama_state_seq_get_data(
    ffi.Pointer<llama_context> ctx,
    ffi.Pointer<ffi.Uint8> dst,
    int size,
    int seq_id,
  ) {
    return _llama_state_seq_get_data(ctx, dst, size, seq_id);
  }

  late final _llama_state_seq_get_dataPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Size Function(
            ffi.Pointer<llama_context>,
            ffi.Pointer<ffi.Uint8>,
            ffi.Size,
            llama_seq_id,
          )
        >
      >('llama_state_seq_get_data');
  late final _llama_state_seq_get_data = _llama_state_seq_get_dataPtr
      .asFunction<
        int Function(
          ffi.Pointer<llama_context>,
          ffi.Pointer<ffi.Uint8>,
          int,
          int,
        )
      >();

  /// Copy the sequence data (originally copied with `llama_state_seq_get_data`) into the specified sequence
  /// Returns:
  /// - Positive: Ok
  /// - Zero: Failed to load
  int llama_state_seq_set_data(
    ffi.Pointer<llama_context> ctx,
    ffi.Pointer<ffi.Uint8> src,
    int size,
    int dest_seq_id,
  ) {
    return _llama_state_seq_set_data(ctx, src, size, dest_seq_id);
  }

  late final _llama_state_seq_set_dataPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Size Function(
            ffi.Pointer<llama_context>,
            ffi.Pointer<ffi.Uint8>,
            ffi.Size,
            llama_seq_id,
          )
        >
      >('llama_state_seq_set_data');
  late final _llama_state_seq_set_data = _llama_state_seq_set_dataPtr
      .asFunction<
        int Function(
          ffi.Pointer<llama_context>,
          ffi.Pointer<ffi.Uint8>,
          int,
          int,
        )
      >();

  int llama_state_seq_save_file(
    ffi.Pointer<llama_context> ctx,
    ffi.Pointer<ffi.Char> filepath,
    int seq_id,
    ffi.Pointer<llama_token> tokens,
    int n_token_count,
  ) {
    return _llama_state_seq_save_file(
      ctx,
      filepath,
      seq_id,
      tokens,
      n_token_count,
    );
  }

  late final _llama_state_seq_save_filePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Size Function(
            ffi.Pointer<llama_context>,
            ffi.Pointer<ffi.Char>,
            llama_seq_id,
            ffi.Pointer<llama_token>,
            ffi.Size,
          )
        >
      >('llama_state_seq_save_file');
  late final _llama_state_seq_save_file = _llama_state_seq_save_filePtr
      .asFunction<
        int Function(
          ffi.Pointer<llama_context>,
          ffi.Pointer<ffi.Char>,
          int,
          ffi.Pointer<llama_token>,
          int,
        )
      >();

  int llama_state_seq_load_file(
    ffi.Pointer<llama_context> ctx,
    ffi.Pointer<ffi.Char> filepath,
    int dest_seq_id,
    ffi.Pointer<llama_token> tokens_out,
    int n_token_capacity,
    ffi.Pointer<ffi.Size> n_token_count_out,
  ) {
    return _llama_state_seq_load_file(
      ctx,
      filepath,
      dest_seq_id,
      tokens_out,
      n_token_capacity,
      n_token_count_out,
    );
  }

  late final _llama_state_seq_load_filePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Size Function(
            ffi.Pointer<llama_context>,
            ffi.Pointer<ffi.Char>,
            llama_seq_id,
            ffi.Pointer<llama_token>,
            ffi.Size,
            ffi.Pointer<ffi.Size>,
          )
        >
      >('llama_state_seq_load_file');
  late final _llama_state_seq_load_file = _llama_state_seq_load_filePtr
      .asFunction<
        int Function(
          ffi.Pointer<llama_context>,
          ffi.Pointer<ffi.Char>,
          int,
          ffi.Pointer<llama_token>,
          int,
          ffi.Pointer<ffi.Size>,
        )
      >();

  int llama_state_seq_get_size_ext(
    ffi.Pointer<llama_context> ctx,
    int seq_id,
    int flags,
  ) {
    return _llama_state_seq_get_size_ext(ctx, seq_id, flags);
  }

  late final _llama_state_seq_get_size_extPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Size Function(
            ffi.Pointer<llama_context>,
            llama_seq_id,
            llama_state_seq_flags,
          )
        >
      >('llama_state_seq_get_size_ext');
  late final _llama_state_seq_get_size_ext = _llama_state_seq_get_size_extPtr
      .asFunction<int Function(ffi.Pointer<llama_context>, int, int)>();

  int llama_state_seq_get_data_ext(
    ffi.Pointer<llama_context> ctx,
    ffi.Pointer<ffi.Uint8> dst,
    int size,
    int seq_id,
    int flags,
  ) {
    return _llama_state_seq_get_data_ext(ctx, dst, size, seq_id, flags);
  }

  late final _llama_state_seq_get_data_extPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Size Function(
            ffi.Pointer<llama_context>,
            ffi.Pointer<ffi.Uint8>,
            ffi.Size,
            llama_seq_id,
            llama_state_seq_flags,
          )
        >
      >('llama_state_seq_get_data_ext');
  late final _llama_state_seq_get_data_ext = _llama_state_seq_get_data_extPtr
      .asFunction<
        int Function(
          ffi.Pointer<llama_context>,
          ffi.Pointer<ffi.Uint8>,
          int,
          int,
          int,
        )
      >();

  int llama_state_seq_set_data_ext(
    ffi.Pointer<llama_context> ctx,
    ffi.Pointer<ffi.Uint8> src,
    int size,
    int dest_seq_id,
    int flags,
  ) {
    return _llama_state_seq_set_data_ext(ctx, src, size, dest_seq_id, flags);
  }

  late final _llama_state_seq_set_data_extPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Size Function(
            ffi.Pointer<llama_context>,
            ffi.Pointer<ffi.Uint8>,
            ffi.Size,
            llama_seq_id,
            llama_state_seq_flags,
          )
        >
      >('llama_state_seq_set_data_ext');
  late final _llama_state_seq_set_data_ext = _llama_state_seq_set_data_extPtr
      .asFunction<
        int Function(
          ffi.Pointer<llama_context>,
          ffi.Pointer<ffi.Uint8>,
          int,
          int,
          int,
        )
      >();

  /// Return batch for single sequence of tokens
  /// The sequence ID will be fixed to 0
  /// The position of the tokens will be tracked automatically by llama_decode
  ///
  /// NOTE: this is a helper function to facilitate transition to the new batch API - avoid using it
  llama_batch llama_batch_get_one(
    ffi.Pointer<llama_token> tokens,
    int n_tokens,
  ) {
    return _llama_batch_get_one(tokens, n_tokens);
  }

  late final _llama_batch_get_onePtr =
      _lookup<
        ffi.NativeFunction<
          llama_batch Function(ffi.Pointer<llama_token>, ffi.Int32)
        >
      >('llama_batch_get_one');
  late final _llama_batch_get_one = _llama_batch_get_onePtr
      .asFunction<llama_batch Function(ffi.Pointer<llama_token>, int)>();

  /// Allocates a batch of tokens on the heap that can hold a maximum of n_tokens
  /// Each token can be assigned up to n_seq_max sequence ids
  /// The batch has to be freed with llama_batch_free()
  /// If embd != 0, llama_batch.embd will be allocated with size of n_tokens * embd * sizeof(float)
  /// Otherwise, llama_batch.token will be allocated to store n_tokens llama_token
  /// The rest of the llama_batch members are allocated with size n_tokens
  /// All members are left uninitialized
  llama_batch llama_batch_init(int n_tokens, int embd, int n_seq_max) {
    return _llama_batch_init(n_tokens, embd, n_seq_max);
  }

  late final _llama_batch_initPtr =
      _lookup<
        ffi.NativeFunction<
          llama_batch Function(ffi.Int32, ffi.Int32, ffi.Int32)
        >
      >('llama_batch_init');
  late final _llama_batch_init = _llama_batch_initPtr
      .asFunction<llama_batch Function(int, int, int)>();

  /// Frees a batch of tokens allocated with llama_batch_init()
  void llama_batch_free(llama_batch batch) {
    return _llama_batch_free(batch);
  }

  late final _llama_batch_freePtr =
      _lookup<ffi.NativeFunction<ffi.Void Function(llama_batch)>>(
        'llama_batch_free',
      );
  late final _llama_batch_free = _llama_batch_freePtr
      .asFunction<void Function(llama_batch)>();

  /// Process a batch of tokens.
  /// In contrast to llama_decode() - this call does not use KV cache.
  /// For encode-decoder contexts, processes the batch using the encoder.
  /// Can store the encoder output internally for later use by the decoder's cross-attention layers.
  /// 0 - success
  /// < 0 - error. the memory state is restored to the state before this call
  int llama_encode(ffi.Pointer<llama_context> ctx, llama_batch batch) {
    return _llama_encode(ctx, batch);
  }

  late final _llama_encodePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Int32 Function(ffi.Pointer<llama_context>, llama_batch)
        >
      >('llama_encode');
  late final _llama_encode = _llama_encodePtr
      .asFunction<int Function(ffi.Pointer<llama_context>, llama_batch)>();

  /// Process a batch of tokens.
  /// Requires the context to have a memory.
  /// For encode-decoder contexts, processes the batch using the decoder.
  /// Positive return values does not mean a fatal error, but rather a warning.
  /// Upon fatal-error or abort, the ubatches that managed to be been processed will remain in the memory state of the context
  /// To handle this correctly, query the memory state using llama_memory_seq_pos_min() and llama_memory_seq_pos_max()
  /// Upon other return values, the memory state is restored to the state before this call
  /// 0 - success
  /// 1 - could not find a KV slot for the batch (try reducing the size of the batch or increase the context)
  /// 2 - aborted     (processed ubatches will remain in the context's memory)
  /// -1 - invalid input batch
  /// < -1 - fatal error (processed ubatches will remain in the context's memory)
  int llama_decode(ffi.Pointer<llama_context> ctx, llama_batch batch) {
    return _llama_decode(ctx, batch);
  }

  late final _llama_decodePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Int32 Function(ffi.Pointer<llama_context>, llama_batch)
        >
      >('llama_decode');
  late final _llama_decode = _llama_decodePtr
      .asFunction<int Function(ffi.Pointer<llama_context>, llama_batch)>();

  /// Set the number of threads used for decoding
  /// n_threads is the number of threads used for generation (single token)
  /// n_threads_batch is the number of threads used for prompt and batch processing (multiple tokens)
  void llama_set_n_threads(
    ffi.Pointer<llama_context> ctx,
    int n_threads,
    int n_threads_batch,
  ) {
    return _llama_set_n_threads(ctx, n_threads, n_threads_batch);
  }

  late final _llama_set_n_threadsPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<llama_context>, ffi.Int32, ffi.Int32)
        >
      >('llama_set_n_threads');
  late final _llama_set_n_threads = _llama_set_n_threadsPtr
      .asFunction<void Function(ffi.Pointer<llama_context>, int, int)>();

  /// Get the number of threads used for generation of a single token.
  int llama_n_threads(ffi.Pointer<llama_context> ctx) {
    return _llama_n_threads(ctx);
  }

  late final _llama_n_threadsPtr =
      _lookup<
        ffi.NativeFunction<ffi.Int32 Function(ffi.Pointer<llama_context>)>
      >('llama_n_threads');
  late final _llama_n_threads = _llama_n_threadsPtr
      .asFunction<int Function(ffi.Pointer<llama_context>)>();

  /// Get the number of threads used for prompt and batch processing (multiple token).
  int llama_n_threads_batch(ffi.Pointer<llama_context> ctx) {
    return _llama_n_threads_batch(ctx);
  }

  late final _llama_n_threads_batchPtr =
      _lookup<
        ffi.NativeFunction<ffi.Int32 Function(ffi.Pointer<llama_context>)>
      >('llama_n_threads_batch');
  late final _llama_n_threads_batch = _llama_n_threads_batchPtr
      .asFunction<int Function(ffi.Pointer<llama_context>)>();

  /// Set whether the context outputs embeddings or not
  /// TODO: rename to avoid confusion with llama_get_embeddings()
  void llama_set_embeddings(ffi.Pointer<llama_context> ctx, bool embeddings) {
    return _llama_set_embeddings(ctx, embeddings);
  }

  late final _llama_set_embeddingsPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<llama_context>, ffi.Bool)
        >
      >('llama_set_embeddings');
  late final _llama_set_embeddings = _llama_set_embeddingsPtr
      .asFunction<void Function(ffi.Pointer<llama_context>, bool)>();

  /// Set whether to use causal attention or not
  /// If set to true, the model will only attend to the past tokens
  void llama_set_causal_attn(ffi.Pointer<llama_context> ctx, bool causal_attn) {
    return _llama_set_causal_attn(ctx, causal_attn);
  }

  late final _llama_set_causal_attnPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<llama_context>, ffi.Bool)
        >
      >('llama_set_causal_attn');
  late final _llama_set_causal_attn = _llama_set_causal_attnPtr
      .asFunction<void Function(ffi.Pointer<llama_context>, bool)>();

  /// Set whether the model is in warmup mode or not
  /// If true, all model tensors are activated during llama_decode() to load and cache their weights.
  void llama_set_warmup(ffi.Pointer<llama_context> ctx, bool warmup) {
    return _llama_set_warmup(ctx, warmup);
  }

  late final _llama_set_warmupPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<llama_context>, ffi.Bool)
        >
      >('llama_set_warmup');
  late final _llama_set_warmup = _llama_set_warmupPtr
      .asFunction<void Function(ffi.Pointer<llama_context>, bool)>();

  /// Set abort callback
  void llama_set_abort_callback(
    ffi.Pointer<llama_context> ctx,
    ggml_abort_callback abort_callback,
    ffi.Pointer<ffi.Void> abort_callback_data,
  ) {
    return _llama_set_abort_callback(ctx, abort_callback, abort_callback_data);
  }

  late final _llama_set_abort_callbackPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Void Function(
            ffi.Pointer<llama_context>,
            ggml_abort_callback,
            ffi.Pointer<ffi.Void>,
          )
        >
      >('llama_set_abort_callback');
  late final _llama_set_abort_callback = _llama_set_abort_callbackPtr
      .asFunction<
        void Function(
          ffi.Pointer<llama_context>,
          ggml_abort_callback,
          ffi.Pointer<ffi.Void>,
        )
      >();

  /// Wait until all computations are finished
  /// This is automatically done when using one of the functions below to obtain the computation results
  /// and is not necessary to call it explicitly in most cases
  void llama_synchronize(ffi.Pointer<llama_context> ctx) {
    return _llama_synchronize(ctx);
  }

  late final _llama_synchronizePtr =
      _lookup<
        ffi.NativeFunction<ffi.Void Function(ffi.Pointer<llama_context>)>
      >('llama_synchronize');
  late final _llama_synchronize = _llama_synchronizePtr
      .asFunction<void Function(ffi.Pointer<llama_context>)>();

  /// Token logits obtained from the last call to llama_decode()
  /// The logits for which llama_batch.logits[i] != 0 are stored contiguously
  /// in the order they have appeared in the batch.
  /// Rows: number of tokens for which llama_batch.logits[i] != 0
  /// Cols: n_vocab
  /// TODO: deprecate in favor of llama_get_logits_ith() (ref: https://github.com/ggml-org/llama.cpp/pull/14853#issuecomment-3113143522)
  ffi.Pointer<ffi.Float> llama_get_logits(ffi.Pointer<llama_context> ctx) {
    return _llama_get_logits(ctx);
  }

  late final _llama_get_logitsPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ffi.Float> Function(ffi.Pointer<llama_context>)
        >
      >('llama_get_logits');
  late final _llama_get_logits = _llama_get_logitsPtr
      .asFunction<
        ffi.Pointer<ffi.Float> Function(ffi.Pointer<llama_context>)
      >();

  /// Logits for the ith token. For positive indices, Equivalent to:
  /// llama_get_logits(ctx) + ctx->output_ids[i]*n_vocab
  /// Negative indicies can be used to access logits in reverse order, -1 is the last logit.
  /// returns NULL for invalid ids.
  ffi.Pointer<ffi.Float> llama_get_logits_ith(
    ffi.Pointer<llama_context> ctx,
    int i,
  ) {
    return _llama_get_logits_ith(ctx, i);
  }

  late final _llama_get_logits_ithPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ffi.Float> Function(ffi.Pointer<llama_context>, ffi.Int32)
        >
      >('llama_get_logits_ith');
  late final _llama_get_logits_ith = _llama_get_logits_ithPtr
      .asFunction<
        ffi.Pointer<ffi.Float> Function(ffi.Pointer<llama_context>, int)
      >();

  /// Get all output token embeddings.
  /// when pooling_type == LLAMA_POOLING_TYPE_NONE or when using a generative model,
  /// the embeddings for which llama_batch.logits[i] != 0 are stored contiguously
  /// in the order they have appeared in the batch.
  /// shape: [n_outputs*n_embd]
  /// Otherwise, returns NULL.
  /// TODO: deprecate in favor of llama_get_embeddings_ith() (ref: https://github.com/ggml-org/llama.cpp/pull/14853#issuecomment-3113143522)
  ffi.Pointer<ffi.Float> llama_get_embeddings(ffi.Pointer<llama_context> ctx) {
    return _llama_get_embeddings(ctx);
  }

  late final _llama_get_embeddingsPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ffi.Float> Function(ffi.Pointer<llama_context>)
        >
      >('llama_get_embeddings');
  late final _llama_get_embeddings = _llama_get_embeddingsPtr
      .asFunction<
        ffi.Pointer<ffi.Float> Function(ffi.Pointer<llama_context>)
      >();

  /// Get the embeddings for the ith token. For positive indices, Equivalent to:
  /// llama_get_embeddings(ctx) + ctx->output_ids[i]*n_embd
  /// Negative indicies can be used to access embeddings in reverse order, -1 is the last embedding.
  /// shape: [n_embd] (1-dimensional)
  /// returns NULL for invalid ids.
  ffi.Pointer<ffi.Float> llama_get_embeddings_ith(
    ffi.Pointer<llama_context> ctx,
    int i,
  ) {
    return _llama_get_embeddings_ith(ctx, i);
  }

  late final _llama_get_embeddings_ithPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ffi.Float> Function(ffi.Pointer<llama_context>, ffi.Int32)
        >
      >('llama_get_embeddings_ith');
  late final _llama_get_embeddings_ith = _llama_get_embeddings_ithPtr
      .asFunction<
        ffi.Pointer<ffi.Float> Function(ffi.Pointer<llama_context>, int)
      >();

  /// Get the embeddings for a sequence id
  /// Returns NULL if pooling_type is LLAMA_POOLING_TYPE_NONE
  /// when pooling_type == LLAMA_POOLING_TYPE_RANK, returns float[n_cls_out] with the rank(s) of the sequence
  /// otherwise: float[n_embd] (1-dimensional)
  ffi.Pointer<ffi.Float> llama_get_embeddings_seq(
    ffi.Pointer<llama_context> ctx,
    int seq_id,
  ) {
    return _llama_get_embeddings_seq(ctx, seq_id);
  }

  late final _llama_get_embeddings_seqPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ffi.Float> Function(
            ffi.Pointer<llama_context>,
            llama_seq_id,
          )
        >
      >('llama_get_embeddings_seq');
  late final _llama_get_embeddings_seq = _llama_get_embeddings_seqPtr
      .asFunction<
        ffi.Pointer<ffi.Float> Function(ffi.Pointer<llama_context>, int)
      >();

  /// Vocab
  ffi.Pointer<ffi.Char> llama_vocab_get_text(
    ffi.Pointer<llama_vocab> vocab,
    int token,
  ) {
    return _llama_vocab_get_text(vocab, token);
  }

  late final _llama_vocab_get_textPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ffi.Char> Function(ffi.Pointer<llama_vocab>, llama_token)
        >
      >('llama_vocab_get_text');
  late final _llama_vocab_get_text = _llama_vocab_get_textPtr
      .asFunction<
        ffi.Pointer<ffi.Char> Function(ffi.Pointer<llama_vocab>, int)
      >();

  double llama_vocab_get_score(ffi.Pointer<llama_vocab> vocab, int token) {
    return _llama_vocab_get_score(vocab, token);
  }

  late final _llama_vocab_get_scorePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Float Function(ffi.Pointer<llama_vocab>, llama_token)
        >
      >('llama_vocab_get_score');
  late final _llama_vocab_get_score = _llama_vocab_get_scorePtr
      .asFunction<double Function(ffi.Pointer<llama_vocab>, int)>();

  llama_token_attr llama_vocab_get_attr(
    ffi.Pointer<llama_vocab> vocab,
    Dartllama_token token,
  ) {
    return llama_token_attr.fromValue(_llama_vocab_get_attr(vocab, token));
  }

  late final _llama_vocab_get_attrPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.UnsignedInt Function(ffi.Pointer<llama_vocab>, llama_token)
        >
      >('llama_vocab_get_attr');
  late final _llama_vocab_get_attr = _llama_vocab_get_attrPtr
      .asFunction<int Function(ffi.Pointer<llama_vocab>, int)>();

  /// Check if the token is supposed to end generation (end-of-generation, eg. EOS, EOT, etc.)
  bool llama_vocab_is_eog(ffi.Pointer<llama_vocab> vocab, int token) {
    return _llama_vocab_is_eog(vocab, token);
  }

  late final _llama_vocab_is_eogPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Bool Function(ffi.Pointer<llama_vocab>, llama_token)
        >
      >('llama_vocab_is_eog');
  late final _llama_vocab_is_eog = _llama_vocab_is_eogPtr
      .asFunction<bool Function(ffi.Pointer<llama_vocab>, int)>();

  /// Identify if Token Id is a control token or a render-able token
  bool llama_vocab_is_control(ffi.Pointer<llama_vocab> vocab, int token) {
    return _llama_vocab_is_control(vocab, token);
  }

  late final _llama_vocab_is_controlPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Bool Function(ffi.Pointer<llama_vocab>, llama_token)
        >
      >('llama_vocab_is_control');
  late final _llama_vocab_is_control = _llama_vocab_is_controlPtr
      .asFunction<bool Function(ffi.Pointer<llama_vocab>, int)>();

  /// Special tokens
  int llama_vocab_bos(ffi.Pointer<llama_vocab> vocab) {
    return _llama_vocab_bos(vocab);
  }

  late final _llama_vocab_bosPtr =
      _lookup<
        ffi.NativeFunction<llama_token Function(ffi.Pointer<llama_vocab>)>
      >('llama_vocab_bos');
  late final _llama_vocab_bos = _llama_vocab_bosPtr
      .asFunction<int Function(ffi.Pointer<llama_vocab>)>();

  int llama_vocab_eos(ffi.Pointer<llama_vocab> vocab) {
    return _llama_vocab_eos(vocab);
  }

  late final _llama_vocab_eosPtr =
      _lookup<
        ffi.NativeFunction<llama_token Function(ffi.Pointer<llama_vocab>)>
      >('llama_vocab_eos');
  late final _llama_vocab_eos = _llama_vocab_eosPtr
      .asFunction<int Function(ffi.Pointer<llama_vocab>)>();

  int llama_vocab_eot(ffi.Pointer<llama_vocab> vocab) {
    return _llama_vocab_eot(vocab);
  }

  late final _llama_vocab_eotPtr =
      _lookup<
        ffi.NativeFunction<llama_token Function(ffi.Pointer<llama_vocab>)>
      >('llama_vocab_eot');
  late final _llama_vocab_eot = _llama_vocab_eotPtr
      .asFunction<int Function(ffi.Pointer<llama_vocab>)>();

  int llama_vocab_sep(ffi.Pointer<llama_vocab> vocab) {
    return _llama_vocab_sep(vocab);
  }

  late final _llama_vocab_sepPtr =
      _lookup<
        ffi.NativeFunction<llama_token Function(ffi.Pointer<llama_vocab>)>
      >('llama_vocab_sep');
  late final _llama_vocab_sep = _llama_vocab_sepPtr
      .asFunction<int Function(ffi.Pointer<llama_vocab>)>();

  int llama_vocab_nl(ffi.Pointer<llama_vocab> vocab) {
    return _llama_vocab_nl(vocab);
  }

  late final _llama_vocab_nlPtr =
      _lookup<
        ffi.NativeFunction<llama_token Function(ffi.Pointer<llama_vocab>)>
      >('llama_vocab_nl');
  late final _llama_vocab_nl = _llama_vocab_nlPtr
      .asFunction<int Function(ffi.Pointer<llama_vocab>)>();

  int llama_vocab_pad(ffi.Pointer<llama_vocab> vocab) {
    return _llama_vocab_pad(vocab);
  }

  late final _llama_vocab_padPtr =
      _lookup<
        ffi.NativeFunction<llama_token Function(ffi.Pointer<llama_vocab>)>
      >('llama_vocab_pad');
  late final _llama_vocab_pad = _llama_vocab_padPtr
      .asFunction<int Function(ffi.Pointer<llama_vocab>)>();

  int llama_vocab_mask(ffi.Pointer<llama_vocab> vocab) {
    return _llama_vocab_mask(vocab);
  }

  late final _llama_vocab_maskPtr =
      _lookup<
        ffi.NativeFunction<llama_token Function(ffi.Pointer<llama_vocab>)>
      >('llama_vocab_mask');
  late final _llama_vocab_mask = _llama_vocab_maskPtr
      .asFunction<int Function(ffi.Pointer<llama_vocab>)>();

  bool llama_vocab_get_add_bos(ffi.Pointer<llama_vocab> vocab) {
    return _llama_vocab_get_add_bos(vocab);
  }

  late final _llama_vocab_get_add_bosPtr =
      _lookup<ffi.NativeFunction<ffi.Bool Function(ffi.Pointer<llama_vocab>)>>(
        'llama_vocab_get_add_bos',
      );
  late final _llama_vocab_get_add_bos = _llama_vocab_get_add_bosPtr
      .asFunction<bool Function(ffi.Pointer<llama_vocab>)>();

  bool llama_vocab_get_add_eos(ffi.Pointer<llama_vocab> vocab) {
    return _llama_vocab_get_add_eos(vocab);
  }

  late final _llama_vocab_get_add_eosPtr =
      _lookup<ffi.NativeFunction<ffi.Bool Function(ffi.Pointer<llama_vocab>)>>(
        'llama_vocab_get_add_eos',
      );
  late final _llama_vocab_get_add_eos = _llama_vocab_get_add_eosPtr
      .asFunction<bool Function(ffi.Pointer<llama_vocab>)>();

  bool llama_vocab_get_add_sep(ffi.Pointer<llama_vocab> vocab) {
    return _llama_vocab_get_add_sep(vocab);
  }

  late final _llama_vocab_get_add_sepPtr =
      _lookup<ffi.NativeFunction<ffi.Bool Function(ffi.Pointer<llama_vocab>)>>(
        'llama_vocab_get_add_sep',
      );
  late final _llama_vocab_get_add_sep = _llama_vocab_get_add_sepPtr
      .asFunction<bool Function(ffi.Pointer<llama_vocab>)>();

  int llama_vocab_fim_pre(ffi.Pointer<llama_vocab> vocab) {
    return _llama_vocab_fim_pre(vocab);
  }

  late final _llama_vocab_fim_prePtr =
      _lookup<
        ffi.NativeFunction<llama_token Function(ffi.Pointer<llama_vocab>)>
      >('llama_vocab_fim_pre');
  late final _llama_vocab_fim_pre = _llama_vocab_fim_prePtr
      .asFunction<int Function(ffi.Pointer<llama_vocab>)>();

  int llama_vocab_fim_suf(ffi.Pointer<llama_vocab> vocab) {
    return _llama_vocab_fim_suf(vocab);
  }

  late final _llama_vocab_fim_sufPtr =
      _lookup<
        ffi.NativeFunction<llama_token Function(ffi.Pointer<llama_vocab>)>
      >('llama_vocab_fim_suf');
  late final _llama_vocab_fim_suf = _llama_vocab_fim_sufPtr
      .asFunction<int Function(ffi.Pointer<llama_vocab>)>();

  int llama_vocab_fim_mid(ffi.Pointer<llama_vocab> vocab) {
    return _llama_vocab_fim_mid(vocab);
  }

  late final _llama_vocab_fim_midPtr =
      _lookup<
        ffi.NativeFunction<llama_token Function(ffi.Pointer<llama_vocab>)>
      >('llama_vocab_fim_mid');
  late final _llama_vocab_fim_mid = _llama_vocab_fim_midPtr
      .asFunction<int Function(ffi.Pointer<llama_vocab>)>();

  int llama_vocab_fim_pad(ffi.Pointer<llama_vocab> vocab) {
    return _llama_vocab_fim_pad(vocab);
  }

  late final _llama_vocab_fim_padPtr =
      _lookup<
        ffi.NativeFunction<llama_token Function(ffi.Pointer<llama_vocab>)>
      >('llama_vocab_fim_pad');
  late final _llama_vocab_fim_pad = _llama_vocab_fim_padPtr
      .asFunction<int Function(ffi.Pointer<llama_vocab>)>();

  int llama_vocab_fim_rep(ffi.Pointer<llama_vocab> vocab) {
    return _llama_vocab_fim_rep(vocab);
  }

  late final _llama_vocab_fim_repPtr =
      _lookup<
        ffi.NativeFunction<llama_token Function(ffi.Pointer<llama_vocab>)>
      >('llama_vocab_fim_rep');
  late final _llama_vocab_fim_rep = _llama_vocab_fim_repPtr
      .asFunction<int Function(ffi.Pointer<llama_vocab>)>();

  int llama_vocab_fim_sep(ffi.Pointer<llama_vocab> vocab) {
    return _llama_vocab_fim_sep(vocab);
  }

  late final _llama_vocab_fim_sepPtr =
      _lookup<
        ffi.NativeFunction<llama_token Function(ffi.Pointer<llama_vocab>)>
      >('llama_vocab_fim_sep');
  late final _llama_vocab_fim_sep = _llama_vocab_fim_sepPtr
      .asFunction<int Function(ffi.Pointer<llama_vocab>)>();

  ffi.Pointer<ffi.Char> llama_token_get_text(
    ffi.Pointer<llama_vocab> vocab,
    int token,
  ) {
    return _llama_token_get_text(vocab, token);
  }

  late final _llama_token_get_textPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ffi.Char> Function(ffi.Pointer<llama_vocab>, llama_token)
        >
      >('llama_token_get_text');
  late final _llama_token_get_text = _llama_token_get_textPtr
      .asFunction<
        ffi.Pointer<ffi.Char> Function(ffi.Pointer<llama_vocab>, int)
      >();

  double llama_token_get_score(ffi.Pointer<llama_vocab> vocab, int token) {
    return _llama_token_get_score(vocab, token);
  }

  late final _llama_token_get_scorePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Float Function(ffi.Pointer<llama_vocab>, llama_token)
        >
      >('llama_token_get_score');
  late final _llama_token_get_score = _llama_token_get_scorePtr
      .asFunction<double Function(ffi.Pointer<llama_vocab>, int)>();

  llama_token_attr llama_token_get_attr(
    ffi.Pointer<llama_vocab> vocab,
    Dartllama_token token,
  ) {
    return llama_token_attr.fromValue(_llama_token_get_attr(vocab, token));
  }

  late final _llama_token_get_attrPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.UnsignedInt Function(ffi.Pointer<llama_vocab>, llama_token)
        >
      >('llama_token_get_attr');
  late final _llama_token_get_attr = _llama_token_get_attrPtr
      .asFunction<int Function(ffi.Pointer<llama_vocab>, int)>();

  bool llama_token_is_eog(ffi.Pointer<llama_vocab> vocab, int token) {
    return _llama_token_is_eog(vocab, token);
  }

  late final _llama_token_is_eogPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Bool Function(ffi.Pointer<llama_vocab>, llama_token)
        >
      >('llama_token_is_eog');
  late final _llama_token_is_eog = _llama_token_is_eogPtr
      .asFunction<bool Function(ffi.Pointer<llama_vocab>, int)>();

  bool llama_token_is_control(ffi.Pointer<llama_vocab> vocab, int token) {
    return _llama_token_is_control(vocab, token);
  }

  late final _llama_token_is_controlPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Bool Function(ffi.Pointer<llama_vocab>, llama_token)
        >
      >('llama_token_is_control');
  late final _llama_token_is_control = _llama_token_is_controlPtr
      .asFunction<bool Function(ffi.Pointer<llama_vocab>, int)>();

  int llama_token_bos(ffi.Pointer<llama_vocab> vocab) {
    return _llama_token_bos(vocab);
  }

  late final _llama_token_bosPtr =
      _lookup<
        ffi.NativeFunction<llama_token Function(ffi.Pointer<llama_vocab>)>
      >('llama_token_bos');
  late final _llama_token_bos = _llama_token_bosPtr
      .asFunction<int Function(ffi.Pointer<llama_vocab>)>();

  int llama_token_eos(ffi.Pointer<llama_vocab> vocab) {
    return _llama_token_eos(vocab);
  }

  late final _llama_token_eosPtr =
      _lookup<
        ffi.NativeFunction<llama_token Function(ffi.Pointer<llama_vocab>)>
      >('llama_token_eos');
  late final _llama_token_eos = _llama_token_eosPtr
      .asFunction<int Function(ffi.Pointer<llama_vocab>)>();

  int llama_token_eot(ffi.Pointer<llama_vocab> vocab) {
    return _llama_token_eot(vocab);
  }

  late final _llama_token_eotPtr =
      _lookup<
        ffi.NativeFunction<llama_token Function(ffi.Pointer<llama_vocab>)>
      >('llama_token_eot');
  late final _llama_token_eot = _llama_token_eotPtr
      .asFunction<int Function(ffi.Pointer<llama_vocab>)>();

  int llama_token_cls(ffi.Pointer<llama_vocab> vocab) {
    return _llama_token_cls(vocab);
  }

  late final _llama_token_clsPtr =
      _lookup<
        ffi.NativeFunction<llama_token Function(ffi.Pointer<llama_vocab>)>
      >('llama_token_cls');
  late final _llama_token_cls = _llama_token_clsPtr
      .asFunction<int Function(ffi.Pointer<llama_vocab>)>();

  int llama_token_sep(ffi.Pointer<llama_vocab> vocab) {
    return _llama_token_sep(vocab);
  }

  late final _llama_token_sepPtr =
      _lookup<
        ffi.NativeFunction<llama_token Function(ffi.Pointer<llama_vocab>)>
      >('llama_token_sep');
  late final _llama_token_sep = _llama_token_sepPtr
      .asFunction<int Function(ffi.Pointer<llama_vocab>)>();

  int llama_token_nl(ffi.Pointer<llama_vocab> vocab) {
    return _llama_token_nl(vocab);
  }

  late final _llama_token_nlPtr =
      _lookup<
        ffi.NativeFunction<llama_token Function(ffi.Pointer<llama_vocab>)>
      >('llama_token_nl');
  late final _llama_token_nl = _llama_token_nlPtr
      .asFunction<int Function(ffi.Pointer<llama_vocab>)>();

  int llama_token_pad(ffi.Pointer<llama_vocab> vocab) {
    return _llama_token_pad(vocab);
  }

  late final _llama_token_padPtr =
      _lookup<
        ffi.NativeFunction<llama_token Function(ffi.Pointer<llama_vocab>)>
      >('llama_token_pad');
  late final _llama_token_pad = _llama_token_padPtr
      .asFunction<int Function(ffi.Pointer<llama_vocab>)>();

  bool llama_add_bos_token(ffi.Pointer<llama_vocab> vocab) {
    return _llama_add_bos_token(vocab);
  }

  late final _llama_add_bos_tokenPtr =
      _lookup<ffi.NativeFunction<ffi.Bool Function(ffi.Pointer<llama_vocab>)>>(
        'llama_add_bos_token',
      );
  late final _llama_add_bos_token = _llama_add_bos_tokenPtr
      .asFunction<bool Function(ffi.Pointer<llama_vocab>)>();

  bool llama_add_eos_token(ffi.Pointer<llama_vocab> vocab) {
    return _llama_add_eos_token(vocab);
  }

  late final _llama_add_eos_tokenPtr =
      _lookup<ffi.NativeFunction<ffi.Bool Function(ffi.Pointer<llama_vocab>)>>(
        'llama_add_eos_token',
      );
  late final _llama_add_eos_token = _llama_add_eos_tokenPtr
      .asFunction<bool Function(ffi.Pointer<llama_vocab>)>();

  int llama_token_fim_pre(ffi.Pointer<llama_vocab> vocab) {
    return _llama_token_fim_pre(vocab);
  }

  late final _llama_token_fim_prePtr =
      _lookup<
        ffi.NativeFunction<llama_token Function(ffi.Pointer<llama_vocab>)>
      >('llama_token_fim_pre');
  late final _llama_token_fim_pre = _llama_token_fim_prePtr
      .asFunction<int Function(ffi.Pointer<llama_vocab>)>();

  int llama_token_fim_suf(ffi.Pointer<llama_vocab> vocab) {
    return _llama_token_fim_suf(vocab);
  }

  late final _llama_token_fim_sufPtr =
      _lookup<
        ffi.NativeFunction<llama_token Function(ffi.Pointer<llama_vocab>)>
      >('llama_token_fim_suf');
  late final _llama_token_fim_suf = _llama_token_fim_sufPtr
      .asFunction<int Function(ffi.Pointer<llama_vocab>)>();

  int llama_token_fim_mid(ffi.Pointer<llama_vocab> vocab) {
    return _llama_token_fim_mid(vocab);
  }

  late final _llama_token_fim_midPtr =
      _lookup<
        ffi.NativeFunction<llama_token Function(ffi.Pointer<llama_vocab>)>
      >('llama_token_fim_mid');
  late final _llama_token_fim_mid = _llama_token_fim_midPtr
      .asFunction<int Function(ffi.Pointer<llama_vocab>)>();

  int llama_token_fim_pad(ffi.Pointer<llama_vocab> vocab) {
    return _llama_token_fim_pad(vocab);
  }

  late final _llama_token_fim_padPtr =
      _lookup<
        ffi.NativeFunction<llama_token Function(ffi.Pointer<llama_vocab>)>
      >('llama_token_fim_pad');
  late final _llama_token_fim_pad = _llama_token_fim_padPtr
      .asFunction<int Function(ffi.Pointer<llama_vocab>)>();

  int llama_token_fim_rep(ffi.Pointer<llama_vocab> vocab) {
    return _llama_token_fim_rep(vocab);
  }

  late final _llama_token_fim_repPtr =
      _lookup<
        ffi.NativeFunction<llama_token Function(ffi.Pointer<llama_vocab>)>
      >('llama_token_fim_rep');
  late final _llama_token_fim_rep = _llama_token_fim_repPtr
      .asFunction<int Function(ffi.Pointer<llama_vocab>)>();

  int llama_token_fim_sep(ffi.Pointer<llama_vocab> vocab) {
    return _llama_token_fim_sep(vocab);
  }

  late final _llama_token_fim_sepPtr =
      _lookup<
        ffi.NativeFunction<llama_token Function(ffi.Pointer<llama_vocab>)>
      >('llama_token_fim_sep');
  late final _llama_token_fim_sep = _llama_token_fim_sepPtr
      .asFunction<int Function(ffi.Pointer<llama_vocab>)>();

  int llama_vocab_cls(ffi.Pointer<llama_vocab> vocab) {
    return _llama_vocab_cls(vocab);
  }

  late final _llama_vocab_clsPtr =
      _lookup<
        ffi.NativeFunction<llama_token Function(ffi.Pointer<llama_vocab>)>
      >('llama_vocab_cls');
  late final _llama_vocab_cls = _llama_vocab_clsPtr
      .asFunction<int Function(ffi.Pointer<llama_vocab>)>();

  /// @details Convert the provided text into tokens.
  /// @param tokens The tokens pointer must be large enough to hold the resulting tokens.
  /// @return Returns the number of tokens on success, no more than n_tokens_max
  /// @return Returns a negative number on failure - the number of tokens that would have been returned
  /// @return Returns INT32_MIN on overflow (e.g., tokenization result size exceeds int32_t limit)
  /// @param add_special Allow to add BOS and EOS tokens if model is configured to do so.
  /// @param parse_special Allow tokenizing special and/or control tokens which otherwise are not exposed and treated
  /// as plaintext. Does not insert a leading space.
  int llama_tokenize(
    ffi.Pointer<llama_vocab> vocab,
    ffi.Pointer<ffi.Char> text,
    int text_len,
    ffi.Pointer<llama_token> tokens,
    int n_tokens_max,
    bool add_special,
    bool parse_special,
  ) {
    return _llama_tokenize(
      vocab,
      text,
      text_len,
      tokens,
      n_tokens_max,
      add_special,
      parse_special,
    );
  }

  late final _llama_tokenizePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Int32 Function(
            ffi.Pointer<llama_vocab>,
            ffi.Pointer<ffi.Char>,
            ffi.Int32,
            ffi.Pointer<llama_token>,
            ffi.Int32,
            ffi.Bool,
            ffi.Bool,
          )
        >
      >('llama_tokenize');
  late final _llama_tokenize = _llama_tokenizePtr
      .asFunction<
        int Function(
          ffi.Pointer<llama_vocab>,
          ffi.Pointer<ffi.Char>,
          int,
          ffi.Pointer<llama_token>,
          int,
          bool,
          bool,
        )
      >();

  /// Token Id -> Piece.
  /// Uses the vocabulary in the provided context.
  /// Does not write null terminator to the buffer.
  /// User can skip up to 'lstrip' leading spaces before copying (useful when encoding/decoding multiple tokens with 'add_space_prefix')
  /// @param special If true, special tokens are rendered in the output.
  int llama_token_to_piece(
    ffi.Pointer<llama_vocab> vocab,
    int token,
    ffi.Pointer<ffi.Char> buf,
    int length,
    int lstrip,
    bool special,
  ) {
    return _llama_token_to_piece(vocab, token, buf, length, lstrip, special);
  }

  late final _llama_token_to_piecePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Int32 Function(
            ffi.Pointer<llama_vocab>,
            llama_token,
            ffi.Pointer<ffi.Char>,
            ffi.Int32,
            ffi.Int32,
            ffi.Bool,
          )
        >
      >('llama_token_to_piece');
  late final _llama_token_to_piece = _llama_token_to_piecePtr
      .asFunction<
        int Function(
          ffi.Pointer<llama_vocab>,
          int,
          ffi.Pointer<ffi.Char>,
          int,
          int,
          bool,
        )
      >();

  /// @details Convert the provided tokens into text (inverse of llama_tokenize()).
  /// @param text The char pointer must be large enough to hold the resulting text.
  /// @return Returns the number of chars/bytes on success, no more than text_len_max.
  /// @return Returns a negative number on failure - the number of chars/bytes that would have been returned.
  /// @param remove_special Allow to remove BOS and EOS tokens if model is configured to do so.
  /// @param unparse_special If true, special tokens are rendered in the output.
  int llama_detokenize(
    ffi.Pointer<llama_vocab> vocab,
    ffi.Pointer<llama_token> tokens,
    int n_tokens,
    ffi.Pointer<ffi.Char> text,
    int text_len_max,
    bool remove_special,
    bool unparse_special,
  ) {
    return _llama_detokenize(
      vocab,
      tokens,
      n_tokens,
      text,
      text_len_max,
      remove_special,
      unparse_special,
    );
  }

  late final _llama_detokenizePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Int32 Function(
            ffi.Pointer<llama_vocab>,
            ffi.Pointer<llama_token>,
            ffi.Int32,
            ffi.Pointer<ffi.Char>,
            ffi.Int32,
            ffi.Bool,
            ffi.Bool,
          )
        >
      >('llama_detokenize');
  late final _llama_detokenize = _llama_detokenizePtr
      .asFunction<
        int Function(
          ffi.Pointer<llama_vocab>,
          ffi.Pointer<llama_token>,
          int,
          ffi.Pointer<ffi.Char>,
          int,
          bool,
          bool,
        )
      >();

  /// Apply chat template. Inspired by hf apply_chat_template() on python.
  /// Both "model" and "custom_template" are optional, but at least one is required. "custom_template" has higher precedence than "model"
  /// NOTE: This function does not use a jinja parser. It only support a pre-defined list of template. See more: https://github.com/ggml-org/llama.cpp/wiki/Templates-supported-by-llama_chat_apply_template
  /// @param tmpl A Jinja template to use for this chat. If this is nullptr, the models default chat template will be used instead.
  /// @param chat Pointer to a list of multiple llama_chat_message
  /// @param n_msg Number of llama_chat_message in this chat
  /// @param add_ass Whether to end the prompt with the token(s) that indicate the start of an assistant message.
  /// @param buf A buffer to hold the output formatted prompt. The recommended alloc size is 2 * (total number of characters of all messages)
  /// @param length The size of the allocated buffer
  /// @return The total number of bytes of the formatted prompt. If is it larger than the size of buffer, you may need to re-alloc it and then re-apply the template.
  int llama_chat_apply_template(
    ffi.Pointer<ffi.Char> tmpl,
    ffi.Pointer<llama_chat_message> chat,
    int n_msg,
    bool add_ass,
    ffi.Pointer<ffi.Char> buf,
    int length,
  ) {
    return _llama_chat_apply_template(tmpl, chat, n_msg, add_ass, buf, length);
  }

  late final _llama_chat_apply_templatePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Int32 Function(
            ffi.Pointer<ffi.Char>,
            ffi.Pointer<llama_chat_message>,
            ffi.Size,
            ffi.Bool,
            ffi.Pointer<ffi.Char>,
            ffi.Int32,
          )
        >
      >('llama_chat_apply_template');
  late final _llama_chat_apply_template = _llama_chat_apply_templatePtr
      .asFunction<
        int Function(
          ffi.Pointer<ffi.Char>,
          ffi.Pointer<llama_chat_message>,
          int,
          bool,
          ffi.Pointer<ffi.Char>,
          int,
        )
      >();

  /// Get list of built-in chat templates
  int llama_chat_builtin_templates(
    ffi.Pointer<ffi.Pointer<ffi.Char>> output,
    int len,
  ) {
    return _llama_chat_builtin_templates(output, len);
  }

  late final _llama_chat_builtin_templatesPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Int32 Function(ffi.Pointer<ffi.Pointer<ffi.Char>>, ffi.Size)
        >
      >('llama_chat_builtin_templates');
  late final _llama_chat_builtin_templates = _llama_chat_builtin_templatesPtr
      .asFunction<int Function(ffi.Pointer<ffi.Pointer<ffi.Char>>, int)>();

  /// mirror of llama_sampler_i:
  ffi.Pointer<llama_sampler> llama_sampler_init(
    ffi.Pointer<llama_sampler_i> iface,
    llama_sampler_context_t ctx,
  ) {
    return _llama_sampler_init(iface, ctx);
  }

  late final _llama_sampler_initPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<llama_sampler> Function(
            ffi.Pointer<llama_sampler_i>,
            llama_sampler_context_t,
          )
        >
      >('llama_sampler_init');
  late final _llama_sampler_init = _llama_sampler_initPtr
      .asFunction<
        ffi.Pointer<llama_sampler> Function(
          ffi.Pointer<llama_sampler_i>,
          llama_sampler_context_t,
        )
      >();

  ffi.Pointer<ffi.Char> llama_sampler_name(ffi.Pointer<llama_sampler> smpl) {
    return _llama_sampler_name(smpl);
  }

  late final _llama_sampler_namePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ffi.Char> Function(ffi.Pointer<llama_sampler>)
        >
      >('llama_sampler_name');
  late final _llama_sampler_name = _llama_sampler_namePtr
      .asFunction<ffi.Pointer<ffi.Char> Function(ffi.Pointer<llama_sampler>)>();

  void llama_sampler_accept(ffi.Pointer<llama_sampler> smpl, int token) {
    return _llama_sampler_accept(smpl, token);
  }

  late final _llama_sampler_acceptPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<llama_sampler>, llama_token)
        >
      >('llama_sampler_accept');
  late final _llama_sampler_accept = _llama_sampler_acceptPtr
      .asFunction<void Function(ffi.Pointer<llama_sampler>, int)>();

  void llama_sampler_apply(
    ffi.Pointer<llama_sampler> smpl,
    ffi.Pointer<llama_token_data_array> cur_p,
  ) {
    return _llama_sampler_apply(smpl, cur_p);
  }

  late final _llama_sampler_applyPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Void Function(
            ffi.Pointer<llama_sampler>,
            ffi.Pointer<llama_token_data_array>,
          )
        >
      >('llama_sampler_apply');
  late final _llama_sampler_apply = _llama_sampler_applyPtr
      .asFunction<
        void Function(
          ffi.Pointer<llama_sampler>,
          ffi.Pointer<llama_token_data_array>,
        )
      >();

  void llama_sampler_reset(ffi.Pointer<llama_sampler> smpl) {
    return _llama_sampler_reset(smpl);
  }

  late final _llama_sampler_resetPtr =
      _lookup<
        ffi.NativeFunction<ffi.Void Function(ffi.Pointer<llama_sampler>)>
      >('llama_sampler_reset');
  late final _llama_sampler_reset = _llama_sampler_resetPtr
      .asFunction<void Function(ffi.Pointer<llama_sampler>)>();

  ffi.Pointer<llama_sampler> llama_sampler_clone(
    ffi.Pointer<llama_sampler> smpl,
  ) {
    return _llama_sampler_clone(smpl);
  }

  late final _llama_sampler_clonePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<llama_sampler> Function(ffi.Pointer<llama_sampler>)
        >
      >('llama_sampler_clone');
  late final _llama_sampler_clone = _llama_sampler_clonePtr
      .asFunction<
        ffi.Pointer<llama_sampler> Function(ffi.Pointer<llama_sampler>)
      >();

  /// important: do not free if the sampler has been added to a llama_sampler_chain (via llama_sampler_chain_add)
  void llama_sampler_free(ffi.Pointer<llama_sampler> smpl) {
    return _llama_sampler_free(smpl);
  }

  late final _llama_sampler_freePtr =
      _lookup<
        ffi.NativeFunction<ffi.Void Function(ffi.Pointer<llama_sampler>)>
      >('llama_sampler_free');
  late final _llama_sampler_free = _llama_sampler_freePtr
      .asFunction<void Function(ffi.Pointer<llama_sampler>)>();

  /// llama_sampler_chain
  /// a type of llama_sampler that can chain multiple samplers one after another
  ffi.Pointer<llama_sampler> llama_sampler_chain_init(
    llama_sampler_chain_params params,
  ) {
    return _llama_sampler_chain_init(params);
  }

  late final _llama_sampler_chain_initPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<llama_sampler> Function(llama_sampler_chain_params)
        >
      >('llama_sampler_chain_init');
  late final _llama_sampler_chain_init = _llama_sampler_chain_initPtr
      .asFunction<
        ffi.Pointer<llama_sampler> Function(llama_sampler_chain_params)
      >();

  /// important: takes ownership of the sampler object and will free it when llama_sampler_free is called
  void llama_sampler_chain_add(
    ffi.Pointer<llama_sampler> chain,
    ffi.Pointer<llama_sampler> smpl,
  ) {
    return _llama_sampler_chain_add(chain, smpl);
  }

  late final _llama_sampler_chain_addPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Void Function(
            ffi.Pointer<llama_sampler>,
            ffi.Pointer<llama_sampler>,
          )
        >
      >('llama_sampler_chain_add');
  late final _llama_sampler_chain_add = _llama_sampler_chain_addPtr
      .asFunction<
        void Function(ffi.Pointer<llama_sampler>, ffi.Pointer<llama_sampler>)
      >();

  ffi.Pointer<llama_sampler> llama_sampler_chain_get(
    ffi.Pointer<llama_sampler> chain,
    int i,
  ) {
    return _llama_sampler_chain_get(chain, i);
  }

  late final _llama_sampler_chain_getPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<llama_sampler> Function(
            ffi.Pointer<llama_sampler>,
            ffi.Int32,
          )
        >
      >('llama_sampler_chain_get');
  late final _llama_sampler_chain_get = _llama_sampler_chain_getPtr
      .asFunction<
        ffi.Pointer<llama_sampler> Function(ffi.Pointer<llama_sampler>, int)
      >();

  int llama_sampler_chain_n(ffi.Pointer<llama_sampler> chain) {
    return _llama_sampler_chain_n(chain);
  }

  late final _llama_sampler_chain_nPtr =
      _lookup<ffi.NativeFunction<ffi.Int Function(ffi.Pointer<llama_sampler>)>>(
        'llama_sampler_chain_n',
      );
  late final _llama_sampler_chain_n = _llama_sampler_chain_nPtr
      .asFunction<int Function(ffi.Pointer<llama_sampler>)>();

  /// after removing a sampler, the chain will no longer own it, and it will not be freed when the chain is freed
  ffi.Pointer<llama_sampler> llama_sampler_chain_remove(
    ffi.Pointer<llama_sampler> chain,
    int i,
  ) {
    return _llama_sampler_chain_remove(chain, i);
  }

  late final _llama_sampler_chain_removePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<llama_sampler> Function(
            ffi.Pointer<llama_sampler>,
            ffi.Int32,
          )
        >
      >('llama_sampler_chain_remove');
  late final _llama_sampler_chain_remove = _llama_sampler_chain_removePtr
      .asFunction<
        ffi.Pointer<llama_sampler> Function(ffi.Pointer<llama_sampler>, int)
      >();

  /// available samplers:
  ffi.Pointer<llama_sampler> llama_sampler_init_greedy() {
    return _llama_sampler_init_greedy();
  }

  late final _llama_sampler_init_greedyPtr =
      _lookup<ffi.NativeFunction<ffi.Pointer<llama_sampler> Function()>>(
        'llama_sampler_init_greedy',
      );
  late final _llama_sampler_init_greedy = _llama_sampler_init_greedyPtr
      .asFunction<ffi.Pointer<llama_sampler> Function()>();

  ffi.Pointer<llama_sampler> llama_sampler_init_dist(int seed) {
    return _llama_sampler_init_dist(seed);
  }

  late final _llama_sampler_init_distPtr =
      _lookup<
        ffi.NativeFunction<ffi.Pointer<llama_sampler> Function(ffi.Uint32)>
      >('llama_sampler_init_dist');
  late final _llama_sampler_init_dist = _llama_sampler_init_distPtr
      .asFunction<ffi.Pointer<llama_sampler> Function(int)>();

  /// @details Top-K sampling described in academic paper "The Curious Case of Neural Text Degeneration" https://arxiv.org/abs/1904.09751
  /// Setting k <= 0 makes this a noop
  ffi.Pointer<llama_sampler> llama_sampler_init_top_k(int k) {
    return _llama_sampler_init_top_k(k);
  }

  late final _llama_sampler_init_top_kPtr =
      _lookup<
        ffi.NativeFunction<ffi.Pointer<llama_sampler> Function(ffi.Int32)>
      >('llama_sampler_init_top_k');
  late final _llama_sampler_init_top_k = _llama_sampler_init_top_kPtr
      .asFunction<ffi.Pointer<llama_sampler> Function(int)>();

  /// @details Nucleus sampling described in academic paper "The Curious Case of Neural Text Degeneration" https://arxiv.org/abs/1904.09751
  ffi.Pointer<llama_sampler> llama_sampler_init_top_p(double p, int min_keep) {
    return _llama_sampler_init_top_p(p, min_keep);
  }

  late final _llama_sampler_init_top_pPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<llama_sampler> Function(ffi.Float, ffi.Size)
        >
      >('llama_sampler_init_top_p');
  late final _llama_sampler_init_top_p = _llama_sampler_init_top_pPtr
      .asFunction<ffi.Pointer<llama_sampler> Function(double, int)>();

  /// @details Minimum P sampling as described in https://github.com/ggml-org/llama.cpp/pull/3841
  ffi.Pointer<llama_sampler> llama_sampler_init_min_p(double p, int min_keep) {
    return _llama_sampler_init_min_p(p, min_keep);
  }

  late final _llama_sampler_init_min_pPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<llama_sampler> Function(ffi.Float, ffi.Size)
        >
      >('llama_sampler_init_min_p');
  late final _llama_sampler_init_min_p = _llama_sampler_init_min_pPtr
      .asFunction<ffi.Pointer<llama_sampler> Function(double, int)>();

  /// @details Locally Typical Sampling implementation described in the paper https://arxiv.org/abs/2202.00666.
  ffi.Pointer<llama_sampler> llama_sampler_init_typical(
    double p,
    int min_keep,
  ) {
    return _llama_sampler_init_typical(p, min_keep);
  }

  late final _llama_sampler_init_typicalPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<llama_sampler> Function(ffi.Float, ffi.Size)
        >
      >('llama_sampler_init_typical');
  late final _llama_sampler_init_typical = _llama_sampler_init_typicalPtr
      .asFunction<ffi.Pointer<llama_sampler> Function(double, int)>();

  /// #details Updates the logits l_i` = l_i/t. When t <= 0.0f, the maximum logit is kept at it's original value, the rest are set to -inf
  ffi.Pointer<llama_sampler> llama_sampler_init_temp(double t) {
    return _llama_sampler_init_temp(t);
  }

  late final _llama_sampler_init_tempPtr =
      _lookup<
        ffi.NativeFunction<ffi.Pointer<llama_sampler> Function(ffi.Float)>
      >('llama_sampler_init_temp');
  late final _llama_sampler_init_temp = _llama_sampler_init_tempPtr
      .asFunction<ffi.Pointer<llama_sampler> Function(double)>();

  /// @details Dynamic temperature implementation (a.k.a. entropy) described in the paper https://arxiv.org/abs/2309.02772.
  ffi.Pointer<llama_sampler> llama_sampler_init_temp_ext(
    double t,
    double delta,
    double exponent,
  ) {
    return _llama_sampler_init_temp_ext(t, delta, exponent);
  }

  late final _llama_sampler_init_temp_extPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<llama_sampler> Function(ffi.Float, ffi.Float, ffi.Float)
        >
      >('llama_sampler_init_temp_ext');
  late final _llama_sampler_init_temp_ext = _llama_sampler_init_temp_extPtr
      .asFunction<
        ffi.Pointer<llama_sampler> Function(double, double, double)
      >();

  /// @details XTC sampler as described in https://github.com/oobabooga/text-generation-webui/pull/6335
  ffi.Pointer<llama_sampler> llama_sampler_init_xtc(
    double p,
    double t,
    int min_keep,
    int seed,
  ) {
    return _llama_sampler_init_xtc(p, t, min_keep, seed);
  }

  late final _llama_sampler_init_xtcPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<llama_sampler> Function(
            ffi.Float,
            ffi.Float,
            ffi.Size,
            ffi.Uint32,
          )
        >
      >('llama_sampler_init_xtc');
  late final _llama_sampler_init_xtc = _llama_sampler_init_xtcPtr
      .asFunction<
        ffi.Pointer<llama_sampler> Function(double, double, int, int)
      >();

  /// @details Top n sigma sampling as described in academic paper "Top-n: Not All Logits Are You Need" https://arxiv.org/pdf/2411.07641
  ffi.Pointer<llama_sampler> llama_sampler_init_top_n_sigma(double n) {
    return _llama_sampler_init_top_n_sigma(n);
  }

  late final _llama_sampler_init_top_n_sigmaPtr =
      _lookup<
        ffi.NativeFunction<ffi.Pointer<llama_sampler> Function(ffi.Float)>
      >('llama_sampler_init_top_n_sigma');
  late final _llama_sampler_init_top_n_sigma =
      _llama_sampler_init_top_n_sigmaPtr
          .asFunction<ffi.Pointer<llama_sampler> Function(double)>();

  /// @details Mirostat 1.0 algorithm described in the paper https://arxiv.org/abs/2007.14966. Uses tokens instead of words.
  /// @param candidates A vector of `llama_token_data` containing the candidate tokens, their probabilities (p), and log-odds (logit) for the current position in the generated text.
  /// @param tau  The target cross-entropy (or surprise) value you want to achieve for the generated text. A higher value corresponds to more surprising or less predictable text, while a lower value corresponds to less surprising or more predictable text.
  /// @param eta The learning rate used to update `mu` based on the error between the target and observed surprisal of the sampled word. A larger learning rate will cause `mu` to be updated more quickly, while a smaller learning rate will result in slower updates.
  /// @param m The number of tokens considered in the estimation of `s_hat`. This is an arbitrary value that is used to calculate `s_hat`, which in turn helps to calculate the value of `k`. In the paper, they use `m = 100`, but you can experiment with different values to see how it affects the performance of the algorithm.
  /// @param mu Maximum cross-entropy. This value is initialized to be twice the target cross-entropy (`2 * tau`) and is updated in the algorithm based on the error between the target and observed surprisal.
  ffi.Pointer<llama_sampler> llama_sampler_init_mirostat(
    int n_vocab,
    int seed,
    double tau,
    double eta,
    int m,
  ) {
    return _llama_sampler_init_mirostat(n_vocab, seed, tau, eta, m);
  }

  late final _llama_sampler_init_mirostatPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<llama_sampler> Function(
            ffi.Int32,
            ffi.Uint32,
            ffi.Float,
            ffi.Float,
            ffi.Int32,
          )
        >
      >('llama_sampler_init_mirostat');
  late final _llama_sampler_init_mirostat = _llama_sampler_init_mirostatPtr
      .asFunction<
        ffi.Pointer<llama_sampler> Function(int, int, double, double, int)
      >();

  /// @details Mirostat 2.0 algorithm described in the paper https://arxiv.org/abs/2007.14966. Uses tokens instead of words.
  /// @param candidates A vector of `llama_token_data` containing the candidate tokens, their probabilities (p), and log-odds (logit) for the current position in the generated text.
  /// @param tau  The target cross-entropy (or surprise) value you want to achieve for the generated text. A higher value corresponds to more surprising or less predictable text, while a lower value corresponds to less surprising or more predictable text.
  /// @param eta The learning rate used to update `mu` based on the error between the target and observed surprisal of the sampled word. A larger learning rate will cause `mu` to be updated more quickly, while a smaller learning rate will result in slower updates.
  /// @param mu Maximum cross-entropy. This value is initialized to be twice the target cross-entropy (`2 * tau`) and is updated in the algorithm based on the error between the target and observed surprisal.
  ffi.Pointer<llama_sampler> llama_sampler_init_mirostat_v2(
    int seed,
    double tau,
    double eta,
  ) {
    return _llama_sampler_init_mirostat_v2(seed, tau, eta);
  }

  late final _llama_sampler_init_mirostat_v2Ptr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<llama_sampler> Function(ffi.Uint32, ffi.Float, ffi.Float)
        >
      >('llama_sampler_init_mirostat_v2');
  late final _llama_sampler_init_mirostat_v2 =
      _llama_sampler_init_mirostat_v2Ptr
          .asFunction<
            ffi.Pointer<llama_sampler> Function(int, double, double)
          >();

  /// @details Intializes a GBNF grammar, see grammars/README.md for details.
  /// @param vocab The vocabulary that this grammar will be used with.
  /// @param grammar_str The production rules for the grammar, encoded as a string. Returns an empty grammar if empty. Returns NULL if parsing of grammar_str fails.
  /// @param grammar_root The name of the start symbol for the grammar.
  ffi.Pointer<llama_sampler> llama_sampler_init_grammar(
    ffi.Pointer<llama_vocab> vocab,
    ffi.Pointer<ffi.Char> grammar_str,
    ffi.Pointer<ffi.Char> grammar_root,
  ) {
    return _llama_sampler_init_grammar(vocab, grammar_str, grammar_root);
  }

  late final _llama_sampler_init_grammarPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<llama_sampler> Function(
            ffi.Pointer<llama_vocab>,
            ffi.Pointer<ffi.Char>,
            ffi.Pointer<ffi.Char>,
          )
        >
      >('llama_sampler_init_grammar');
  late final _llama_sampler_init_grammar = _llama_sampler_init_grammarPtr
      .asFunction<
        ffi.Pointer<llama_sampler> Function(
          ffi.Pointer<llama_vocab>,
          ffi.Pointer<ffi.Char>,
          ffi.Pointer<ffi.Char>,
        )
      >();

  ffi.Pointer<llama_sampler> llama_sampler_init_grammar_lazy(
    ffi.Pointer<llama_vocab> vocab,
    ffi.Pointer<ffi.Char> grammar_str,
    ffi.Pointer<ffi.Char> grammar_root,
    ffi.Pointer<ffi.Pointer<ffi.Char>> trigger_words,
    int num_trigger_words,
    ffi.Pointer<llama_token> trigger_tokens,
    int num_trigger_tokens,
  ) {
    return _llama_sampler_init_grammar_lazy(
      vocab,
      grammar_str,
      grammar_root,
      trigger_words,
      num_trigger_words,
      trigger_tokens,
      num_trigger_tokens,
    );
  }

  late final _llama_sampler_init_grammar_lazyPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<llama_sampler> Function(
            ffi.Pointer<llama_vocab>,
            ffi.Pointer<ffi.Char>,
            ffi.Pointer<ffi.Char>,
            ffi.Pointer<ffi.Pointer<ffi.Char>>,
            ffi.Size,
            ffi.Pointer<llama_token>,
            ffi.Size,
          )
        >
      >('llama_sampler_init_grammar_lazy');
  late final _llama_sampler_init_grammar_lazy =
      _llama_sampler_init_grammar_lazyPtr
          .asFunction<
            ffi.Pointer<llama_sampler> Function(
              ffi.Pointer<llama_vocab>,
              ffi.Pointer<ffi.Char>,
              ffi.Pointer<ffi.Char>,
              ffi.Pointer<ffi.Pointer<ffi.Char>>,
              int,
              ffi.Pointer<llama_token>,
              int,
            )
          >();

  /// @details Lazy grammar sampler, introduced in https://github.com/ggml-org/llama.cpp/pull/9639
  /// @param trigger_patterns A list of patterns that will trigger the grammar sampler. Pattern will be matched from the start of the generation output, and grammar sampler will be fed content starting from its first match group.
  /// @param trigger_tokens A list of tokens that will trigger the grammar sampler. Grammar sampler will be fed content starting from the trigger token included.
  ffi.Pointer<llama_sampler> llama_sampler_init_grammar_lazy_patterns(
    ffi.Pointer<llama_vocab> vocab,
    ffi.Pointer<ffi.Char> grammar_str,
    ffi.Pointer<ffi.Char> grammar_root,
    ffi.Pointer<ffi.Pointer<ffi.Char>> trigger_patterns,
    int num_trigger_patterns,
    ffi.Pointer<llama_token> trigger_tokens,
    int num_trigger_tokens,
  ) {
    return _llama_sampler_init_grammar_lazy_patterns(
      vocab,
      grammar_str,
      grammar_root,
      trigger_patterns,
      num_trigger_patterns,
      trigger_tokens,
      num_trigger_tokens,
    );
  }

  late final _llama_sampler_init_grammar_lazy_patternsPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<llama_sampler> Function(
            ffi.Pointer<llama_vocab>,
            ffi.Pointer<ffi.Char>,
            ffi.Pointer<ffi.Char>,
            ffi.Pointer<ffi.Pointer<ffi.Char>>,
            ffi.Size,
            ffi.Pointer<llama_token>,
            ffi.Size,
          )
        >
      >('llama_sampler_init_grammar_lazy_patterns');
  late final _llama_sampler_init_grammar_lazy_patterns =
      _llama_sampler_init_grammar_lazy_patternsPtr
          .asFunction<
            ffi.Pointer<llama_sampler> Function(
              ffi.Pointer<llama_vocab>,
              ffi.Pointer<ffi.Char>,
              ffi.Pointer<ffi.Char>,
              ffi.Pointer<ffi.Pointer<ffi.Char>>,
              int,
              ffi.Pointer<llama_token>,
              int,
            )
          >();

  /// NOTE: Avoid using on the full vocabulary as searching for repeated tokens can become slow. For example, apply top-k or top-p sampling first.
  ffi.Pointer<llama_sampler> llama_sampler_init_penalties(
    int penalty_last_n,
    double penalty_repeat,
    double penalty_freq,
    double penalty_present,
  ) {
    return _llama_sampler_init_penalties(
      penalty_last_n,
      penalty_repeat,
      penalty_freq,
      penalty_present,
    );
  }

  late final _llama_sampler_init_penaltiesPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<llama_sampler> Function(
            ffi.Int32,
            ffi.Float,
            ffi.Float,
            ffi.Float,
          )
        >
      >('llama_sampler_init_penalties');
  late final _llama_sampler_init_penalties = _llama_sampler_init_penaltiesPtr
      .asFunction<
        ffi.Pointer<llama_sampler> Function(int, double, double, double)
      >();

  /// @details DRY sampler, designed by p-e-w, as described in: https://github.com/oobabooga/text-generation-webui/pull/5677, porting Koboldcpp implementation authored by pi6am: https://github.com/LostRuins/koboldcpp/pull/982
  ffi.Pointer<llama_sampler> llama_sampler_init_dry(
    ffi.Pointer<llama_vocab> vocab,
    int n_ctx_train,
    double dry_multiplier,
    double dry_base,
    int dry_allowed_length,
    int dry_penalty_last_n,
    ffi.Pointer<ffi.Pointer<ffi.Char>> seq_breakers,
    int num_breakers,
  ) {
    return _llama_sampler_init_dry(
      vocab,
      n_ctx_train,
      dry_multiplier,
      dry_base,
      dry_allowed_length,
      dry_penalty_last_n,
      seq_breakers,
      num_breakers,
    );
  }

  late final _llama_sampler_init_dryPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<llama_sampler> Function(
            ffi.Pointer<llama_vocab>,
            ffi.Int32,
            ffi.Float,
            ffi.Float,
            ffi.Int32,
            ffi.Int32,
            ffi.Pointer<ffi.Pointer<ffi.Char>>,
            ffi.Size,
          )
        >
      >('llama_sampler_init_dry');
  late final _llama_sampler_init_dry = _llama_sampler_init_dryPtr
      .asFunction<
        ffi.Pointer<llama_sampler> Function(
          ffi.Pointer<llama_vocab>,
          int,
          double,
          double,
          int,
          int,
          ffi.Pointer<ffi.Pointer<ffi.Char>>,
          int,
        )
      >();

  ffi.Pointer<llama_sampler> llama_sampler_init_logit_bias(
    int n_vocab,
    int n_logit_bias,
    ffi.Pointer<llama_logit_bias> logit_bias,
  ) {
    return _llama_sampler_init_logit_bias(n_vocab, n_logit_bias, logit_bias);
  }

  late final _llama_sampler_init_logit_biasPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<llama_sampler> Function(
            ffi.Int32,
            ffi.Int32,
            ffi.Pointer<llama_logit_bias>,
          )
        >
      >('llama_sampler_init_logit_bias');
  late final _llama_sampler_init_logit_bias = _llama_sampler_init_logit_biasPtr
      .asFunction<
        ffi.Pointer<llama_sampler> Function(
          int,
          int,
          ffi.Pointer<llama_logit_bias>,
        )
      >();

  /// this sampler is meant to be used for fill-in-the-middle infilling
  /// it's supposed to be used after top_k + top_p sampling
  ///
  /// 1. if the sum of the EOG probs times the number of candidates is higher than the sum of the other probs -> pick EOG
  /// 2. combine probs of tokens that have the same prefix
  ///
  /// example:
  ///
  /// - before:
  /// "hel":   0.5
  /// "hell":  0.2
  /// "hello": 0.1
  /// "dummy": 0.1
  ///
  /// - after:
  /// "hel":   0.8
  /// "dummy": 0.1
  ///
  /// 3. discard non-EOG tokens with low prob
  /// 4. if no tokens are left -> pick EOT
  ffi.Pointer<llama_sampler> llama_sampler_init_infill(
    ffi.Pointer<llama_vocab> vocab,
  ) {
    return _llama_sampler_init_infill(vocab);
  }

  late final _llama_sampler_init_infillPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<llama_sampler> Function(ffi.Pointer<llama_vocab>)
        >
      >('llama_sampler_init_infill');
  late final _llama_sampler_init_infill = _llama_sampler_init_infillPtr
      .asFunction<
        ffi.Pointer<llama_sampler> Function(ffi.Pointer<llama_vocab>)
      >();

  /// Returns the seed used by the sampler if applicable, LLAMA_DEFAULT_SEED otherwise
  int llama_sampler_get_seed(ffi.Pointer<llama_sampler> smpl) {
    return _llama_sampler_get_seed(smpl);
  }

  late final _llama_sampler_get_seedPtr =
      _lookup<
        ffi.NativeFunction<ffi.Uint32 Function(ffi.Pointer<llama_sampler>)>
      >('llama_sampler_get_seed');
  late final _llama_sampler_get_seed = _llama_sampler_get_seedPtr
      .asFunction<int Function(ffi.Pointer<llama_sampler>)>();

  /// @details Sample and accept a token from the idx-th output of the last evaluation
  ///
  /// Shorthand for:
  /// const auto * logits = llama_get_logits_ith(ctx, idx);
  /// llama_token_data_array cur_p = { ... init from logits ... };
  /// llama_sampler_apply(smpl, &cur_p);
  /// auto token = cur_p.data[cur_p.selected].id;
  /// llama_sampler_accept(smpl, token);
  /// return token;
  /// Returns the sampled token
  int llama_sampler_sample(
    ffi.Pointer<llama_sampler> smpl,
    ffi.Pointer<llama_context> ctx,
    int idx,
  ) {
    return _llama_sampler_sample(smpl, ctx, idx);
  }

  late final _llama_sampler_samplePtr =
      _lookup<
        ffi.NativeFunction<
          llama_token Function(
            ffi.Pointer<llama_sampler>,
            ffi.Pointer<llama_context>,
            ffi.Int32,
          )
        >
      >('llama_sampler_sample');
  late final _llama_sampler_sample = _llama_sampler_samplePtr
      .asFunction<
        int Function(
          ffi.Pointer<llama_sampler>,
          ffi.Pointer<llama_context>,
          int,
        )
      >();

  /// @details Build a split GGUF final path for this chunk.
  /// llama_split_path(split_path, sizeof(split_path), "/models/ggml-model-q4_0", 2, 4) => split_path = "/models/ggml-model-q4_0-00002-of-00004.gguf"
  /// Returns the split_path length.
  int llama_split_path(
    ffi.Pointer<ffi.Char> split_path,
    int maxlen,
    ffi.Pointer<ffi.Char> path_prefix,
    int split_no,
    int split_count,
  ) {
    return _llama_split_path(
      split_path,
      maxlen,
      path_prefix,
      split_no,
      split_count,
    );
  }

  late final _llama_split_pathPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Int Function(
            ffi.Pointer<ffi.Char>,
            ffi.Size,
            ffi.Pointer<ffi.Char>,
            ffi.Int,
            ffi.Int,
          )
        >
      >('llama_split_path');
  late final _llama_split_path = _llama_split_pathPtr
      .asFunction<
        int Function(
          ffi.Pointer<ffi.Char>,
          int,
          ffi.Pointer<ffi.Char>,
          int,
          int,
        )
      >();

  /// @details Extract the path prefix from the split_path if and only if the split_no and split_count match.
  /// llama_split_prefix(split_prefix, 64, "/models/ggml-model-q4_0-00002-of-00004.gguf", 2, 4) => split_prefix = "/models/ggml-model-q4_0"
  /// Returns the split_prefix length.
  int llama_split_prefix(
    ffi.Pointer<ffi.Char> split_prefix,
    int maxlen,
    ffi.Pointer<ffi.Char> split_path,
    int split_no,
    int split_count,
  ) {
    return _llama_split_prefix(
      split_prefix,
      maxlen,
      split_path,
      split_no,
      split_count,
    );
  }

  late final _llama_split_prefixPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Int Function(
            ffi.Pointer<ffi.Char>,
            ffi.Size,
            ffi.Pointer<ffi.Char>,
            ffi.Int,
            ffi.Int,
          )
        >
      >('llama_split_prefix');
  late final _llama_split_prefix = _llama_split_prefixPtr
      .asFunction<
        int Function(
          ffi.Pointer<ffi.Char>,
          int,
          ffi.Pointer<ffi.Char>,
          int,
          int,
        )
      >();

  /// Print system information
  ffi.Pointer<ffi.Char> llama_print_system_info() {
    return _llama_print_system_info();
  }

  late final _llama_print_system_infoPtr =
      _lookup<ffi.NativeFunction<ffi.Pointer<ffi.Char> Function()>>(
        'llama_print_system_info',
      );
  late final _llama_print_system_info = _llama_print_system_infoPtr
      .asFunction<ffi.Pointer<ffi.Char> Function()>();

  /// Set callback for all future logging events.
  /// If this is not called, or NULL is supplied, everything is output on stderr.
  void llama_log_set(
    ggml_log_callback log_callback,
    ffi.Pointer<ffi.Void> user_data,
  ) {
    return _llama_log_set(log_callback, user_data);
  }

  late final _llama_log_setPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Void Function(ggml_log_callback, ffi.Pointer<ffi.Void>)
        >
      >('llama_log_set');
  late final _llama_log_set = _llama_log_setPtr
      .asFunction<void Function(ggml_log_callback, ffi.Pointer<ffi.Void>)>();

  llama_perf_context_data llama_perf_context(ffi.Pointer<llama_context> ctx) {
    return _llama_perf_context(ctx);
  }

  late final _llama_perf_contextPtr =
      _lookup<
        ffi.NativeFunction<
          llama_perf_context_data Function(ffi.Pointer<llama_context>)
        >
      >('llama_perf_context');
  late final _llama_perf_context = _llama_perf_contextPtr
      .asFunction<
        llama_perf_context_data Function(ffi.Pointer<llama_context>)
      >();

  void llama_perf_context_print(ffi.Pointer<llama_context> ctx) {
    return _llama_perf_context_print(ctx);
  }

  late final _llama_perf_context_printPtr =
      _lookup<
        ffi.NativeFunction<ffi.Void Function(ffi.Pointer<llama_context>)>
      >('llama_perf_context_print');
  late final _llama_perf_context_print = _llama_perf_context_printPtr
      .asFunction<void Function(ffi.Pointer<llama_context>)>();

  void llama_perf_context_reset(ffi.Pointer<llama_context> ctx) {
    return _llama_perf_context_reset(ctx);
  }

  late final _llama_perf_context_resetPtr =
      _lookup<
        ffi.NativeFunction<ffi.Void Function(ffi.Pointer<llama_context>)>
      >('llama_perf_context_reset');
  late final _llama_perf_context_reset = _llama_perf_context_resetPtr
      .asFunction<void Function(ffi.Pointer<llama_context>)>();

  /// NOTE: the following work only with samplers constructed via llama_sampler_chain_init
  llama_perf_sampler_data llama_perf_sampler(ffi.Pointer<llama_sampler> chain) {
    return _llama_perf_sampler(chain);
  }

  late final _llama_perf_samplerPtr =
      _lookup<
        ffi.NativeFunction<
          llama_perf_sampler_data Function(ffi.Pointer<llama_sampler>)
        >
      >('llama_perf_sampler');
  late final _llama_perf_sampler = _llama_perf_samplerPtr
      .asFunction<
        llama_perf_sampler_data Function(ffi.Pointer<llama_sampler>)
      >();

  void llama_perf_sampler_print(ffi.Pointer<llama_sampler> chain) {
    return _llama_perf_sampler_print(chain);
  }

  late final _llama_perf_sampler_printPtr =
      _lookup<
        ffi.NativeFunction<ffi.Void Function(ffi.Pointer<llama_sampler>)>
      >('llama_perf_sampler_print');
  late final _llama_perf_sampler_print = _llama_perf_sampler_printPtr
      .asFunction<void Function(ffi.Pointer<llama_sampler>)>();

  void llama_perf_sampler_reset(ffi.Pointer<llama_sampler> chain) {
    return _llama_perf_sampler_reset(chain);
  }

  late final _llama_perf_sampler_resetPtr =
      _lookup<
        ffi.NativeFunction<ffi.Void Function(ffi.Pointer<llama_sampler>)>
      >('llama_perf_sampler_reset');
  late final _llama_perf_sampler_reset = _llama_perf_sampler_resetPtr
      .asFunction<void Function(ffi.Pointer<llama_sampler>)>();

  /// print a breakdown of per-device memory use via LLAMA_LOG:
  void llama_memory_breakdown_print(ffi.Pointer<llama_context> ctx) {
    return _llama_memory_breakdown_print(ctx);
  }

  late final _llama_memory_breakdown_printPtr =
      _lookup<
        ffi.NativeFunction<ffi.Void Function(ffi.Pointer<llama_context>)>
      >('llama_memory_breakdown_print');
  late final _llama_memory_breakdown_print = _llama_memory_breakdown_printPtr
      .asFunction<void Function(ffi.Pointer<llama_context>)>();

  /// always returns true
  bool llama_opt_param_filter_all(
    ffi.Pointer<ggml_tensor> tensor,
    ffi.Pointer<ffi.Void> userdata,
  ) {
    return _llama_opt_param_filter_all(tensor, userdata);
  }

  late final _llama_opt_param_filter_allPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Bool Function(ffi.Pointer<ggml_tensor>, ffi.Pointer<ffi.Void>)
        >
      >('llama_opt_param_filter_all');
  late final _llama_opt_param_filter_all = _llama_opt_param_filter_allPtr
      .asFunction<
        bool Function(ffi.Pointer<ggml_tensor>, ffi.Pointer<ffi.Void>)
      >();

  void llama_opt_init(
    ffi.Pointer<llama_context> lctx,
    ffi.Pointer<llama_model> model,
    llama_opt_params lopt_params,
  ) {
    return _llama_opt_init(lctx, model, lopt_params);
  }

  late final _llama_opt_initPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Void Function(
            ffi.Pointer<llama_context>,
            ffi.Pointer<llama_model>,
            llama_opt_params,
          )
        >
      >('llama_opt_init');
  late final _llama_opt_init = _llama_opt_initPtr
      .asFunction<
        void Function(
          ffi.Pointer<llama_context>,
          ffi.Pointer<llama_model>,
          llama_opt_params,
        )
      >();

  void llama_opt_epoch(
    ffi.Pointer<llama_context> lctx,
    ggml_opt_dataset_t dataset,
    ggml_opt_result_t result_train,
    ggml_opt_result_t result_eval,
    int idata_split,
    ggml_opt_epoch_callback callback_train,
    ggml_opt_epoch_callback callback_eval,
  ) {
    return _llama_opt_epoch(
      lctx,
      dataset,
      result_train,
      result_eval,
      idata_split,
      callback_train,
      callback_eval,
    );
  }

  late final _llama_opt_epochPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Void Function(
            ffi.Pointer<llama_context>,
            ggml_opt_dataset_t,
            ggml_opt_result_t,
            ggml_opt_result_t,
            ffi.Int64,
            ggml_opt_epoch_callback,
            ggml_opt_epoch_callback,
          )
        >
      >('llama_opt_epoch');
  late final _llama_opt_epoch = _llama_opt_epochPtr
      .asFunction<
        void Function(
          ffi.Pointer<llama_context>,
          ggml_opt_dataset_t,
          ggml_opt_result_t,
          ggml_opt_result_t,
          int,
          ggml_opt_epoch_callback,
          ggml_opt_epoch_callback,
        )
      >();

  ffi.Pointer<gguf_context> gguf_init_empty() {
    return _gguf_init_empty();
  }

  late final _gguf_init_emptyPtr =
      _lookup<ffi.NativeFunction<ffi.Pointer<gguf_context> Function()>>(
        'gguf_init_empty',
      );
  late final _gguf_init_empty = _gguf_init_emptyPtr
      .asFunction<ffi.Pointer<gguf_context> Function()>();

  ffi.Pointer<gguf_context> gguf_init_from_file(
    ffi.Pointer<ffi.Char> fname,
    gguf_init_params params,
  ) {
    return _gguf_init_from_file(fname, params);
  }

  late final _gguf_init_from_filePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<gguf_context> Function(
            ffi.Pointer<ffi.Char>,
            gguf_init_params,
          )
        >
      >('gguf_init_from_file');
  late final _gguf_init_from_file = _gguf_init_from_filePtr
      .asFunction<
        ffi.Pointer<gguf_context> Function(
          ffi.Pointer<ffi.Char>,
          gguf_init_params,
        )
      >();

  /// GGML_API struct gguf_context * gguf_init_from_buffer(..);
  void gguf_free(ffi.Pointer<gguf_context> ctx) {
    return _gguf_free(ctx);
  }

  late final _gguf_freePtr =
      _lookup<ffi.NativeFunction<ffi.Void Function(ffi.Pointer<gguf_context>)>>(
        'gguf_free',
      );
  late final _gguf_free = _gguf_freePtr
      .asFunction<void Function(ffi.Pointer<gguf_context>)>();

  ffi.Pointer<ffi.Char> gguf_type_name(gguf_type type) {
    return _gguf_type_name(type.value);
  }

  late final _gguf_type_namePtr =
      _lookup<
        ffi.NativeFunction<ffi.Pointer<ffi.Char> Function(ffi.UnsignedInt)>
      >('gguf_type_name');
  late final _gguf_type_name = _gguf_type_namePtr
      .asFunction<ffi.Pointer<ffi.Char> Function(int)>();

  int gguf_get_version(ffi.Pointer<gguf_context> ctx) {
    return _gguf_get_version(ctx);
  }

  late final _gguf_get_versionPtr =
      _lookup<
        ffi.NativeFunction<ffi.Uint32 Function(ffi.Pointer<gguf_context>)>
      >('gguf_get_version');
  late final _gguf_get_version = _gguf_get_versionPtr
      .asFunction<int Function(ffi.Pointer<gguf_context>)>();

  int gguf_get_alignment(ffi.Pointer<gguf_context> ctx) {
    return _gguf_get_alignment(ctx);
  }

  late final _gguf_get_alignmentPtr =
      _lookup<ffi.NativeFunction<ffi.Size Function(ffi.Pointer<gguf_context>)>>(
        'gguf_get_alignment',
      );
  late final _gguf_get_alignment = _gguf_get_alignmentPtr
      .asFunction<int Function(ffi.Pointer<gguf_context>)>();

  int gguf_get_data_offset(ffi.Pointer<gguf_context> ctx) {
    return _gguf_get_data_offset(ctx);
  }

  late final _gguf_get_data_offsetPtr =
      _lookup<ffi.NativeFunction<ffi.Size Function(ffi.Pointer<gguf_context>)>>(
        'gguf_get_data_offset',
      );
  late final _gguf_get_data_offset = _gguf_get_data_offsetPtr
      .asFunction<int Function(ffi.Pointer<gguf_context>)>();

  int gguf_get_n_kv(ffi.Pointer<gguf_context> ctx) {
    return _gguf_get_n_kv(ctx);
  }

  late final _gguf_get_n_kvPtr =
      _lookup<
        ffi.NativeFunction<ffi.Int64 Function(ffi.Pointer<gguf_context>)>
      >('gguf_get_n_kv');
  late final _gguf_get_n_kv = _gguf_get_n_kvPtr
      .asFunction<int Function(ffi.Pointer<gguf_context>)>();

  int gguf_find_key(ffi.Pointer<gguf_context> ctx, ffi.Pointer<ffi.Char> key) {
    return _gguf_find_key(ctx, key);
  }

  late final _gguf_find_keyPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Int64 Function(ffi.Pointer<gguf_context>, ffi.Pointer<ffi.Char>)
        >
      >('gguf_find_key');
  late final _gguf_find_key = _gguf_find_keyPtr
      .asFunction<
        int Function(ffi.Pointer<gguf_context>, ffi.Pointer<ffi.Char>)
      >();

  ffi.Pointer<ffi.Char> gguf_get_key(
    ffi.Pointer<gguf_context> ctx,
    int key_id,
  ) {
    return _gguf_get_key(ctx, key_id);
  }

  late final _gguf_get_keyPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ffi.Char> Function(ffi.Pointer<gguf_context>, ffi.Int64)
        >
      >('gguf_get_key');
  late final _gguf_get_key = _gguf_get_keyPtr
      .asFunction<
        ffi.Pointer<ffi.Char> Function(ffi.Pointer<gguf_context>, int)
      >();

  gguf_type gguf_get_kv_type(ffi.Pointer<gguf_context> ctx, int key_id) {
    return gguf_type.fromValue(_gguf_get_kv_type(ctx, key_id));
  }

  late final _gguf_get_kv_typePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.UnsignedInt Function(ffi.Pointer<gguf_context>, ffi.Int64)
        >
      >('gguf_get_kv_type');
  late final _gguf_get_kv_type = _gguf_get_kv_typePtr
      .asFunction<int Function(ffi.Pointer<gguf_context>, int)>();

  gguf_type gguf_get_arr_type(ffi.Pointer<gguf_context> ctx, int key_id) {
    return gguf_type.fromValue(_gguf_get_arr_type(ctx, key_id));
  }

  late final _gguf_get_arr_typePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.UnsignedInt Function(ffi.Pointer<gguf_context>, ffi.Int64)
        >
      >('gguf_get_arr_type');
  late final _gguf_get_arr_type = _gguf_get_arr_typePtr
      .asFunction<int Function(ffi.Pointer<gguf_context>, int)>();

  /// will abort if the wrong type is used for the key
  int gguf_get_val_u8(ffi.Pointer<gguf_context> ctx, int key_id) {
    return _gguf_get_val_u8(ctx, key_id);
  }

  late final _gguf_get_val_u8Ptr =
      _lookup<
        ffi.NativeFunction<
          ffi.Uint8 Function(ffi.Pointer<gguf_context>, ffi.Int64)
        >
      >('gguf_get_val_u8');
  late final _gguf_get_val_u8 = _gguf_get_val_u8Ptr
      .asFunction<int Function(ffi.Pointer<gguf_context>, int)>();

  int gguf_get_val_i8(ffi.Pointer<gguf_context> ctx, int key_id) {
    return _gguf_get_val_i8(ctx, key_id);
  }

  late final _gguf_get_val_i8Ptr =
      _lookup<
        ffi.NativeFunction<
          ffi.Int8 Function(ffi.Pointer<gguf_context>, ffi.Int64)
        >
      >('gguf_get_val_i8');
  late final _gguf_get_val_i8 = _gguf_get_val_i8Ptr
      .asFunction<int Function(ffi.Pointer<gguf_context>, int)>();

  int gguf_get_val_u16(ffi.Pointer<gguf_context> ctx, int key_id) {
    return _gguf_get_val_u16(ctx, key_id);
  }

  late final _gguf_get_val_u16Ptr =
      _lookup<
        ffi.NativeFunction<
          ffi.Uint16 Function(ffi.Pointer<gguf_context>, ffi.Int64)
        >
      >('gguf_get_val_u16');
  late final _gguf_get_val_u16 = _gguf_get_val_u16Ptr
      .asFunction<int Function(ffi.Pointer<gguf_context>, int)>();

  int gguf_get_val_i16(ffi.Pointer<gguf_context> ctx, int key_id) {
    return _gguf_get_val_i16(ctx, key_id);
  }

  late final _gguf_get_val_i16Ptr =
      _lookup<
        ffi.NativeFunction<
          ffi.Int16 Function(ffi.Pointer<gguf_context>, ffi.Int64)
        >
      >('gguf_get_val_i16');
  late final _gguf_get_val_i16 = _gguf_get_val_i16Ptr
      .asFunction<int Function(ffi.Pointer<gguf_context>, int)>();

  int gguf_get_val_u32(ffi.Pointer<gguf_context> ctx, int key_id) {
    return _gguf_get_val_u32(ctx, key_id);
  }

  late final _gguf_get_val_u32Ptr =
      _lookup<
        ffi.NativeFunction<
          ffi.Uint32 Function(ffi.Pointer<gguf_context>, ffi.Int64)
        >
      >('gguf_get_val_u32');
  late final _gguf_get_val_u32 = _gguf_get_val_u32Ptr
      .asFunction<int Function(ffi.Pointer<gguf_context>, int)>();

  int gguf_get_val_i32(ffi.Pointer<gguf_context> ctx, int key_id) {
    return _gguf_get_val_i32(ctx, key_id);
  }

  late final _gguf_get_val_i32Ptr =
      _lookup<
        ffi.NativeFunction<
          ffi.Int32 Function(ffi.Pointer<gguf_context>, ffi.Int64)
        >
      >('gguf_get_val_i32');
  late final _gguf_get_val_i32 = _gguf_get_val_i32Ptr
      .asFunction<int Function(ffi.Pointer<gguf_context>, int)>();

  double gguf_get_val_f32(ffi.Pointer<gguf_context> ctx, int key_id) {
    return _gguf_get_val_f32(ctx, key_id);
  }

  late final _gguf_get_val_f32Ptr =
      _lookup<
        ffi.NativeFunction<
          ffi.Float Function(ffi.Pointer<gguf_context>, ffi.Int64)
        >
      >('gguf_get_val_f32');
  late final _gguf_get_val_f32 = _gguf_get_val_f32Ptr
      .asFunction<double Function(ffi.Pointer<gguf_context>, int)>();

  int gguf_get_val_u64(ffi.Pointer<gguf_context> ctx, int key_id) {
    return _gguf_get_val_u64(ctx, key_id);
  }

  late final _gguf_get_val_u64Ptr =
      _lookup<
        ffi.NativeFunction<
          ffi.Uint64 Function(ffi.Pointer<gguf_context>, ffi.Int64)
        >
      >('gguf_get_val_u64');
  late final _gguf_get_val_u64 = _gguf_get_val_u64Ptr
      .asFunction<int Function(ffi.Pointer<gguf_context>, int)>();

  int gguf_get_val_i64(ffi.Pointer<gguf_context> ctx, int key_id) {
    return _gguf_get_val_i64(ctx, key_id);
  }

  late final _gguf_get_val_i64Ptr =
      _lookup<
        ffi.NativeFunction<
          ffi.Int64 Function(ffi.Pointer<gguf_context>, ffi.Int64)
        >
      >('gguf_get_val_i64');
  late final _gguf_get_val_i64 = _gguf_get_val_i64Ptr
      .asFunction<int Function(ffi.Pointer<gguf_context>, int)>();

  double gguf_get_val_f64(ffi.Pointer<gguf_context> ctx, int key_id) {
    return _gguf_get_val_f64(ctx, key_id);
  }

  late final _gguf_get_val_f64Ptr =
      _lookup<
        ffi.NativeFunction<
          ffi.Double Function(ffi.Pointer<gguf_context>, ffi.Int64)
        >
      >('gguf_get_val_f64');
  late final _gguf_get_val_f64 = _gguf_get_val_f64Ptr
      .asFunction<double Function(ffi.Pointer<gguf_context>, int)>();

  bool gguf_get_val_bool(ffi.Pointer<gguf_context> ctx, int key_id) {
    return _gguf_get_val_bool(ctx, key_id);
  }

  late final _gguf_get_val_boolPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Bool Function(ffi.Pointer<gguf_context>, ffi.Int64)
        >
      >('gguf_get_val_bool');
  late final _gguf_get_val_bool = _gguf_get_val_boolPtr
      .asFunction<bool Function(ffi.Pointer<gguf_context>, int)>();

  ffi.Pointer<ffi.Char> gguf_get_val_str(
    ffi.Pointer<gguf_context> ctx,
    int key_id,
  ) {
    return _gguf_get_val_str(ctx, key_id);
  }

  late final _gguf_get_val_strPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ffi.Char> Function(ffi.Pointer<gguf_context>, ffi.Int64)
        >
      >('gguf_get_val_str');
  late final _gguf_get_val_str = _gguf_get_val_strPtr
      .asFunction<
        ffi.Pointer<ffi.Char> Function(ffi.Pointer<gguf_context>, int)
      >();

  ffi.Pointer<ffi.Void> gguf_get_val_data(
    ffi.Pointer<gguf_context> ctx,
    int key_id,
  ) {
    return _gguf_get_val_data(ctx, key_id);
  }

  late final _gguf_get_val_dataPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ffi.Void> Function(ffi.Pointer<gguf_context>, ffi.Int64)
        >
      >('gguf_get_val_data');
  late final _gguf_get_val_data = _gguf_get_val_dataPtr
      .asFunction<
        ffi.Pointer<ffi.Void> Function(ffi.Pointer<gguf_context>, int)
      >();

  int gguf_get_arr_n(ffi.Pointer<gguf_context> ctx, int key_id) {
    return _gguf_get_arr_n(ctx, key_id);
  }

  late final _gguf_get_arr_nPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Size Function(ffi.Pointer<gguf_context>, ffi.Int64)
        >
      >('gguf_get_arr_n');
  late final _gguf_get_arr_n = _gguf_get_arr_nPtr
      .asFunction<int Function(ffi.Pointer<gguf_context>, int)>();

  /// get raw pointer to the first element of the array with the given key_id
  /// for bool arrays, note that they are always stored as int8 on all platforms (usually this makes no difference)
  ffi.Pointer<ffi.Void> gguf_get_arr_data(
    ffi.Pointer<gguf_context> ctx,
    int key_id,
  ) {
    return _gguf_get_arr_data(ctx, key_id);
  }

  late final _gguf_get_arr_dataPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ffi.Void> Function(ffi.Pointer<gguf_context>, ffi.Int64)
        >
      >('gguf_get_arr_data');
  late final _gguf_get_arr_data = _gguf_get_arr_dataPtr
      .asFunction<
        ffi.Pointer<ffi.Void> Function(ffi.Pointer<gguf_context>, int)
      >();

  /// get ith C string from array with given key_id
  ffi.Pointer<ffi.Char> gguf_get_arr_str(
    ffi.Pointer<gguf_context> ctx,
    int key_id,
    int i,
  ) {
    return _gguf_get_arr_str(ctx, key_id, i);
  }

  late final _gguf_get_arr_strPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ffi.Char> Function(
            ffi.Pointer<gguf_context>,
            ffi.Int64,
            ffi.Size,
          )
        >
      >('gguf_get_arr_str');
  late final _gguf_get_arr_str = _gguf_get_arr_strPtr
      .asFunction<
        ffi.Pointer<ffi.Char> Function(ffi.Pointer<gguf_context>, int, int)
      >();

  int gguf_get_n_tensors(ffi.Pointer<gguf_context> ctx) {
    return _gguf_get_n_tensors(ctx);
  }

  late final _gguf_get_n_tensorsPtr =
      _lookup<
        ffi.NativeFunction<ffi.Int64 Function(ffi.Pointer<gguf_context>)>
      >('gguf_get_n_tensors');
  late final _gguf_get_n_tensors = _gguf_get_n_tensorsPtr
      .asFunction<int Function(ffi.Pointer<gguf_context>)>();

  int gguf_find_tensor(
    ffi.Pointer<gguf_context> ctx,
    ffi.Pointer<ffi.Char> name,
  ) {
    return _gguf_find_tensor(ctx, name);
  }

  late final _gguf_find_tensorPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Int64 Function(ffi.Pointer<gguf_context>, ffi.Pointer<ffi.Char>)
        >
      >('gguf_find_tensor');
  late final _gguf_find_tensor = _gguf_find_tensorPtr
      .asFunction<
        int Function(ffi.Pointer<gguf_context>, ffi.Pointer<ffi.Char>)
      >();

  int gguf_get_tensor_offset(ffi.Pointer<gguf_context> ctx, int tensor_id) {
    return _gguf_get_tensor_offset(ctx, tensor_id);
  }

  late final _gguf_get_tensor_offsetPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Size Function(ffi.Pointer<gguf_context>, ffi.Int64)
        >
      >('gguf_get_tensor_offset');
  late final _gguf_get_tensor_offset = _gguf_get_tensor_offsetPtr
      .asFunction<int Function(ffi.Pointer<gguf_context>, int)>();

  ffi.Pointer<ffi.Char> gguf_get_tensor_name(
    ffi.Pointer<gguf_context> ctx,
    int tensor_id,
  ) {
    return _gguf_get_tensor_name(ctx, tensor_id);
  }

  late final _gguf_get_tensor_namePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ffi.Char> Function(ffi.Pointer<gguf_context>, ffi.Int64)
        >
      >('gguf_get_tensor_name');
  late final _gguf_get_tensor_name = _gguf_get_tensor_namePtr
      .asFunction<
        ffi.Pointer<ffi.Char> Function(ffi.Pointer<gguf_context>, int)
      >();

  ggml_type gguf_get_tensor_type(ffi.Pointer<gguf_context> ctx, int tensor_id) {
    return ggml_type.fromValue(_gguf_get_tensor_type(ctx, tensor_id));
  }

  late final _gguf_get_tensor_typePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.UnsignedInt Function(ffi.Pointer<gguf_context>, ffi.Int64)
        >
      >('gguf_get_tensor_type');
  late final _gguf_get_tensor_type = _gguf_get_tensor_typePtr
      .asFunction<int Function(ffi.Pointer<gguf_context>, int)>();

  int gguf_get_tensor_size(ffi.Pointer<gguf_context> ctx, int tensor_id) {
    return _gguf_get_tensor_size(ctx, tensor_id);
  }

  late final _gguf_get_tensor_sizePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Size Function(ffi.Pointer<gguf_context>, ffi.Int64)
        >
      >('gguf_get_tensor_size');
  late final _gguf_get_tensor_size = _gguf_get_tensor_sizePtr
      .asFunction<int Function(ffi.Pointer<gguf_context>, int)>();

  /// removes key if it exists, returns id that the key had prior to removal (-1 if it didn't exist)
  int gguf_remove_key(
    ffi.Pointer<gguf_context> ctx,
    ffi.Pointer<ffi.Char> key,
  ) {
    return _gguf_remove_key(ctx, key);
  }

  late final _gguf_remove_keyPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Int64 Function(ffi.Pointer<gguf_context>, ffi.Pointer<ffi.Char>)
        >
      >('gguf_remove_key');
  late final _gguf_remove_key = _gguf_remove_keyPtr
      .asFunction<
        int Function(ffi.Pointer<gguf_context>, ffi.Pointer<ffi.Char>)
      >();

  /// overrides an existing KV pair or adds a new one, the new KV pair is always at the back
  void gguf_set_val_u8(
    ffi.Pointer<gguf_context> ctx,
    ffi.Pointer<ffi.Char> key,
    int val,
  ) {
    return _gguf_set_val_u8(ctx, key, val);
  }

  late final _gguf_set_val_u8Ptr =
      _lookup<
        ffi.NativeFunction<
          ffi.Void Function(
            ffi.Pointer<gguf_context>,
            ffi.Pointer<ffi.Char>,
            ffi.Uint8,
          )
        >
      >('gguf_set_val_u8');
  late final _gguf_set_val_u8 = _gguf_set_val_u8Ptr
      .asFunction<
        void Function(ffi.Pointer<gguf_context>, ffi.Pointer<ffi.Char>, int)
      >();

  void gguf_set_val_i8(
    ffi.Pointer<gguf_context> ctx,
    ffi.Pointer<ffi.Char> key,
    int val,
  ) {
    return _gguf_set_val_i8(ctx, key, val);
  }

  late final _gguf_set_val_i8Ptr =
      _lookup<
        ffi.NativeFunction<
          ffi.Void Function(
            ffi.Pointer<gguf_context>,
            ffi.Pointer<ffi.Char>,
            ffi.Int8,
          )
        >
      >('gguf_set_val_i8');
  late final _gguf_set_val_i8 = _gguf_set_val_i8Ptr
      .asFunction<
        void Function(ffi.Pointer<gguf_context>, ffi.Pointer<ffi.Char>, int)
      >();

  void gguf_set_val_u16(
    ffi.Pointer<gguf_context> ctx,
    ffi.Pointer<ffi.Char> key,
    int val,
  ) {
    return _gguf_set_val_u16(ctx, key, val);
  }

  late final _gguf_set_val_u16Ptr =
      _lookup<
        ffi.NativeFunction<
          ffi.Void Function(
            ffi.Pointer<gguf_context>,
            ffi.Pointer<ffi.Char>,
            ffi.Uint16,
          )
        >
      >('gguf_set_val_u16');
  late final _gguf_set_val_u16 = _gguf_set_val_u16Ptr
      .asFunction<
        void Function(ffi.Pointer<gguf_context>, ffi.Pointer<ffi.Char>, int)
      >();

  void gguf_set_val_i16(
    ffi.Pointer<gguf_context> ctx,
    ffi.Pointer<ffi.Char> key,
    int val,
  ) {
    return _gguf_set_val_i16(ctx, key, val);
  }

  late final _gguf_set_val_i16Ptr =
      _lookup<
        ffi.NativeFunction<
          ffi.Void Function(
            ffi.Pointer<gguf_context>,
            ffi.Pointer<ffi.Char>,
            ffi.Int16,
          )
        >
      >('gguf_set_val_i16');
  late final _gguf_set_val_i16 = _gguf_set_val_i16Ptr
      .asFunction<
        void Function(ffi.Pointer<gguf_context>, ffi.Pointer<ffi.Char>, int)
      >();

  void gguf_set_val_u32(
    ffi.Pointer<gguf_context> ctx,
    ffi.Pointer<ffi.Char> key,
    int val,
  ) {
    return _gguf_set_val_u32(ctx, key, val);
  }

  late final _gguf_set_val_u32Ptr =
      _lookup<
        ffi.NativeFunction<
          ffi.Void Function(
            ffi.Pointer<gguf_context>,
            ffi.Pointer<ffi.Char>,
            ffi.Uint32,
          )
        >
      >('gguf_set_val_u32');
  late final _gguf_set_val_u32 = _gguf_set_val_u32Ptr
      .asFunction<
        void Function(ffi.Pointer<gguf_context>, ffi.Pointer<ffi.Char>, int)
      >();

  void gguf_set_val_i32(
    ffi.Pointer<gguf_context> ctx,
    ffi.Pointer<ffi.Char> key,
    int val,
  ) {
    return _gguf_set_val_i32(ctx, key, val);
  }

  late final _gguf_set_val_i32Ptr =
      _lookup<
        ffi.NativeFunction<
          ffi.Void Function(
            ffi.Pointer<gguf_context>,
            ffi.Pointer<ffi.Char>,
            ffi.Int32,
          )
        >
      >('gguf_set_val_i32');
  late final _gguf_set_val_i32 = _gguf_set_val_i32Ptr
      .asFunction<
        void Function(ffi.Pointer<gguf_context>, ffi.Pointer<ffi.Char>, int)
      >();

  void gguf_set_val_f32(
    ffi.Pointer<gguf_context> ctx,
    ffi.Pointer<ffi.Char> key,
    double val,
  ) {
    return _gguf_set_val_f32(ctx, key, val);
  }

  late final _gguf_set_val_f32Ptr =
      _lookup<
        ffi.NativeFunction<
          ffi.Void Function(
            ffi.Pointer<gguf_context>,
            ffi.Pointer<ffi.Char>,
            ffi.Float,
          )
        >
      >('gguf_set_val_f32');
  late final _gguf_set_val_f32 = _gguf_set_val_f32Ptr
      .asFunction<
        void Function(ffi.Pointer<gguf_context>, ffi.Pointer<ffi.Char>, double)
      >();

  void gguf_set_val_u64(
    ffi.Pointer<gguf_context> ctx,
    ffi.Pointer<ffi.Char> key,
    int val,
  ) {
    return _gguf_set_val_u64(ctx, key, val);
  }

  late final _gguf_set_val_u64Ptr =
      _lookup<
        ffi.NativeFunction<
          ffi.Void Function(
            ffi.Pointer<gguf_context>,
            ffi.Pointer<ffi.Char>,
            ffi.Uint64,
          )
        >
      >('gguf_set_val_u64');
  late final _gguf_set_val_u64 = _gguf_set_val_u64Ptr
      .asFunction<
        void Function(ffi.Pointer<gguf_context>, ffi.Pointer<ffi.Char>, int)
      >();

  void gguf_set_val_i64(
    ffi.Pointer<gguf_context> ctx,
    ffi.Pointer<ffi.Char> key,
    int val,
  ) {
    return _gguf_set_val_i64(ctx, key, val);
  }

  late final _gguf_set_val_i64Ptr =
      _lookup<
        ffi.NativeFunction<
          ffi.Void Function(
            ffi.Pointer<gguf_context>,
            ffi.Pointer<ffi.Char>,
            ffi.Int64,
          )
        >
      >('gguf_set_val_i64');
  late final _gguf_set_val_i64 = _gguf_set_val_i64Ptr
      .asFunction<
        void Function(ffi.Pointer<gguf_context>, ffi.Pointer<ffi.Char>, int)
      >();

  void gguf_set_val_f64(
    ffi.Pointer<gguf_context> ctx,
    ffi.Pointer<ffi.Char> key,
    double val,
  ) {
    return _gguf_set_val_f64(ctx, key, val);
  }

  late final _gguf_set_val_f64Ptr =
      _lookup<
        ffi.NativeFunction<
          ffi.Void Function(
            ffi.Pointer<gguf_context>,
            ffi.Pointer<ffi.Char>,
            ffi.Double,
          )
        >
      >('gguf_set_val_f64');
  late final _gguf_set_val_f64 = _gguf_set_val_f64Ptr
      .asFunction<
        void Function(ffi.Pointer<gguf_context>, ffi.Pointer<ffi.Char>, double)
      >();

  void gguf_set_val_bool(
    ffi.Pointer<gguf_context> ctx,
    ffi.Pointer<ffi.Char> key,
    bool val,
  ) {
    return _gguf_set_val_bool(ctx, key, val);
  }

  late final _gguf_set_val_boolPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Void Function(
            ffi.Pointer<gguf_context>,
            ffi.Pointer<ffi.Char>,
            ffi.Bool,
          )
        >
      >('gguf_set_val_bool');
  late final _gguf_set_val_bool = _gguf_set_val_boolPtr
      .asFunction<
        void Function(ffi.Pointer<gguf_context>, ffi.Pointer<ffi.Char>, bool)
      >();

  void gguf_set_val_str(
    ffi.Pointer<gguf_context> ctx,
    ffi.Pointer<ffi.Char> key,
    ffi.Pointer<ffi.Char> val,
  ) {
    return _gguf_set_val_str(ctx, key, val);
  }

  late final _gguf_set_val_strPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Void Function(
            ffi.Pointer<gguf_context>,
            ffi.Pointer<ffi.Char>,
            ffi.Pointer<ffi.Char>,
          )
        >
      >('gguf_set_val_str');
  late final _gguf_set_val_str = _gguf_set_val_strPtr
      .asFunction<
        void Function(
          ffi.Pointer<gguf_context>,
          ffi.Pointer<ffi.Char>,
          ffi.Pointer<ffi.Char>,
        )
      >();

  /// creates a new array with n elements of the given type and copies the corresponding number of bytes from data
  void gguf_set_arr_data(
    ffi.Pointer<gguf_context> ctx,
    ffi.Pointer<ffi.Char> key,
    gguf_type type,
    ffi.Pointer<ffi.Void> data,
    int n,
  ) {
    return _gguf_set_arr_data(ctx, key, type.value, data, n);
  }

  late final _gguf_set_arr_dataPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Void Function(
            ffi.Pointer<gguf_context>,
            ffi.Pointer<ffi.Char>,
            ffi.UnsignedInt,
            ffi.Pointer<ffi.Void>,
            ffi.Size,
          )
        >
      >('gguf_set_arr_data');
  late final _gguf_set_arr_data = _gguf_set_arr_dataPtr
      .asFunction<
        void Function(
          ffi.Pointer<gguf_context>,
          ffi.Pointer<ffi.Char>,
          int,
          ffi.Pointer<ffi.Void>,
          int,
        )
      >();

  /// creates a new array with n strings and copies the corresponding strings from data
  void gguf_set_arr_str(
    ffi.Pointer<gguf_context> ctx,
    ffi.Pointer<ffi.Char> key,
    ffi.Pointer<ffi.Pointer<ffi.Char>> data,
    int n,
  ) {
    return _gguf_set_arr_str(ctx, key, data, n);
  }

  late final _gguf_set_arr_strPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Void Function(
            ffi.Pointer<gguf_context>,
            ffi.Pointer<ffi.Char>,
            ffi.Pointer<ffi.Pointer<ffi.Char>>,
            ffi.Size,
          )
        >
      >('gguf_set_arr_str');
  late final _gguf_set_arr_str = _gguf_set_arr_strPtr
      .asFunction<
        void Function(
          ffi.Pointer<gguf_context>,
          ffi.Pointer<ffi.Char>,
          ffi.Pointer<ffi.Pointer<ffi.Char>>,
          int,
        )
      >();

  /// set or add KV pairs from another context
  void gguf_set_kv(
    ffi.Pointer<gguf_context> ctx,
    ffi.Pointer<gguf_context> src,
  ) {
    return _gguf_set_kv(ctx, src);
  }

  late final _gguf_set_kvPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Void Function(
            ffi.Pointer<gguf_context>,
            ffi.Pointer<gguf_context>,
          )
        >
      >('gguf_set_kv');
  late final _gguf_set_kv = _gguf_set_kvPtr
      .asFunction<
        void Function(ffi.Pointer<gguf_context>, ffi.Pointer<gguf_context>)
      >();

  /// add tensor to GGUF context, tensor name must be unique
  void gguf_add_tensor(
    ffi.Pointer<gguf_context> ctx,
    ffi.Pointer<ggml_tensor> tensor,
  ) {
    return _gguf_add_tensor(ctx, tensor);
  }

  late final _gguf_add_tensorPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<gguf_context>, ffi.Pointer<ggml_tensor>)
        >
      >('gguf_add_tensor');
  late final _gguf_add_tensor = _gguf_add_tensorPtr
      .asFunction<
        void Function(ffi.Pointer<gguf_context>, ffi.Pointer<ggml_tensor>)
      >();

  /// after changing a tensor's type, the offsets of all tensors with higher indices are immediately recalculated
  /// in such a way that the tensor data remains as one contiguous block (except for padding)
  void gguf_set_tensor_type(
    ffi.Pointer<gguf_context> ctx,
    ffi.Pointer<ffi.Char> name,
    ggml_type type,
  ) {
    return _gguf_set_tensor_type(ctx, name, type.value);
  }

  late final _gguf_set_tensor_typePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Void Function(
            ffi.Pointer<gguf_context>,
            ffi.Pointer<ffi.Char>,
            ffi.UnsignedInt,
          )
        >
      >('gguf_set_tensor_type');
  late final _gguf_set_tensor_type = _gguf_set_tensor_typePtr
      .asFunction<
        void Function(ffi.Pointer<gguf_context>, ffi.Pointer<ffi.Char>, int)
      >();

  /// assumes that at least gguf_get_tensor_size bytes can be read from data
  void gguf_set_tensor_data(
    ffi.Pointer<gguf_context> ctx,
    ffi.Pointer<ffi.Char> name,
    ffi.Pointer<ffi.Void> data,
  ) {
    return _gguf_set_tensor_data(ctx, name, data);
  }

  late final _gguf_set_tensor_dataPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Void Function(
            ffi.Pointer<gguf_context>,
            ffi.Pointer<ffi.Char>,
            ffi.Pointer<ffi.Void>,
          )
        >
      >('gguf_set_tensor_data');
  late final _gguf_set_tensor_data = _gguf_set_tensor_dataPtr
      .asFunction<
        void Function(
          ffi.Pointer<gguf_context>,
          ffi.Pointer<ffi.Char>,
          ffi.Pointer<ffi.Void>,
        )
      >();

  /// write the entire context to a binary file
  bool gguf_write_to_file(
    ffi.Pointer<gguf_context> ctx,
    ffi.Pointer<ffi.Char> fname,
    bool only_meta,
  ) {
    return _gguf_write_to_file(ctx, fname, only_meta);
  }

  late final _gguf_write_to_filePtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Bool Function(
            ffi.Pointer<gguf_context>,
            ffi.Pointer<ffi.Char>,
            ffi.Bool,
          )
        >
      >('gguf_write_to_file');
  late final _gguf_write_to_file = _gguf_write_to_filePtr
      .asFunction<
        bool Function(ffi.Pointer<gguf_context>, ffi.Pointer<ffi.Char>, bool)
      >();

  /// get the size in bytes of the meta data (header, kv pairs, tensor info) including padding
  int gguf_get_meta_size(ffi.Pointer<gguf_context> ctx) {
    return _gguf_get_meta_size(ctx);
  }

  late final _gguf_get_meta_sizePtr =
      _lookup<ffi.NativeFunction<ffi.Size Function(ffi.Pointer<gguf_context>)>>(
        'gguf_get_meta_size',
      );
  late final _gguf_get_meta_size = _gguf_get_meta_sizePtr
      .asFunction<int Function(ffi.Pointer<gguf_context>)>();

  /// writes the meta data to pointer "data"
  void gguf_get_meta_data(
    ffi.Pointer<gguf_context> ctx,
    ffi.Pointer<ffi.Void> data,
  ) {
    return _gguf_get_meta_data(ctx, data);
  }

  late final _gguf_get_meta_dataPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<gguf_context>, ffi.Pointer<ffi.Void>)
        >
      >('gguf_get_meta_data');
  late final _gguf_get_meta_data = _gguf_get_meta_dataPtr
      .asFunction<
        void Function(ffi.Pointer<gguf_context>, ffi.Pointer<ffi.Void>)
      >();
}

typedef ggml_abort_callback_tFunction =
    ffi.Void Function(ffi.Pointer<ffi.Char> error_message);
typedef Dartggml_abort_callback_tFunction =
    void Function(ffi.Pointer<ffi.Char> error_message);

/// Function type used in fatal error callbacks
typedef ggml_abort_callback_t =
    ffi.Pointer<ffi.NativeFunction<ggml_abort_callback_tFunction>>;

enum ggml_status {
  GGML_STATUS_ALLOC_FAILED(-2),
  GGML_STATUS_FAILED(-1),
  GGML_STATUS_SUCCESS(0),
  GGML_STATUS_ABORTED(1);

  final int value;
  const ggml_status(this.value);

  static ggml_status fromValue(int value) => switch (value) {
    -2 => GGML_STATUS_ALLOC_FAILED,
    -1 => GGML_STATUS_FAILED,
    0 => GGML_STATUS_SUCCESS,
    1 => GGML_STATUS_ABORTED,
    _ => throw ArgumentError('Unknown value for ggml_status: $value'),
  };
}

/// ieee 754-2008 half-precision float16
/// todo: make this not an integral type
typedef ggml_fp16_t = ffi.Uint16;
typedef Dartggml_fp16_t = int;

/// google brain half-precision bfloat16
final class ggml_bf16_t extends ffi.Struct {
  @ffi.Uint16()
  external int bits;
}

final class ggml_object extends ffi.Opaque {}

final class ggml_context extends ffi.Opaque {}

final class ggml_cgraph extends ffi.Opaque {}

/// NOTE: always add types at the end of the enum to keep backward compatibility
enum ggml_type {
  GGML_TYPE_F32(0),
  GGML_TYPE_F16(1),
  GGML_TYPE_Q4_0(2),
  GGML_TYPE_Q4_1(3),

  /// GGML_TYPE_Q4_2 = 4, support has been removed
  /// GGML_TYPE_Q4_3 = 5, support has been removed
  GGML_TYPE_Q5_0(6),
  GGML_TYPE_Q5_1(7),
  GGML_TYPE_Q8_0(8),
  GGML_TYPE_Q8_1(9),
  GGML_TYPE_Q2_K(10),
  GGML_TYPE_Q3_K(11),
  GGML_TYPE_Q4_K(12),
  GGML_TYPE_Q5_K(13),
  GGML_TYPE_Q6_K(14),
  GGML_TYPE_Q8_K(15),
  GGML_TYPE_IQ2_XXS(16),
  GGML_TYPE_IQ2_XS(17),
  GGML_TYPE_IQ3_XXS(18),
  GGML_TYPE_IQ1_S(19),
  GGML_TYPE_IQ4_NL(20),
  GGML_TYPE_IQ3_S(21),
  GGML_TYPE_IQ2_S(22),
  GGML_TYPE_IQ4_XS(23),
  GGML_TYPE_I8(24),
  GGML_TYPE_I16(25),
  GGML_TYPE_I32(26),
  GGML_TYPE_I64(27),
  GGML_TYPE_F64(28),
  GGML_TYPE_IQ1_M(29),
  GGML_TYPE_BF16(30),

  /// GGML_TYPE_Q4_0_4_4 = 31, support has been removed from gguf files
  /// GGML_TYPE_Q4_0_4_8 = 32,
  /// GGML_TYPE_Q4_0_8_8 = 33,
  GGML_TYPE_TQ1_0(34),
  GGML_TYPE_TQ2_0(35),

  /// MXFP4 (1 block)
  GGML_TYPE_MXFP4(39),
  GGML_TYPE_COUNT(40);

  final int value;
  const ggml_type(this.value);

  static ggml_type fromValue(int value) => switch (value) {
    0 => GGML_TYPE_F32,
    1 => GGML_TYPE_F16,
    2 => GGML_TYPE_Q4_0,
    3 => GGML_TYPE_Q4_1,
    6 => GGML_TYPE_Q5_0,
    7 => GGML_TYPE_Q5_1,
    8 => GGML_TYPE_Q8_0,
    9 => GGML_TYPE_Q8_1,
    10 => GGML_TYPE_Q2_K,
    11 => GGML_TYPE_Q3_K,
    12 => GGML_TYPE_Q4_K,
    13 => GGML_TYPE_Q5_K,
    14 => GGML_TYPE_Q6_K,
    15 => GGML_TYPE_Q8_K,
    16 => GGML_TYPE_IQ2_XXS,
    17 => GGML_TYPE_IQ2_XS,
    18 => GGML_TYPE_IQ3_XXS,
    19 => GGML_TYPE_IQ1_S,
    20 => GGML_TYPE_IQ4_NL,
    21 => GGML_TYPE_IQ3_S,
    22 => GGML_TYPE_IQ2_S,
    23 => GGML_TYPE_IQ4_XS,
    24 => GGML_TYPE_I8,
    25 => GGML_TYPE_I16,
    26 => GGML_TYPE_I32,
    27 => GGML_TYPE_I64,
    28 => GGML_TYPE_F64,
    29 => GGML_TYPE_IQ1_M,
    30 => GGML_TYPE_BF16,
    34 => GGML_TYPE_TQ1_0,
    35 => GGML_TYPE_TQ2_0,
    39 => GGML_TYPE_MXFP4,
    40 => GGML_TYPE_COUNT,
    _ => throw ArgumentError('Unknown value for ggml_type: $value'),
  };
}

/// precision
enum ggml_prec {
  /// stored as ggml_tensor.op_params, 0 by default
  GGML_PREC_DEFAULT(0),
  GGML_PREC_F32(10);

  final int value;
  const ggml_prec(this.value);

  static ggml_prec fromValue(int value) => switch (value) {
    0 => GGML_PREC_DEFAULT,
    10 => GGML_PREC_F32,
    _ => throw ArgumentError('Unknown value for ggml_prec: $value'),
  };
}

/// model file types
enum ggml_ftype {
  GGML_FTYPE_UNKNOWN(-1),
  GGML_FTYPE_ALL_F32(0),

  /// except 1d tensors
  GGML_FTYPE_MOSTLY_F16(1),

  /// except 1d tensors
  GGML_FTYPE_MOSTLY_Q4_0(2),

  /// except 1d tensors
  GGML_FTYPE_MOSTLY_Q4_1(3),

  /// tok_embeddings.weight and output.weight are F16
  GGML_FTYPE_MOSTLY_Q4_1_SOME_F16(4),

  /// except 1d tensors
  GGML_FTYPE_MOSTLY_Q8_0(7),

  /// except 1d tensors
  GGML_FTYPE_MOSTLY_Q5_0(8),

  /// except 1d tensors
  GGML_FTYPE_MOSTLY_Q5_1(9),

  /// except 1d tensors
  GGML_FTYPE_MOSTLY_Q2_K(10),

  /// except 1d tensors
  GGML_FTYPE_MOSTLY_Q3_K(11),

  /// except 1d tensors
  GGML_FTYPE_MOSTLY_Q4_K(12),

  /// except 1d tensors
  GGML_FTYPE_MOSTLY_Q5_K(13),

  /// except 1d tensors
  GGML_FTYPE_MOSTLY_Q6_K(14),

  /// except 1d tensors
  GGML_FTYPE_MOSTLY_IQ2_XXS(15),

  /// except 1d tensors
  GGML_FTYPE_MOSTLY_IQ2_XS(16),

  /// except 1d tensors
  GGML_FTYPE_MOSTLY_IQ3_XXS(17),

  /// except 1d tensors
  GGML_FTYPE_MOSTLY_IQ1_S(18),

  /// except 1d tensors
  GGML_FTYPE_MOSTLY_IQ4_NL(19),

  /// except 1d tensors
  GGML_FTYPE_MOSTLY_IQ3_S(20),

  /// except 1d tensors
  GGML_FTYPE_MOSTLY_IQ2_S(21),

  /// except 1d tensors
  GGML_FTYPE_MOSTLY_IQ4_XS(22),

  /// except 1d tensors
  GGML_FTYPE_MOSTLY_IQ1_M(23),

  /// except 1d tensors
  GGML_FTYPE_MOSTLY_BF16(24),

  /// except 1d tensors
  GGML_FTYPE_MOSTLY_MXFP4(25);

  final int value;
  const ggml_ftype(this.value);

  static ggml_ftype fromValue(int value) => switch (value) {
    -1 => GGML_FTYPE_UNKNOWN,
    0 => GGML_FTYPE_ALL_F32,
    1 => GGML_FTYPE_MOSTLY_F16,
    2 => GGML_FTYPE_MOSTLY_Q4_0,
    3 => GGML_FTYPE_MOSTLY_Q4_1,
    4 => GGML_FTYPE_MOSTLY_Q4_1_SOME_F16,
    7 => GGML_FTYPE_MOSTLY_Q8_0,
    8 => GGML_FTYPE_MOSTLY_Q5_0,
    9 => GGML_FTYPE_MOSTLY_Q5_1,
    10 => GGML_FTYPE_MOSTLY_Q2_K,
    11 => GGML_FTYPE_MOSTLY_Q3_K,
    12 => GGML_FTYPE_MOSTLY_Q4_K,
    13 => GGML_FTYPE_MOSTLY_Q5_K,
    14 => GGML_FTYPE_MOSTLY_Q6_K,
    15 => GGML_FTYPE_MOSTLY_IQ2_XXS,
    16 => GGML_FTYPE_MOSTLY_IQ2_XS,
    17 => GGML_FTYPE_MOSTLY_IQ3_XXS,
    18 => GGML_FTYPE_MOSTLY_IQ1_S,
    19 => GGML_FTYPE_MOSTLY_IQ4_NL,
    20 => GGML_FTYPE_MOSTLY_IQ3_S,
    21 => GGML_FTYPE_MOSTLY_IQ2_S,
    22 => GGML_FTYPE_MOSTLY_IQ4_XS,
    23 => GGML_FTYPE_MOSTLY_IQ1_M,
    24 => GGML_FTYPE_MOSTLY_BF16,
    25 => GGML_FTYPE_MOSTLY_MXFP4,
    _ => throw ArgumentError('Unknown value for ggml_ftype: $value'),
  };
}

/// available tensor operations:
enum ggml_op {
  GGML_OP_NONE(0),
  GGML_OP_DUP(1),
  GGML_OP_ADD(2),
  GGML_OP_ADD_ID(3),
  GGML_OP_ADD1(4),
  GGML_OP_ACC(5),
  GGML_OP_SUB(6),
  GGML_OP_MUL(7),
  GGML_OP_DIV(8),
  GGML_OP_SQR(9),
  GGML_OP_SQRT(10),
  GGML_OP_LOG(11),
  GGML_OP_SIN(12),
  GGML_OP_COS(13),
  GGML_OP_SUM(14),
  GGML_OP_SUM_ROWS(15),
  GGML_OP_CUMSUM(16),
  GGML_OP_MEAN(17),
  GGML_OP_ARGMAX(18),
  GGML_OP_COUNT_EQUAL(19),
  GGML_OP_REPEAT(20),
  GGML_OP_REPEAT_BACK(21),
  GGML_OP_CONCAT(22),
  GGML_OP_SILU_BACK(23),

  /// normalize
  GGML_OP_NORM(24),
  GGML_OP_RMS_NORM(25),
  GGML_OP_RMS_NORM_BACK(26),
  GGML_OP_GROUP_NORM(27),
  GGML_OP_L2_NORM(28),
  GGML_OP_MUL_MAT(29),
  GGML_OP_MUL_MAT_ID(30),
  GGML_OP_OUT_PROD(31),
  GGML_OP_SCALE(32),
  GGML_OP_SET(33),
  GGML_OP_CPY(34),
  GGML_OP_CONT(35),
  GGML_OP_RESHAPE(36),
  GGML_OP_VIEW(37),
  GGML_OP_PERMUTE(38),
  GGML_OP_TRANSPOSE(39),
  GGML_OP_GET_ROWS(40),
  GGML_OP_GET_ROWS_BACK(41),
  GGML_OP_SET_ROWS(42),
  GGML_OP_DIAG(43),
  GGML_OP_DIAG_MASK_INF(44),
  GGML_OP_DIAG_MASK_ZERO(45),
  GGML_OP_SOFT_MAX(46),
  GGML_OP_SOFT_MAX_BACK(47),
  GGML_OP_ROPE(48),
  GGML_OP_ROPE_BACK(49),
  GGML_OP_CLAMP(50),
  GGML_OP_CONV_TRANSPOSE_1D(51),
  GGML_OP_IM2COL(52),
  GGML_OP_IM2COL_BACK(53),
  GGML_OP_IM2COL_3D(54),
  GGML_OP_CONV_2D(55),
  GGML_OP_CONV_3D(56),
  GGML_OP_CONV_2D_DW(57),
  GGML_OP_CONV_TRANSPOSE_2D(58),
  GGML_OP_POOL_1D(59),
  GGML_OP_POOL_2D(60),
  GGML_OP_POOL_2D_BACK(61),
  GGML_OP_UPSCALE(62),
  GGML_OP_PAD(63),
  GGML_OP_PAD_REFLECT_1D(64),
  GGML_OP_ROLL(65),
  GGML_OP_ARANGE(66),
  GGML_OP_TIMESTEP_EMBEDDING(67),
  GGML_OP_ARGSORT(68),
  GGML_OP_TOP_K(69),
  GGML_OP_LEAKY_RELU(70),
  GGML_OP_TRI(71),
  GGML_OP_FILL(72),
  GGML_OP_FLASH_ATTN_EXT(73),
  GGML_OP_FLASH_ATTN_BACK(74),
  GGML_OP_SSM_CONV(75),
  GGML_OP_SSM_SCAN(76),
  GGML_OP_WIN_PART(77),
  GGML_OP_WIN_UNPART(78),
  GGML_OP_GET_REL_POS(79),
  GGML_OP_ADD_REL_POS(80),
  GGML_OP_RWKV_WKV6(81),
  GGML_OP_GATED_LINEAR_ATTN(82),
  GGML_OP_RWKV_WKV7(83),
  GGML_OP_SOLVE_TRI(84),
  GGML_OP_UNARY(85),
  GGML_OP_MAP_CUSTOM1(86),
  GGML_OP_MAP_CUSTOM2(87),
  GGML_OP_MAP_CUSTOM3(88),
  GGML_OP_CUSTOM(89),
  GGML_OP_CROSS_ENTROPY_LOSS(90),
  GGML_OP_CROSS_ENTROPY_LOSS_BACK(91),
  GGML_OP_OPT_STEP_ADAMW(92),
  GGML_OP_OPT_STEP_SGD(93),
  GGML_OP_GLU(94),
  GGML_OP_COUNT(95);

  final int value;
  const ggml_op(this.value);

  static ggml_op fromValue(int value) => switch (value) {
    0 => GGML_OP_NONE,
    1 => GGML_OP_DUP,
    2 => GGML_OP_ADD,
    3 => GGML_OP_ADD_ID,
    4 => GGML_OP_ADD1,
    5 => GGML_OP_ACC,
    6 => GGML_OP_SUB,
    7 => GGML_OP_MUL,
    8 => GGML_OP_DIV,
    9 => GGML_OP_SQR,
    10 => GGML_OP_SQRT,
    11 => GGML_OP_LOG,
    12 => GGML_OP_SIN,
    13 => GGML_OP_COS,
    14 => GGML_OP_SUM,
    15 => GGML_OP_SUM_ROWS,
    16 => GGML_OP_CUMSUM,
    17 => GGML_OP_MEAN,
    18 => GGML_OP_ARGMAX,
    19 => GGML_OP_COUNT_EQUAL,
    20 => GGML_OP_REPEAT,
    21 => GGML_OP_REPEAT_BACK,
    22 => GGML_OP_CONCAT,
    23 => GGML_OP_SILU_BACK,
    24 => GGML_OP_NORM,
    25 => GGML_OP_RMS_NORM,
    26 => GGML_OP_RMS_NORM_BACK,
    27 => GGML_OP_GROUP_NORM,
    28 => GGML_OP_L2_NORM,
    29 => GGML_OP_MUL_MAT,
    30 => GGML_OP_MUL_MAT_ID,
    31 => GGML_OP_OUT_PROD,
    32 => GGML_OP_SCALE,
    33 => GGML_OP_SET,
    34 => GGML_OP_CPY,
    35 => GGML_OP_CONT,
    36 => GGML_OP_RESHAPE,
    37 => GGML_OP_VIEW,
    38 => GGML_OP_PERMUTE,
    39 => GGML_OP_TRANSPOSE,
    40 => GGML_OP_GET_ROWS,
    41 => GGML_OP_GET_ROWS_BACK,
    42 => GGML_OP_SET_ROWS,
    43 => GGML_OP_DIAG,
    44 => GGML_OP_DIAG_MASK_INF,
    45 => GGML_OP_DIAG_MASK_ZERO,
    46 => GGML_OP_SOFT_MAX,
    47 => GGML_OP_SOFT_MAX_BACK,
    48 => GGML_OP_ROPE,
    49 => GGML_OP_ROPE_BACK,
    50 => GGML_OP_CLAMP,
    51 => GGML_OP_CONV_TRANSPOSE_1D,
    52 => GGML_OP_IM2COL,
    53 => GGML_OP_IM2COL_BACK,
    54 => GGML_OP_IM2COL_3D,
    55 => GGML_OP_CONV_2D,
    56 => GGML_OP_CONV_3D,
    57 => GGML_OP_CONV_2D_DW,
    58 => GGML_OP_CONV_TRANSPOSE_2D,
    59 => GGML_OP_POOL_1D,
    60 => GGML_OP_POOL_2D,
    61 => GGML_OP_POOL_2D_BACK,
    62 => GGML_OP_UPSCALE,
    63 => GGML_OP_PAD,
    64 => GGML_OP_PAD_REFLECT_1D,
    65 => GGML_OP_ROLL,
    66 => GGML_OP_ARANGE,
    67 => GGML_OP_TIMESTEP_EMBEDDING,
    68 => GGML_OP_ARGSORT,
    69 => GGML_OP_TOP_K,
    70 => GGML_OP_LEAKY_RELU,
    71 => GGML_OP_TRI,
    72 => GGML_OP_FILL,
    73 => GGML_OP_FLASH_ATTN_EXT,
    74 => GGML_OP_FLASH_ATTN_BACK,
    75 => GGML_OP_SSM_CONV,
    76 => GGML_OP_SSM_SCAN,
    77 => GGML_OP_WIN_PART,
    78 => GGML_OP_WIN_UNPART,
    79 => GGML_OP_GET_REL_POS,
    80 => GGML_OP_ADD_REL_POS,
    81 => GGML_OP_RWKV_WKV6,
    82 => GGML_OP_GATED_LINEAR_ATTN,
    83 => GGML_OP_RWKV_WKV7,
    84 => GGML_OP_SOLVE_TRI,
    85 => GGML_OP_UNARY,
    86 => GGML_OP_MAP_CUSTOM1,
    87 => GGML_OP_MAP_CUSTOM2,
    88 => GGML_OP_MAP_CUSTOM3,
    89 => GGML_OP_CUSTOM,
    90 => GGML_OP_CROSS_ENTROPY_LOSS,
    91 => GGML_OP_CROSS_ENTROPY_LOSS_BACK,
    92 => GGML_OP_OPT_STEP_ADAMW,
    93 => GGML_OP_OPT_STEP_SGD,
    94 => GGML_OP_GLU,
    95 => GGML_OP_COUNT,
    _ => throw ArgumentError('Unknown value for ggml_op: $value'),
  };
}

enum ggml_unary_op {
  GGML_UNARY_OP_ABS(0),
  GGML_UNARY_OP_SGN(1),
  GGML_UNARY_OP_NEG(2),
  GGML_UNARY_OP_STEP(3),
  GGML_UNARY_OP_TANH(4),
  GGML_UNARY_OP_ELU(5),
  GGML_UNARY_OP_RELU(6),
  GGML_UNARY_OP_SIGMOID(7),
  GGML_UNARY_OP_GELU(8),
  GGML_UNARY_OP_GELU_QUICK(9),
  GGML_UNARY_OP_SILU(10),
  GGML_UNARY_OP_HARDSWISH(11),
  GGML_UNARY_OP_HARDSIGMOID(12),
  GGML_UNARY_OP_EXP(13),
  GGML_UNARY_OP_EXPM1(14),
  GGML_UNARY_OP_SOFTPLUS(15),
  GGML_UNARY_OP_GELU_ERF(16),
  GGML_UNARY_OP_XIELU(17),
  GGML_UNARY_OP_FLOOR(18),
  GGML_UNARY_OP_CEIL(19),
  GGML_UNARY_OP_ROUND(20),
  GGML_UNARY_OP_TRUNC(21),
  GGML_UNARY_OP_COUNT(22);

  final int value;
  const ggml_unary_op(this.value);

  static ggml_unary_op fromValue(int value) => switch (value) {
    0 => GGML_UNARY_OP_ABS,
    1 => GGML_UNARY_OP_SGN,
    2 => GGML_UNARY_OP_NEG,
    3 => GGML_UNARY_OP_STEP,
    4 => GGML_UNARY_OP_TANH,
    5 => GGML_UNARY_OP_ELU,
    6 => GGML_UNARY_OP_RELU,
    7 => GGML_UNARY_OP_SIGMOID,
    8 => GGML_UNARY_OP_GELU,
    9 => GGML_UNARY_OP_GELU_QUICK,
    10 => GGML_UNARY_OP_SILU,
    11 => GGML_UNARY_OP_HARDSWISH,
    12 => GGML_UNARY_OP_HARDSIGMOID,
    13 => GGML_UNARY_OP_EXP,
    14 => GGML_UNARY_OP_EXPM1,
    15 => GGML_UNARY_OP_SOFTPLUS,
    16 => GGML_UNARY_OP_GELU_ERF,
    17 => GGML_UNARY_OP_XIELU,
    18 => GGML_UNARY_OP_FLOOR,
    19 => GGML_UNARY_OP_CEIL,
    20 => GGML_UNARY_OP_ROUND,
    21 => GGML_UNARY_OP_TRUNC,
    22 => GGML_UNARY_OP_COUNT,
    _ => throw ArgumentError('Unknown value for ggml_unary_op: $value'),
  };
}

enum ggml_glu_op {
  GGML_GLU_OP_REGLU(0),
  GGML_GLU_OP_GEGLU(1),
  GGML_GLU_OP_SWIGLU(2),
  GGML_GLU_OP_SWIGLU_OAI(3),
  GGML_GLU_OP_GEGLU_ERF(4),
  GGML_GLU_OP_GEGLU_QUICK(5),
  GGML_GLU_OP_COUNT(6);

  final int value;
  const ggml_glu_op(this.value);

  static ggml_glu_op fromValue(int value) => switch (value) {
    0 => GGML_GLU_OP_REGLU,
    1 => GGML_GLU_OP_GEGLU,
    2 => GGML_GLU_OP_SWIGLU,
    3 => GGML_GLU_OP_SWIGLU_OAI,
    4 => GGML_GLU_OP_GEGLU_ERF,
    5 => GGML_GLU_OP_GEGLU_QUICK,
    6 => GGML_GLU_OP_COUNT,
    _ => throw ArgumentError('Unknown value for ggml_glu_op: $value'),
  };
}

enum ggml_object_type {
  GGML_OBJECT_TYPE_TENSOR(0),
  GGML_OBJECT_TYPE_GRAPH(1),
  GGML_OBJECT_TYPE_WORK_BUFFER(2);

  final int value;
  const ggml_object_type(this.value);

  static ggml_object_type fromValue(int value) => switch (value) {
    0 => GGML_OBJECT_TYPE_TENSOR,
    1 => GGML_OBJECT_TYPE_GRAPH,
    2 => GGML_OBJECT_TYPE_WORK_BUFFER,
    _ => throw ArgumentError('Unknown value for ggml_object_type: $value'),
  };
}

enum ggml_log_level {
  GGML_LOG_LEVEL_NONE(0),
  GGML_LOG_LEVEL_DEBUG(1),
  GGML_LOG_LEVEL_INFO(2),
  GGML_LOG_LEVEL_WARN(3),
  GGML_LOG_LEVEL_ERROR(4),

  /// continue previous log
  GGML_LOG_LEVEL_CONT(5);

  final int value;
  const ggml_log_level(this.value);

  static ggml_log_level fromValue(int value) => switch (value) {
    0 => GGML_LOG_LEVEL_NONE,
    1 => GGML_LOG_LEVEL_DEBUG,
    2 => GGML_LOG_LEVEL_INFO,
    3 => GGML_LOG_LEVEL_WARN,
    4 => GGML_LOG_LEVEL_ERROR,
    5 => GGML_LOG_LEVEL_CONT,
    _ => throw ArgumentError('Unknown value for ggml_log_level: $value'),
  };
}

/// this tensor...
enum ggml_tensor_flag {
  /// ...is an input for the GGML compute graph
  GGML_TENSOR_FLAG_INPUT(1),

  /// ...is an output for the GGML compute graph
  GGML_TENSOR_FLAG_OUTPUT(2),

  /// ...contains trainable parameters
  GGML_TENSOR_FLAG_PARAM(4),

  /// ...defines loss for numerical optimization (multiple loss tensors add up)
  GGML_TENSOR_FLAG_LOSS(8);

  final int value;
  const ggml_tensor_flag(this.value);

  static ggml_tensor_flag fromValue(int value) => switch (value) {
    1 => GGML_TENSOR_FLAG_INPUT,
    2 => GGML_TENSOR_FLAG_OUTPUT,
    4 => GGML_TENSOR_FLAG_PARAM,
    8 => GGML_TENSOR_FLAG_LOSS,
    _ => throw ArgumentError('Unknown value for ggml_tensor_flag: $value'),
  };
}

enum ggml_tri_type {
  GGML_TRI_TYPE_UPPER_DIAG(0),
  GGML_TRI_TYPE_UPPER(1),
  GGML_TRI_TYPE_LOWER_DIAG(2),
  GGML_TRI_TYPE_LOWER(3);

  final int value;
  const ggml_tri_type(this.value);

  static ggml_tri_type fromValue(int value) => switch (value) {
    0 => GGML_TRI_TYPE_UPPER_DIAG,
    1 => GGML_TRI_TYPE_UPPER,
    2 => GGML_TRI_TYPE_LOWER_DIAG,
    3 => GGML_TRI_TYPE_LOWER,
    _ => throw ArgumentError('Unknown value for ggml_tri_type: $value'),
  };
}

final class ggml_init_params extends ffi.Struct {
  /// bytes
  @ffi.Size()
  external int mem_size;

  /// if NULL, memory will be allocated internally
  external ffi.Pointer<ffi.Void> mem_buffer;

  /// don't allocate memory for the tensor data
  @ffi.Bool()
  external bool no_alloc;
}

final class ggml_backend_buffer extends ffi.Opaque {}

/// n-dimensional tensor
final class ggml_tensor extends ffi.Struct {
  @ffi.UnsignedInt()
  external int typeAsInt;

  ggml_type get type => ggml_type.fromValue(typeAsInt);

  external ffi.Pointer<ggml_backend_buffer> buffer;

  /// number of elements
  @ffi.Array.multi([4])
  external ffi.Array<ffi.Int64> ne;

  /// stride in bytes:
  /// nb[0] = ggml_type_size(type)
  /// nb[1] = nb[0]   * (ne[0] / ggml_blck_size(type)) + padding
  /// nb[i] = nb[i-1] * ne[i-1]
  @ffi.Array.multi([4])
  external ffi.Array<ffi.Size> nb;

  /// compute data
  @ffi.UnsignedInt()
  external int opAsInt;

  ggml_op get op => ggml_op.fromValue(opAsInt);

  /// op params - allocated as int32_t for alignment
  @ffi.Array.multi([16])
  external ffi.Array<ffi.Int32> op_params;

  @ffi.Int32()
  external int flags;

  @ffi.Array.multi([10])
  external ffi.Array<ffi.Pointer<ggml_tensor>> src;

  /// source tensor and offset for views
  external ffi.Pointer<ggml_tensor> view_src;

  @ffi.Size()
  external int view_offs;

  external ffi.Pointer<ffi.Void> data;

  @ffi.Array.multi([64])
  external ffi.Array<ffi.Char> name;

  /// extra things e.g. for ggml-cuda.cu
  external ffi.Pointer<ffi.Void> extra;

  @ffi.Array.multi([8])
  external ffi.Array<ffi.Char> padding;
}

typedef ggml_abort_callbackFunction =
    ffi.Bool Function(ffi.Pointer<ffi.Void> data);
typedef Dartggml_abort_callbackFunction =
    bool Function(ffi.Pointer<ffi.Void> data);

/// Abort callback
/// If not NULL, called before ggml computation
/// If it returns true, the computation is aborted
typedef ggml_abort_callback =
    ffi.Pointer<ffi.NativeFunction<ggml_abort_callbackFunction>>;
typedef ggml_guid_t = ffi.Pointer<ffi.Pointer<ffi.Uint8>>;

final class _IO_marker extends ffi.Opaque {}

typedef __off_t = ffi.Long;
typedef Dart__off_t = int;
typedef _IO_lock_t = ffi.Void;
typedef Dart_IO_lock_t = void;
typedef __off64_t = ffi.Long;
typedef Dart__off64_t = int;

final class _IO_codecvt extends ffi.Opaque {}

final class _IO_wide_data extends ffi.Opaque {}

final class _IO_FILE extends ffi.Struct {
  @ffi.Int()
  external int _flags;

  external ffi.Pointer<ffi.Char> _IO_read_ptr;

  external ffi.Pointer<ffi.Char> _IO_read_end;

  external ffi.Pointer<ffi.Char> _IO_read_base;

  external ffi.Pointer<ffi.Char> _IO_write_base;

  external ffi.Pointer<ffi.Char> _IO_write_ptr;

  external ffi.Pointer<ffi.Char> _IO_write_end;

  external ffi.Pointer<ffi.Char> _IO_buf_base;

  external ffi.Pointer<ffi.Char> _IO_buf_end;

  external ffi.Pointer<ffi.Char> _IO_save_base;

  external ffi.Pointer<ffi.Char> _IO_backup_base;

  external ffi.Pointer<ffi.Char> _IO_save_end;

  external ffi.Pointer<_IO_marker> _markers;

  external ffi.Pointer<_IO_FILE> _chain;

  @ffi.Int()
  external int _fileno;

  @ffi.Int()
  external int _flags2;

  @__off_t()
  external int _old_offset;

  @ffi.UnsignedShort()
  external int _cur_column;

  @ffi.SignedChar()
  external int _vtable_offset;

  @ffi.Array.multi([1])
  external ffi.Array<ffi.Char> _shortbuf;

  external ffi.Pointer<_IO_lock_t> _lock;

  @__off64_t()
  external int _offset;

  external ffi.Pointer<_IO_codecvt> _codecvt;

  external ffi.Pointer<_IO_wide_data> _wide_data;

  external ffi.Pointer<_IO_FILE> _freeres_list;

  external ffi.Pointer<ffi.Void> _freeres_buf;

  @ffi.Size()
  external int __pad5;

  @ffi.Int()
  external int _mode;

  @ffi.Array.multi([20])
  external ffi.Array<ffi.Char> _unused2;
}

typedef FILE = _IO_FILE;

enum ggml_op_pool {
  GGML_OP_POOL_MAX(0),
  GGML_OP_POOL_AVG(1),
  GGML_OP_POOL_COUNT(2);

  final int value;
  const ggml_op_pool(this.value);

  static ggml_op_pool fromValue(int value) => switch (value) {
    0 => GGML_OP_POOL_MAX,
    1 => GGML_OP_POOL_AVG,
    2 => GGML_OP_POOL_COUNT,
    _ => throw ArgumentError('Unknown value for ggml_op_pool: $value'),
  };
}

enum ggml_scale_mode {
  GGML_SCALE_MODE_NEAREST(0),
  GGML_SCALE_MODE_BILINEAR(1),
  GGML_SCALE_MODE_BICUBIC(2),
  GGML_SCALE_MODE_COUNT(3);

  final int value;
  const ggml_scale_mode(this.value);

  static ggml_scale_mode fromValue(int value) => switch (value) {
    0 => GGML_SCALE_MODE_NEAREST,
    1 => GGML_SCALE_MODE_BILINEAR,
    2 => GGML_SCALE_MODE_BICUBIC,
    3 => GGML_SCALE_MODE_COUNT,
    _ => throw ArgumentError('Unknown value for ggml_scale_mode: $value'),
  };
}

enum ggml_scale_flag {
  GGML_SCALE_FLAG_ALIGN_CORNERS(256),
  GGML_SCALE_FLAG_ANTIALIAS(512);

  final int value;
  const ggml_scale_flag(this.value);

  static ggml_scale_flag fromValue(int value) => switch (value) {
    256 => GGML_SCALE_FLAG_ALIGN_CORNERS,
    512 => GGML_SCALE_FLAG_ANTIALIAS,
    _ => throw ArgumentError('Unknown value for ggml_scale_flag: $value'),
  };
}

/// sort rows
enum ggml_sort_order {
  GGML_SORT_ORDER_ASC(0),
  GGML_SORT_ORDER_DESC(1);

  final int value;
  const ggml_sort_order(this.value);

  static ggml_sort_order fromValue(int value) => switch (value) {
    0 => GGML_SORT_ORDER_ASC,
    1 => GGML_SORT_ORDER_DESC,
    _ => throw ArgumentError('Unknown value for ggml_sort_order: $value'),
  };
}

typedef ggml_custom1_op_tFunction =
    ffi.Void Function(
      ffi.Pointer<ggml_tensor> dst,
      ffi.Pointer<ggml_tensor> a,
      ffi.Int ith,
      ffi.Int nth,
      ffi.Pointer<ffi.Void> userdata,
    );
typedef Dartggml_custom1_op_tFunction =
    void Function(
      ffi.Pointer<ggml_tensor> dst,
      ffi.Pointer<ggml_tensor> a,
      int ith,
      int nth,
      ffi.Pointer<ffi.Void> userdata,
    );

/// custom operators
typedef ggml_custom1_op_t =
    ffi.Pointer<ffi.NativeFunction<ggml_custom1_op_tFunction>>;
typedef ggml_custom2_op_tFunction =
    ffi.Void Function(
      ffi.Pointer<ggml_tensor> dst,
      ffi.Pointer<ggml_tensor> a,
      ffi.Pointer<ggml_tensor> b,
      ffi.Int ith,
      ffi.Int nth,
      ffi.Pointer<ffi.Void> userdata,
    );
typedef Dartggml_custom2_op_tFunction =
    void Function(
      ffi.Pointer<ggml_tensor> dst,
      ffi.Pointer<ggml_tensor> a,
      ffi.Pointer<ggml_tensor> b,
      int ith,
      int nth,
      ffi.Pointer<ffi.Void> userdata,
    );
typedef ggml_custom2_op_t =
    ffi.Pointer<ffi.NativeFunction<ggml_custom2_op_tFunction>>;
typedef ggml_custom3_op_tFunction =
    ffi.Void Function(
      ffi.Pointer<ggml_tensor> dst,
      ffi.Pointer<ggml_tensor> a,
      ffi.Pointer<ggml_tensor> b,
      ffi.Pointer<ggml_tensor> c,
      ffi.Int ith,
      ffi.Int nth,
      ffi.Pointer<ffi.Void> userdata,
    );
typedef Dartggml_custom3_op_tFunction =
    void Function(
      ffi.Pointer<ggml_tensor> dst,
      ffi.Pointer<ggml_tensor> a,
      ffi.Pointer<ggml_tensor> b,
      ffi.Pointer<ggml_tensor> c,
      int ith,
      int nth,
      ffi.Pointer<ffi.Void> userdata,
    );
typedef ggml_custom3_op_t =
    ffi.Pointer<ffi.NativeFunction<ggml_custom3_op_tFunction>>;
typedef ggml_custom_op_tFunction =
    ffi.Void Function(
      ffi.Pointer<ggml_tensor> dst,
      ffi.Int ith,
      ffi.Int nth,
      ffi.Pointer<ffi.Void> userdata,
    );
typedef Dartggml_custom_op_tFunction =
    void Function(
      ffi.Pointer<ggml_tensor> dst,
      int ith,
      int nth,
      ffi.Pointer<ffi.Void> userdata,
    );
typedef ggml_custom_op_t =
    ffi.Pointer<ffi.NativeFunction<ggml_custom_op_tFunction>>;
typedef ggml_log_callbackFunction =
    ffi.Void Function(
      ffi.UnsignedInt level,
      ffi.Pointer<ffi.Char> text,
      ffi.Pointer<ffi.Void> user_data,
    );
typedef Dartggml_log_callbackFunction =
    void Function(
      ggml_log_level level,
      ffi.Pointer<ffi.Char> text,
      ffi.Pointer<ffi.Void> user_data,
    );

/// TODO these functions were sandwiched in the old optimization interface, is there a better place for them?
typedef ggml_log_callback =
    ffi.Pointer<ffi.NativeFunction<ggml_log_callbackFunction>>;
typedef ggml_to_float_tFunction =
    ffi.Void Function(
      ffi.Pointer<ffi.Void> x,
      ffi.Pointer<ffi.Float> y,
      ffi.Int64 k,
    );
typedef Dartggml_to_float_tFunction =
    void Function(ffi.Pointer<ffi.Void> x, ffi.Pointer<ffi.Float> y, int k);
typedef ggml_to_float_t =
    ffi.Pointer<ffi.NativeFunction<ggml_to_float_tFunction>>;
typedef ggml_from_float_tFunction =
    ffi.Void Function(
      ffi.Pointer<ffi.Float> x,
      ffi.Pointer<ffi.Void> y,
      ffi.Int64 k,
    );
typedef Dartggml_from_float_tFunction =
    void Function(ffi.Pointer<ffi.Float> x, ffi.Pointer<ffi.Void> y, int k);
typedef ggml_from_float_t =
    ffi.Pointer<ffi.NativeFunction<ggml_from_float_tFunction>>;

final class ggml_type_traits extends ffi.Struct {
  external ffi.Pointer<ffi.Char> type_name;

  @ffi.Int64()
  external int blck_size;

  /// interleave elements in blocks
  @ffi.Int64()
  external int blck_size_interleave;

  @ffi.Size()
  external int type_size;

  @ffi.Bool()
  external bool is_quantized;

  external ggml_to_float_t to_float;

  external ggml_from_float_t from_float_ref;
}

/// scheduling priorities
enum ggml_sched_priority {
  GGML_SCHED_PRIO_LOW(-1),
  GGML_SCHED_PRIO_NORMAL(0),
  GGML_SCHED_PRIO_MEDIUM(1),
  GGML_SCHED_PRIO_HIGH(2),
  GGML_SCHED_PRIO_REALTIME(3);

  final int value;
  const ggml_sched_priority(this.value);

  static ggml_sched_priority fromValue(int value) => switch (value) {
    -1 => GGML_SCHED_PRIO_LOW,
    0 => GGML_SCHED_PRIO_NORMAL,
    1 => GGML_SCHED_PRIO_MEDIUM,
    2 => GGML_SCHED_PRIO_HIGH,
    3 => GGML_SCHED_PRIO_REALTIME,
    _ => throw ArgumentError('Unknown value for ggml_sched_priority: $value'),
  };
}

/// threadpool params
/// Use ggml_threadpool_params_default() or ggml_threadpool_params_init() to populate the defaults
final class ggml_threadpool_params extends ffi.Struct {
  /// mask of cpu cores (all-zeros means use default affinity settings)
  @ffi.Array.multi([512])
  external ffi.Array<ffi.Bool> cpumask;

  /// number of threads
  @ffi.Int()
  external int n_threads;

  /// thread priority
  @ffi.Int()
  external int prioAsInt;

  ggml_sched_priority get prio => ggml_sched_priority.fromValue(prioAsInt);

  /// polling level (0 - no polling, 100 - aggressive polling)
  @ffi.Uint32()
  external int poll;

  /// strict cpu placement
  @ffi.Bool()
  external bool strict_cpu;

  /// start in paused state
  @ffi.Bool()
  external bool paused;
}

final class ggml_threadpool extends ffi.Opaque {}

typedef ggml_threadpool_t = ffi.Pointer<ggml_threadpool>;

final class ggml_backend_buffer_type extends ffi.Opaque {}

typedef ggml_backend_buffer_type_t = ffi.Pointer<ggml_backend_buffer_type>;
typedef ggml_backend_buffer_t = ffi.Pointer<ggml_backend_buffer>;

final class ggml_backend extends ffi.Opaque {}

typedef ggml_backend_t = ffi.Pointer<ggml_backend>;

/// Tensor allocator
final class ggml_tallocr extends ffi.Struct {
  external ggml_backend_buffer_t buffer;

  external ffi.Pointer<ffi.Void> base;

  @ffi.Size()
  external int alignment;

  @ffi.Size()
  external int offset;
}

final class ggml_gallocr extends ffi.Opaque {}

/// special tensor flags for use with the graph allocator:
/// ggml_set_input(): all input tensors are allocated at the beginning of the graph in non-overlapping addresses
/// ggml_set_output(): output tensors are never freed and never overwritten
typedef ggml_gallocr_t = ffi.Pointer<ggml_gallocr>;

final class ggml_backend_event extends ffi.Opaque {}

typedef ggml_backend_event_t = ffi.Pointer<ggml_backend_event>;
typedef ggml_backend_graph_plan_t = ffi.Pointer<ffi.Void>;

final class ggml_backend_reg extends ffi.Opaque {}

typedef ggml_backend_reg_t = ffi.Pointer<ggml_backend_reg>;

final class ggml_backend_device extends ffi.Opaque {}

typedef ggml_backend_dev_t = ffi.Pointer<ggml_backend_device>;

/// Backend buffer
enum ggml_backend_buffer_usage {
  GGML_BACKEND_BUFFER_USAGE_ANY(0),
  GGML_BACKEND_BUFFER_USAGE_WEIGHTS(1),
  GGML_BACKEND_BUFFER_USAGE_COMPUTE(2);

  final int value;
  const ggml_backend_buffer_usage(this.value);

  static ggml_backend_buffer_usage fromValue(int value) => switch (value) {
    0 => GGML_BACKEND_BUFFER_USAGE_ANY,
    1 => GGML_BACKEND_BUFFER_USAGE_WEIGHTS,
    2 => GGML_BACKEND_BUFFER_USAGE_COMPUTE,
    _ => throw ArgumentError(
      'Unknown value for ggml_backend_buffer_usage: $value',
    ),
  };
}

/// Backend device
enum ggml_backend_dev_type {
  /// CPU device using system memory
  GGML_BACKEND_DEVICE_TYPE_CPU(0),

  /// GPU device using dedicated memory
  GGML_BACKEND_DEVICE_TYPE_GPU(1),

  /// integrated GPU device using host memory
  GGML_BACKEND_DEVICE_TYPE_IGPU(2),

  /// accelerator devices intended to be used together with the CPU backend (e.g. BLAS or AMX)
  GGML_BACKEND_DEVICE_TYPE_ACCEL(3);

  final int value;
  const ggml_backend_dev_type(this.value);

  static ggml_backend_dev_type fromValue(int value) => switch (value) {
    0 => GGML_BACKEND_DEVICE_TYPE_CPU,
    1 => GGML_BACKEND_DEVICE_TYPE_GPU,
    2 => GGML_BACKEND_DEVICE_TYPE_IGPU,
    3 => GGML_BACKEND_DEVICE_TYPE_ACCEL,
    _ => throw ArgumentError('Unknown value for ggml_backend_dev_type: $value'),
  };
}

/// functionality supported by the device
final class ggml_backend_dev_caps extends ffi.Struct {
  /// asynchronous operations
  @ffi.Bool()
  external bool async;

  /// pinned host buffer
  @ffi.Bool()
  external bool host_buffer;

  /// creating buffers from host ptr
  @ffi.Bool()
  external bool buffer_from_host_ptr;

  /// event synchronization
  @ffi.Bool()
  external bool events;
}

/// all the device properties
final class ggml_backend_dev_props extends ffi.Struct {
  /// device name
  external ffi.Pointer<ffi.Char> name;

  /// device description
  external ffi.Pointer<ffi.Char> description;

  /// device free memory in bytes
  @ffi.Size()
  external int memory_free;

  /// device total memory in bytes
  @ffi.Size()
  external int memory_total;

  /// device type
  @ffi.UnsignedInt()
  external int typeAsInt;

  ggml_backend_dev_type get type => ggml_backend_dev_type.fromValue(typeAsInt);

  /// device id
  /// for PCI devices, this should be the PCI bus id formatted as "domain:bus:device.function" (e.g. "0000:01:00.0")
  /// if the id is unknown, this should be NULL
  external ffi.Pointer<ffi.Char> device_id;

  /// device capabilities
  external ggml_backend_dev_caps caps;
}

typedef ggml_backend_split_buffer_type_tFunction =
    ggml_backend_buffer_type_t Function(
      ffi.Int main_device,
      ffi.Pointer<ffi.Float> tensor_split,
    );
typedef Dartggml_backend_split_buffer_type_tFunction =
    ggml_backend_buffer_type_t Function(
      int main_device,
      ffi.Pointer<ffi.Float> tensor_split,
    );

/// Split buffer type for tensor parallelism
typedef ggml_backend_split_buffer_type_t =
    ffi.Pointer<ffi.NativeFunction<ggml_backend_split_buffer_type_tFunction>>;
typedef ggml_backend_set_n_threads_tFunction =
    ffi.Void Function(ggml_backend_t backend, ffi.Int n_threads);
typedef Dartggml_backend_set_n_threads_tFunction =
    void Function(ggml_backend_t backend, int n_threads);

/// Set the number of threads for the backend
typedef ggml_backend_set_n_threads_t =
    ffi.Pointer<ffi.NativeFunction<ggml_backend_set_n_threads_tFunction>>;
typedef ggml_backend_dev_get_extra_bufts_tFunction =
    ffi.Pointer<ggml_backend_buffer_type_t> Function(ggml_backend_dev_t device);

/// Get additional buffer types provided by the device (returns a NULL-terminated array)
typedef ggml_backend_dev_get_extra_bufts_t =
    ffi.Pointer<ffi.NativeFunction<ggml_backend_dev_get_extra_bufts_tFunction>>;
typedef ggml_backend_set_abort_callback_tFunction =
    ffi.Void Function(
      ggml_backend_t backend,
      ggml_abort_callback abort_callback,
      ffi.Pointer<ffi.Void> abort_callback_data,
    );
typedef Dartggml_backend_set_abort_callback_tFunction =
    void Function(
      ggml_backend_t backend,
      ggml_abort_callback abort_callback,
      ffi.Pointer<ffi.Void> abort_callback_data,
    );

/// Set the abort callback for the backend
typedef ggml_backend_set_abort_callback_t =
    ffi.Pointer<ffi.NativeFunction<ggml_backend_set_abort_callback_tFunction>>;

/// Get a list of feature flags supported by the backend (returns a NULL-terminated array)
final class ggml_backend_feature extends ffi.Struct {
  external ffi.Pointer<ffi.Char> name;

  external ffi.Pointer<ffi.Char> value;
}

typedef ggml_backend_get_features_tFunction =
    ffi.Pointer<ggml_backend_feature> Function(ggml_backend_reg_t reg);
typedef ggml_backend_get_features_t =
    ffi.Pointer<ffi.NativeFunction<ggml_backend_get_features_tFunction>>;

final class ggml_backend_sched extends ffi.Opaque {}

/// The backend scheduler allows for multiple backend devices to be used together
/// Handles compute buffer allocation, assignment of tensors to backends, and copying of tensors between backends
/// The backends are selected based on:
/// - the backend that supports the operation
/// - the location of the pre-allocated tensors (e.g. the weights)
///     /*
///       Example usage:
///
/// operations that use tensors allocated in a buffer with USAGE_WEIGHTS will be assigned
/// preferrably to run on the same backend as the buffer
///         ggml_backend_buffer_set_usage(buf_weights, GGML_BACKEND_BUFFER_USAGE_WEIGHTS);
///
///         sched = ggml_backend_sched_new({backend_gpu, backend_gpu2, backend_cpu}, NULL, num_backends, GGML_DEFAULT_GRAPH_SIZE, false, true);
///
/// initialize buffers from a max size graph (optional)
///         reserve_graph = build_graph(sched, max_batch_size);
///
/// manually assign nodes to a backend (optional, should not be needed in most cases)
///         struct ggml_tensor * node = ggml_mul_mat(ctx, ...);
///         ggml_backend_sched_set_tensor_backend(sched, node, backend_gpu);
///
///         ggml_backend_sched_reserve(sched, reserve_graph);
///
/// compute
///         graph = build_graph(sched); // the graph and its tensors are single-use in terms of allocation, multi-use in terms of computation
///         for (int i = 0; i < 10; ++i) {
///             ggml_backend_sched_graph_compute(sched, graph); // on the first iteration the graph is allocated automatically
///         }
///
/// if there are graph inputs:
///         graph = build_graph(sched); // get a new graph that is not allocated (the metadata for the old graph is freed once ggml_free is called)
///         ggml_backend_sched_reset(sched); // clear the allocation of the previous graph
///         ggml_backend_sched_alloc_graph(sched, graph); // explicitly allocate the new graph but do not execute it
///         ggml_backend_tensor_set(input_tensor, ...); // copy data to the newly allocated graph tensors
///         ggml_backend_sched_graph_compute(sched, graph); // execute the graph
///
/// as an alternative to the above it is also possible to assign the inputs to a dedicated context and
/// allocate them statically via ggml_backend_alloc_ctx_tensors
///     }
///     */
typedef ggml_backend_sched_t = ffi.Pointer<ggml_backend_sched>;
typedef ggml_backend_sched_eval_callbackFunction =
    ffi.Bool Function(
      ffi.Pointer<ggml_tensor> t,
      ffi.Bool ask,
      ffi.Pointer<ffi.Void> user_data,
    );
typedef Dartggml_backend_sched_eval_callbackFunction =
    bool Function(
      ffi.Pointer<ggml_tensor> t,
      bool ask,
      ffi.Pointer<ffi.Void> user_data,
    );

/// Evaluation callback for each node in the graph (set with ggml_backend_sched_set_eval_callback)
/// when ask == true, the scheduler wants to know if the user wants to observe this node
/// this allows the scheduler to batch nodes together in order to evaluate them in a single call
///
/// when ask == false, the scheduler is passing the node tensor to the user for observation
/// if the user returns false, the scheduler will cancel the graph compute
typedef ggml_backend_sched_eval_callback =
    ffi.Pointer<ffi.NativeFunction<ggml_backend_sched_eval_callbackFunction>>;

/// Utils
final class ggml_backend_graph_copy$1 extends ffi.Struct {
  external ggml_backend_buffer_t buffer;

  external ffi.Pointer<ggml_context> ctx_allocated;

  external ffi.Pointer<ggml_context> ctx_unallocated;

  external ffi.Pointer<ggml_cgraph> graph;
}

typedef ggml_backend_eval_callbackFunction =
    ffi.Bool Function(
      ffi.Int node_index,
      ffi.Pointer<ggml_tensor> t1,
      ffi.Pointer<ggml_tensor> t2,
      ffi.Pointer<ffi.Void> user_data,
    );
typedef Dartggml_backend_eval_callbackFunction =
    bool Function(
      int node_index,
      ffi.Pointer<ggml_tensor> t1,
      ffi.Pointer<ggml_tensor> t2,
      ffi.Pointer<ffi.Void> user_data,
    );
typedef ggml_backend_eval_callback =
    ffi.Pointer<ffi.NativeFunction<ggml_backend_eval_callbackFunction>>;

/// the compute plan that needs to be prepared for ggml_graph_compute()
/// since https://github.com/ggml-org/ggml/issues/287
final class ggml_cplan extends ffi.Struct {
  /// size of work buffer, calculated by `ggml_graph_plan()`
  @ffi.Size()
  external int work_size;

  /// work buffer, to be allocated by caller before calling to `ggml_graph_compute()`
  external ffi.Pointer<ffi.Uint8> work_data;

  @ffi.Int()
  external int n_threads;

  external ffi.Pointer<ggml_threadpool> threadpool;

  /// abort ggml_graph_compute when true
  external ggml_abort_callback abort_callback;

  external ffi.Pointer<ffi.Void> abort_callback_data;
}

/// numa strategies
enum ggml_numa_strategy {
  GGML_NUMA_STRATEGY_DISABLED(0),
  GGML_NUMA_STRATEGY_DISTRIBUTE(1),
  GGML_NUMA_STRATEGY_ISOLATE(2),
  GGML_NUMA_STRATEGY_NUMACTL(3),
  GGML_NUMA_STRATEGY_MIRROR(4),
  GGML_NUMA_STRATEGY_COUNT(5);

  final int value;
  const ggml_numa_strategy(this.value);

  static ggml_numa_strategy fromValue(int value) => switch (value) {
    0 => GGML_NUMA_STRATEGY_DISABLED,
    1 => GGML_NUMA_STRATEGY_DISTRIBUTE,
    2 => GGML_NUMA_STRATEGY_ISOLATE,
    3 => GGML_NUMA_STRATEGY_NUMACTL,
    4 => GGML_NUMA_STRATEGY_MIRROR,
    5 => GGML_NUMA_STRATEGY_COUNT,
    _ => throw ArgumentError('Unknown value for ggml_numa_strategy: $value'),
  };
}

typedef ggml_vec_dot_tFunction =
    ffi.Void Function(
      ffi.Int n,
      ffi.Pointer<ffi.Float> s,
      ffi.Size bs,
      ffi.Pointer<ffi.Void> x,
      ffi.Size bx,
      ffi.Pointer<ffi.Void> y,
      ffi.Size by,
      ffi.Int nrc,
    );
typedef Dartggml_vec_dot_tFunction =
    void Function(
      int n,
      ffi.Pointer<ffi.Float> s,
      int bs,
      ffi.Pointer<ffi.Void> x,
      int bx,
      ffi.Pointer<ffi.Void> y,
      int by,
      int nrc,
    );

/// Internal types and functions exposed for tests and benchmarks
typedef ggml_vec_dot_t =
    ffi.Pointer<ffi.NativeFunction<ggml_vec_dot_tFunction>>;

final class ggml_type_traits_cpu extends ffi.Struct {
  external ggml_from_float_t from_float;

  external ggml_vec_dot_t vec_dot;

  @ffi.UnsignedInt()
  external int vec_dot_typeAsInt;

  ggml_type get vec_dot_type => ggml_type.fromValue(vec_dot_typeAsInt);

  /// number of rows to process simultaneously
  @ffi.Int64()
  external int nrows;
}

final class ggml_opt_dataset extends ffi.Opaque {}

final class ggml_opt_context extends ffi.Opaque {}

final class ggml_opt_result extends ffi.Opaque {}

typedef ggml_opt_dataset_t = ffi.Pointer<ggml_opt_dataset>;
typedef ggml_opt_context_t = ffi.Pointer<ggml_opt_context>;
typedef ggml_opt_result_t = ffi.Pointer<ggml_opt_result>;

/// built-in loss types, i.e. the built-in quantities minimized by the optimizer
/// custom loss types can be defined via mean or sum which simply reduce the outputs for all datapoints to a single value
enum ggml_opt_loss_type {
  GGML_OPT_LOSS_TYPE_MEAN(0),
  GGML_OPT_LOSS_TYPE_SUM(1),
  GGML_OPT_LOSS_TYPE_CROSS_ENTROPY(2),
  GGML_OPT_LOSS_TYPE_MEAN_SQUARED_ERROR(3);

  final int value;
  const ggml_opt_loss_type(this.value);

  static ggml_opt_loss_type fromValue(int value) => switch (value) {
    0 => GGML_OPT_LOSS_TYPE_MEAN,
    1 => GGML_OPT_LOSS_TYPE_SUM,
    2 => GGML_OPT_LOSS_TYPE_CROSS_ENTROPY,
    3 => GGML_OPT_LOSS_TYPE_MEAN_SQUARED_ERROR,
    _ => throw ArgumentError('Unknown value for ggml_opt_loss_type: $value'),
  };
}

/// ====== Model / Context ======
enum ggml_opt_build_type {
  GGML_OPT_BUILD_TYPE_FORWARD(10),
  GGML_OPT_BUILD_TYPE_GRAD(20),
  GGML_OPT_BUILD_TYPE_OPT(30);

  final int value;
  const ggml_opt_build_type(this.value);

  static ggml_opt_build_type fromValue(int value) => switch (value) {
    10 => GGML_OPT_BUILD_TYPE_FORWARD,
    20 => GGML_OPT_BUILD_TYPE_GRAD,
    30 => GGML_OPT_BUILD_TYPE_OPT,
    _ => throw ArgumentError('Unknown value for ggml_opt_build_type: $value'),
  };
}

enum ggml_opt_optimizer_type {
  GGML_OPT_OPTIMIZER_TYPE_ADAMW(0),
  GGML_OPT_OPTIMIZER_TYPE_SGD(1),
  GGML_OPT_OPTIMIZER_TYPE_COUNT(2);

  final int value;
  const ggml_opt_optimizer_type(this.value);

  static ggml_opt_optimizer_type fromValue(int value) => switch (value) {
    0 => GGML_OPT_OPTIMIZER_TYPE_ADAMW,
    1 => GGML_OPT_OPTIMIZER_TYPE_SGD,
    2 => GGML_OPT_OPTIMIZER_TYPE_COUNT,
    _ => throw ArgumentError(
      'Unknown value for ggml_opt_optimizer_type: $value',
    ),
  };
}

final class UnnamedStruct extends ffi.Struct {
  /// learning rate
  @ffi.Float()
  external double alpha;

  /// first AdamW momentum
  @ffi.Float()
  external double beta1;

  /// second AdamW momentum
  @ffi.Float()
  external double beta2;

  /// epsilon for numerical stability
  @ffi.Float()
  external double eps;

  /// weight decay - 0.0f to disable
  @ffi.Float()
  external double wd;
}

final class UnnamedStruct$1 extends ffi.Struct {
  /// learning rate
  @ffi.Float()
  external double alpha;

  /// weight decay
  @ffi.Float()
  external double wd;
}

/// parameters that control which optimizer is used and how said optimizer tries to find the minimal loss
final class ggml_opt_optimizer_params extends ffi.Struct {
  external UnnamedStruct adamw;

  external UnnamedStruct$1 sgd;
}

typedef ggml_opt_get_optimizer_paramsFunction =
    ggml_opt_optimizer_params Function(ffi.Pointer<ffi.Void> userdata);

/// callback to calculate optimizer parameters prior to a backward pass
/// userdata can be used to pass arbitrary data
typedef ggml_opt_get_optimizer_params =
    ffi.Pointer<ffi.NativeFunction<ggml_opt_get_optimizer_paramsFunction>>;

/// parameters for initializing a new optimization context
final class ggml_opt_params extends ffi.Struct {
  /// defines which backends are used to construct the compute graphs
  external ggml_backend_sched_t backend_sched;

  /// by default the forward graph needs to be reconstructed for each eval
  /// if ctx_compute, inputs, and outputs are set the graphs are instead allocated statically
  external ffi.Pointer<ggml_context> ctx_compute;

  external ffi.Pointer<ggml_tensor> inputs;

  external ffi.Pointer<ggml_tensor> outputs;

  @ffi.UnsignedInt()
  external int loss_typeAsInt;

  ggml_opt_loss_type get loss_type =>
      ggml_opt_loss_type.fromValue(loss_typeAsInt);

  @ffi.UnsignedInt()
  external int build_typeAsInt;

  ggml_opt_build_type get build_type =>
      ggml_opt_build_type.fromValue(build_typeAsInt);

  /// after how many gradient accumulation steps an optimizer step should be done
  @ffi.Int32()
  external int opt_period;

  /// callback for calculating optimizer parameters
  external ggml_opt_get_optimizer_params get_opt_pars;

  /// userdata for calculating optimizer parameters
  external ffi.Pointer<ffi.Void> get_opt_pars_ud;

  /// only GGML_OPT_OPTIMIZER_TYPE_ADAMW needs m, v momenta per parameter tensor
  @ffi.UnsignedInt()
  external int optimizerAsInt;

  ggml_opt_optimizer_type get optimizer =>
      ggml_opt_optimizer_type.fromValue(optimizerAsInt);
}

typedef ggml_opt_epoch_callbackFunction =
    ffi.Void Function(
      ffi.Bool train,
      ggml_opt_context_t opt_ctx,
      ggml_opt_dataset_t dataset,
      ggml_opt_result_t result,
      ffi.Int64 ibatch,
      ffi.Int64 ibatch_max,
      ffi.Int64 t_start_us,
    );
typedef Dartggml_opt_epoch_callbackFunction =
    void Function(
      bool train,
      ggml_opt_context_t opt_ctx,
      ggml_opt_dataset_t dataset,
      ggml_opt_result_t result,
      int ibatch,
      int ibatch_max,
      int t_start_us,
    );

/// signature for a callback while evaluating opt_ctx on dataset, called after an evaluation
typedef ggml_opt_epoch_callback =
    ffi.Pointer<ffi.NativeFunction<ggml_opt_epoch_callbackFunction>>;

/// C interface
///
/// TODO: show sample usage
final class llama_vocab extends ffi.Opaque {}

final class llama_model extends ffi.Opaque {}

final class llama_context extends ffi.Opaque {}

typedef llama_token = ffi.Int32;
typedef Dartllama_token = int;

/// TODO: simplify (https://github.com/ggml-org/llama.cpp/pull/9294#pullrequestreview-2286561979)
final class llama_token_data extends ffi.Struct {
  /// token id
  @llama_token()
  external int id;

  /// log-odds of the token
  @ffi.Float()
  external double logit;

  /// probability of the token
  @ffi.Float()
  external double p;
}

final class llama_token_data_array extends ffi.Struct {
  /// TODO: consider SoA
  /// NOTE: this pointer can be modified by the samplers
  external ffi.Pointer<llama_token_data> data;

  @ffi.Size()
  external int size;

  /// this is the index in the data array (i.e. not the token id)
  @ffi.Int64()
  external int selected;

  /// note: do not assume the data is sorted - always check this flag
  @ffi.Bool()
  external bool sorted;
}

/// user code can implement the interface below in order to create custom llama_sampler
final class llama_sampler_i extends ffi.Struct {
  /// can be NULL
  external ffi.Pointer<
    ffi.NativeFunction<
      ffi.Pointer<ffi.Char> Function(ffi.Pointer<llama_sampler> smpl)
    >
  >
  name;

  /// can be NULL
  external ffi.Pointer<
    ffi.NativeFunction<
      ffi.Void Function(ffi.Pointer<llama_sampler> smpl, llama_token token)
    >
  >
  accept;

  /// required
  external ffi.Pointer<
    ffi.NativeFunction<
      ffi.Void Function(
        ffi.Pointer<llama_sampler> smpl,
        ffi.Pointer<llama_token_data_array> cur_p,
      )
    >
  >
  apply;

  /// can be NULL
  external ffi.Pointer<
    ffi.NativeFunction<ffi.Void Function(ffi.Pointer<llama_sampler> smpl)>
  >
  reset;

  /// can be NULL if ctx is NULL
  external ffi.Pointer<
    ffi.NativeFunction<
      ffi.Pointer<llama_sampler> Function(ffi.Pointer<llama_sampler> smpl)
    >
  >
  clone;

  /// can be NULL if ctx is NULL
  external ffi.Pointer<
    ffi.NativeFunction<ffi.Void Function(ffi.Pointer<llama_sampler> smpl)>
  >
  free;
}

/// Sampling API
///
/// Sample usage:
///
/// // prepare the sampling chain at the start
/// auto sparams = llama_sampler_chain_default_params();
///
/// llama_sampler * smpl = llama_sampler_chain_init(sparams);
///
/// llama_sampler_chain_add(smpl, llama_sampler_init_top_k(50));
/// llama_sampler_chain_add(smpl, llama_sampler_init_top_p(0.9, 1));
/// llama_sampler_chain_add(smpl, llama_sampler_init_temp (0.8));
///
/// // typically, the chain should end with a sampler such as "greedy", "dist" or "mirostat"
/// // this sampler will be responsible to select the actual token
/// llama_sampler_chain_add(smpl, llama_sampler_init_dist(seed));
///
/// ...
///
/// // decoding loop:
/// while (...) {
/// ...
///
/// llama_decode(ctx, batch);
///
/// // sample from the logits of the last token in the batch
/// const llama_token id = llama_sampler_sample(smpl, ctx, -1);
///
/// ...
/// }
///
/// llama_sampler_free(smpl);
///
/// TODO: In the future, llama_sampler will be utilized to offload the sampling to the backends (e.g. GPU).
typedef llama_sampler_context_t = ffi.Pointer<ffi.Void>;

final class llama_sampler extends ffi.Struct {
  external ffi.Pointer<llama_sampler_i> iface;

  external llama_sampler_context_t ctx;
}

final class llama_memory_i extends ffi.Opaque {}

typedef llama_memory_t = ffi.Pointer<llama_memory_i>;
typedef llama_pos = ffi.Int32;
typedef Dartllama_pos = int;
typedef llama_seq_id = ffi.Int32;
typedef Dartllama_seq_id = int;

enum llama_vocab_type {
  /// For models without vocab
  LLAMA_VOCAB_TYPE_NONE(0),

  /// LLaMA tokenizer based on byte-level BPE with byte fallback
  LLAMA_VOCAB_TYPE_SPM(1),

  /// GPT-2 tokenizer based on byte-level BPE
  LLAMA_VOCAB_TYPE_BPE(2),

  /// BERT tokenizer based on WordPiece
  LLAMA_VOCAB_TYPE_WPM(3),

  /// T5 tokenizer based on Unigram
  LLAMA_VOCAB_TYPE_UGM(4),

  /// RWKV tokenizer based on greedy tokenization
  LLAMA_VOCAB_TYPE_RWKV(5),

  /// PLaMo-2 tokenizer based on Aho-Corasick with dynamic programming
  LLAMA_VOCAB_TYPE_PLAMO2(6);

  final int value;
  const llama_vocab_type(this.value);

  static llama_vocab_type fromValue(int value) => switch (value) {
    0 => LLAMA_VOCAB_TYPE_NONE,
    1 => LLAMA_VOCAB_TYPE_SPM,
    2 => LLAMA_VOCAB_TYPE_BPE,
    3 => LLAMA_VOCAB_TYPE_WPM,
    4 => LLAMA_VOCAB_TYPE_UGM,
    5 => LLAMA_VOCAB_TYPE_RWKV,
    6 => LLAMA_VOCAB_TYPE_PLAMO2,
    _ => throw ArgumentError('Unknown value for llama_vocab_type: $value'),
  };
}

enum llama_rope_type {
  LLAMA_ROPE_TYPE_NONE(-1),
  LLAMA_ROPE_TYPE_NORM(0),
  LLAMA_ROPE_TYPE_NEOX(2),
  LLAMA_ROPE_TYPE_MROPE(8),
  LLAMA_ROPE_TYPE_IMROPE(40),
  LLAMA_ROPE_TYPE_VISION(24);

  final int value;
  const llama_rope_type(this.value);

  static llama_rope_type fromValue(int value) => switch (value) {
    -1 => LLAMA_ROPE_TYPE_NONE,
    0 => LLAMA_ROPE_TYPE_NORM,
    2 => LLAMA_ROPE_TYPE_NEOX,
    8 => LLAMA_ROPE_TYPE_MROPE,
    40 => LLAMA_ROPE_TYPE_IMROPE,
    24 => LLAMA_ROPE_TYPE_VISION,
    _ => throw ArgumentError('Unknown value for llama_rope_type: $value'),
  };
}

enum llama_token_type {
  LLAMA_TOKEN_TYPE_UNDEFINED(0),
  LLAMA_TOKEN_TYPE_NORMAL(1),
  LLAMA_TOKEN_TYPE_UNKNOWN(2),
  LLAMA_TOKEN_TYPE_CONTROL(3),
  LLAMA_TOKEN_TYPE_USER_DEFINED(4),
  LLAMA_TOKEN_TYPE_UNUSED(5),
  LLAMA_TOKEN_TYPE_BYTE(6);

  final int value;
  const llama_token_type(this.value);

  static llama_token_type fromValue(int value) => switch (value) {
    0 => LLAMA_TOKEN_TYPE_UNDEFINED,
    1 => LLAMA_TOKEN_TYPE_NORMAL,
    2 => LLAMA_TOKEN_TYPE_UNKNOWN,
    3 => LLAMA_TOKEN_TYPE_CONTROL,
    4 => LLAMA_TOKEN_TYPE_USER_DEFINED,
    5 => LLAMA_TOKEN_TYPE_UNUSED,
    6 => LLAMA_TOKEN_TYPE_BYTE,
    _ => throw ArgumentError('Unknown value for llama_token_type: $value'),
  };
}

enum llama_token_attr {
  LLAMA_TOKEN_ATTR_UNDEFINED(0),
  LLAMA_TOKEN_ATTR_UNKNOWN(1),
  LLAMA_TOKEN_ATTR_UNUSED(2),
  LLAMA_TOKEN_ATTR_NORMAL(4),

  /// SPECIAL?
  LLAMA_TOKEN_ATTR_CONTROL(8),
  LLAMA_TOKEN_ATTR_USER_DEFINED(16),
  LLAMA_TOKEN_ATTR_BYTE(32),
  LLAMA_TOKEN_ATTR_NORMALIZED(64),
  LLAMA_TOKEN_ATTR_LSTRIP(128),
  LLAMA_TOKEN_ATTR_RSTRIP(256),
  LLAMA_TOKEN_ATTR_SINGLE_WORD(512);

  final int value;
  const llama_token_attr(this.value);

  static llama_token_attr fromValue(int value) => switch (value) {
    0 => LLAMA_TOKEN_ATTR_UNDEFINED,
    1 => LLAMA_TOKEN_ATTR_UNKNOWN,
    2 => LLAMA_TOKEN_ATTR_UNUSED,
    4 => LLAMA_TOKEN_ATTR_NORMAL,
    8 => LLAMA_TOKEN_ATTR_CONTROL,
    16 => LLAMA_TOKEN_ATTR_USER_DEFINED,
    32 => LLAMA_TOKEN_ATTR_BYTE,
    64 => LLAMA_TOKEN_ATTR_NORMALIZED,
    128 => LLAMA_TOKEN_ATTR_LSTRIP,
    256 => LLAMA_TOKEN_ATTR_RSTRIP,
    512 => LLAMA_TOKEN_ATTR_SINGLE_WORD,
    _ => throw ArgumentError('Unknown value for llama_token_attr: $value'),
  };
}

/// model file types
enum llama_ftype {
  LLAMA_FTYPE_ALL_F32(0),

  /// except 1d tensors
  LLAMA_FTYPE_MOSTLY_F16(1),

  /// except 1d tensors
  LLAMA_FTYPE_MOSTLY_Q4_0(2),

  /// except 1d tensors
  LLAMA_FTYPE_MOSTLY_Q4_1(3),

  /// except 1d tensors
  LLAMA_FTYPE_MOSTLY_Q8_0(7),

  /// except 1d tensors
  LLAMA_FTYPE_MOSTLY_Q5_0(8),

  /// except 1d tensors
  LLAMA_FTYPE_MOSTLY_Q5_1(9),

  /// except 1d tensors
  LLAMA_FTYPE_MOSTLY_Q2_K(10),

  /// except 1d tensors
  LLAMA_FTYPE_MOSTLY_Q3_K_S(11),

  /// except 1d tensors
  LLAMA_FTYPE_MOSTLY_Q3_K_M(12),

  /// except 1d tensors
  LLAMA_FTYPE_MOSTLY_Q3_K_L(13),

  /// except 1d tensors
  LLAMA_FTYPE_MOSTLY_Q4_K_S(14),

  /// except 1d tensors
  LLAMA_FTYPE_MOSTLY_Q4_K_M(15),

  /// except 1d tensors
  LLAMA_FTYPE_MOSTLY_Q5_K_S(16),

  /// except 1d tensors
  LLAMA_FTYPE_MOSTLY_Q5_K_M(17),

  /// except 1d tensors
  LLAMA_FTYPE_MOSTLY_Q6_K(18),

  /// except 1d tensors
  LLAMA_FTYPE_MOSTLY_IQ2_XXS(19),

  /// except 1d tensors
  LLAMA_FTYPE_MOSTLY_IQ2_XS(20),

  /// except 1d tensors
  LLAMA_FTYPE_MOSTLY_Q2_K_S(21),

  /// except 1d tensors
  LLAMA_FTYPE_MOSTLY_IQ3_XS(22),

  /// except 1d tensors
  LLAMA_FTYPE_MOSTLY_IQ3_XXS(23),

  /// except 1d tensors
  LLAMA_FTYPE_MOSTLY_IQ1_S(24),

  /// except 1d tensors
  LLAMA_FTYPE_MOSTLY_IQ4_NL(25),

  /// except 1d tensors
  LLAMA_FTYPE_MOSTLY_IQ3_S(26),

  /// except 1d tensors
  LLAMA_FTYPE_MOSTLY_IQ3_M(27),

  /// except 1d tensors
  LLAMA_FTYPE_MOSTLY_IQ2_S(28),

  /// except 1d tensors
  LLAMA_FTYPE_MOSTLY_IQ2_M(29),

  /// except 1d tensors
  LLAMA_FTYPE_MOSTLY_IQ4_XS(30),

  /// except 1d tensors
  LLAMA_FTYPE_MOSTLY_IQ1_M(31),

  /// except 1d tensors
  LLAMA_FTYPE_MOSTLY_BF16(32),

  /// except 1d tensors
  LLAMA_FTYPE_MOSTLY_TQ1_0(36),

  /// except 1d tensors
  LLAMA_FTYPE_MOSTLY_TQ2_0(37),

  /// except 1d tensors
  LLAMA_FTYPE_MOSTLY_MXFP4_MOE(38),

  /// not specified in the model file
  LLAMA_FTYPE_GUESSED(1024);

  final int value;
  const llama_ftype(this.value);

  static llama_ftype fromValue(int value) => switch (value) {
    0 => LLAMA_FTYPE_ALL_F32,
    1 => LLAMA_FTYPE_MOSTLY_F16,
    2 => LLAMA_FTYPE_MOSTLY_Q4_0,
    3 => LLAMA_FTYPE_MOSTLY_Q4_1,
    7 => LLAMA_FTYPE_MOSTLY_Q8_0,
    8 => LLAMA_FTYPE_MOSTLY_Q5_0,
    9 => LLAMA_FTYPE_MOSTLY_Q5_1,
    10 => LLAMA_FTYPE_MOSTLY_Q2_K,
    11 => LLAMA_FTYPE_MOSTLY_Q3_K_S,
    12 => LLAMA_FTYPE_MOSTLY_Q3_K_M,
    13 => LLAMA_FTYPE_MOSTLY_Q3_K_L,
    14 => LLAMA_FTYPE_MOSTLY_Q4_K_S,
    15 => LLAMA_FTYPE_MOSTLY_Q4_K_M,
    16 => LLAMA_FTYPE_MOSTLY_Q5_K_S,
    17 => LLAMA_FTYPE_MOSTLY_Q5_K_M,
    18 => LLAMA_FTYPE_MOSTLY_Q6_K,
    19 => LLAMA_FTYPE_MOSTLY_IQ2_XXS,
    20 => LLAMA_FTYPE_MOSTLY_IQ2_XS,
    21 => LLAMA_FTYPE_MOSTLY_Q2_K_S,
    22 => LLAMA_FTYPE_MOSTLY_IQ3_XS,
    23 => LLAMA_FTYPE_MOSTLY_IQ3_XXS,
    24 => LLAMA_FTYPE_MOSTLY_IQ1_S,
    25 => LLAMA_FTYPE_MOSTLY_IQ4_NL,
    26 => LLAMA_FTYPE_MOSTLY_IQ3_S,
    27 => LLAMA_FTYPE_MOSTLY_IQ3_M,
    28 => LLAMA_FTYPE_MOSTLY_IQ2_S,
    29 => LLAMA_FTYPE_MOSTLY_IQ2_M,
    30 => LLAMA_FTYPE_MOSTLY_IQ4_XS,
    31 => LLAMA_FTYPE_MOSTLY_IQ1_M,
    32 => LLAMA_FTYPE_MOSTLY_BF16,
    36 => LLAMA_FTYPE_MOSTLY_TQ1_0,
    37 => LLAMA_FTYPE_MOSTLY_TQ2_0,
    38 => LLAMA_FTYPE_MOSTLY_MXFP4_MOE,
    1024 => LLAMA_FTYPE_GUESSED,
    _ => throw ArgumentError('Unknown value for llama_ftype: $value'),
  };
}

enum llama_rope_scaling_type {
  LLAMA_ROPE_SCALING_TYPE_UNSPECIFIED(-1),
  LLAMA_ROPE_SCALING_TYPE_NONE(0),
  LLAMA_ROPE_SCALING_TYPE_LINEAR(1),
  LLAMA_ROPE_SCALING_TYPE_YARN(2),
  LLAMA_ROPE_SCALING_TYPE_LONGROPE(3);

  static const LLAMA_ROPE_SCALING_TYPE_MAX_VALUE =
      LLAMA_ROPE_SCALING_TYPE_LONGROPE;

  final int value;
  const llama_rope_scaling_type(this.value);

  static llama_rope_scaling_type fromValue(int value) => switch (value) {
    -1 => LLAMA_ROPE_SCALING_TYPE_UNSPECIFIED,
    0 => LLAMA_ROPE_SCALING_TYPE_NONE,
    1 => LLAMA_ROPE_SCALING_TYPE_LINEAR,
    2 => LLAMA_ROPE_SCALING_TYPE_YARN,
    3 => LLAMA_ROPE_SCALING_TYPE_LONGROPE,
    _ => throw ArgumentError(
      'Unknown value for llama_rope_scaling_type: $value',
    ),
  };

  @override
  String toString() {
    if (this == LLAMA_ROPE_SCALING_TYPE_LONGROPE)
      return "llama_rope_scaling_type.LLAMA_ROPE_SCALING_TYPE_LONGROPE, llama_rope_scaling_type.LLAMA_ROPE_SCALING_TYPE_MAX_VALUE";
    return super.toString();
  }
}

enum llama_pooling_type {
  LLAMA_POOLING_TYPE_UNSPECIFIED(-1),
  LLAMA_POOLING_TYPE_NONE(0),
  LLAMA_POOLING_TYPE_MEAN(1),
  LLAMA_POOLING_TYPE_CLS(2),
  LLAMA_POOLING_TYPE_LAST(3),

  /// used by reranking models to attach the classification head to the graph
  LLAMA_POOLING_TYPE_RANK(4);

  final int value;
  const llama_pooling_type(this.value);

  static llama_pooling_type fromValue(int value) => switch (value) {
    -1 => LLAMA_POOLING_TYPE_UNSPECIFIED,
    0 => LLAMA_POOLING_TYPE_NONE,
    1 => LLAMA_POOLING_TYPE_MEAN,
    2 => LLAMA_POOLING_TYPE_CLS,
    3 => LLAMA_POOLING_TYPE_LAST,
    4 => LLAMA_POOLING_TYPE_RANK,
    _ => throw ArgumentError('Unknown value for llama_pooling_type: $value'),
  };
}

enum llama_attention_type {
  LLAMA_ATTENTION_TYPE_UNSPECIFIED(-1),
  LLAMA_ATTENTION_TYPE_CAUSAL(0),
  LLAMA_ATTENTION_TYPE_NON_CAUSAL(1);

  final int value;
  const llama_attention_type(this.value);

  static llama_attention_type fromValue(int value) => switch (value) {
    -1 => LLAMA_ATTENTION_TYPE_UNSPECIFIED,
    0 => LLAMA_ATTENTION_TYPE_CAUSAL,
    1 => LLAMA_ATTENTION_TYPE_NON_CAUSAL,
    _ => throw ArgumentError('Unknown value for llama_attention_type: $value'),
  };
}

enum llama_flash_attn_type {
  LLAMA_FLASH_ATTN_TYPE_AUTO(-1),
  LLAMA_FLASH_ATTN_TYPE_DISABLED(0),
  LLAMA_FLASH_ATTN_TYPE_ENABLED(1);

  final int value;
  const llama_flash_attn_type(this.value);

  static llama_flash_attn_type fromValue(int value) => switch (value) {
    -1 => LLAMA_FLASH_ATTN_TYPE_AUTO,
    0 => LLAMA_FLASH_ATTN_TYPE_DISABLED,
    1 => LLAMA_FLASH_ATTN_TYPE_ENABLED,
    _ => throw ArgumentError('Unknown value for llama_flash_attn_type: $value'),
  };
}

enum llama_split_mode {
  /// single GPU
  LLAMA_SPLIT_MODE_NONE(0),

  /// split layers and KV across GPUs
  LLAMA_SPLIT_MODE_LAYER(1),

  /// split layers and KV across GPUs, use tensor parallelism if supported
  LLAMA_SPLIT_MODE_ROW(2);

  final int value;
  const llama_split_mode(this.value);

  static llama_split_mode fromValue(int value) => switch (value) {
    0 => LLAMA_SPLIT_MODE_NONE,
    1 => LLAMA_SPLIT_MODE_LAYER,
    2 => LLAMA_SPLIT_MODE_ROW,
    _ => throw ArgumentError('Unknown value for llama_split_mode: $value'),
  };
}

typedef llama_progress_callbackFunction =
    ffi.Bool Function(ffi.Float progress, ffi.Pointer<ffi.Void> user_data);
typedef Dartllama_progress_callbackFunction =
    bool Function(double progress, ffi.Pointer<ffi.Void> user_data);
typedef llama_progress_callback =
    ffi.Pointer<ffi.NativeFunction<llama_progress_callbackFunction>>;

/// Input data for llama_encode/llama_decode
/// A llama_batch object can contain input about one or many sequences
/// The provided arrays (i.e. token, embd, pos, etc.) must have size of n_tokens
///
/// - token  : the token ids of the input (used when embd is NULL)
/// - embd   : token embeddings (i.e. float vector of size n_embd) (used when token is NULL)
/// - pos    : the positions of the respective token in the sequence
/// (if set to NULL, the token position will be tracked automatically by llama_encode/llama_decode)
/// - seq_id : the sequence to which the respective token belongs
/// (if set to NULL, the sequence ID will be assumed to be 0)
/// - logits : if zero, the logits (and/or the embeddings) for the respective token will not be output
/// (if set to NULL:
/// - if embeddings: all tokens are output
/// - if not:        only the last token is output
/// )
final class llama_batch extends ffi.Struct {
  @ffi.Int32()
  external int n_tokens;

  external ffi.Pointer<llama_token> token;

  external ffi.Pointer<ffi.Float> embd;

  external ffi.Pointer<llama_pos> pos;

  external ffi.Pointer<ffi.Int32> n_seq_id;

  external ffi.Pointer<ffi.Pointer<llama_seq_id>> seq_id;

  /// TODO: rename this to "output"
  external ffi.Pointer<ffi.Int8> logits;
}

enum llama_model_kv_override_type {
  LLAMA_KV_OVERRIDE_TYPE_INT(0),
  LLAMA_KV_OVERRIDE_TYPE_FLOAT(1),
  LLAMA_KV_OVERRIDE_TYPE_BOOL(2),
  LLAMA_KV_OVERRIDE_TYPE_STR(3);

  final int value;
  const llama_model_kv_override_type(this.value);

  static llama_model_kv_override_type fromValue(int value) => switch (value) {
    0 => LLAMA_KV_OVERRIDE_TYPE_INT,
    1 => LLAMA_KV_OVERRIDE_TYPE_FLOAT,
    2 => LLAMA_KV_OVERRIDE_TYPE_BOOL,
    3 => LLAMA_KV_OVERRIDE_TYPE_STR,
    _ => throw ArgumentError(
      'Unknown value for llama_model_kv_override_type: $value',
    ),
  };
}

enum llama_model_meta_key {
  LLAMA_MODEL_META_KEY_SAMPLING_SEQUENCE(0),
  LLAMA_MODEL_META_KEY_SAMPLING_TOP_K(1),
  LLAMA_MODEL_META_KEY_SAMPLING_TOP_P(2),
  LLAMA_MODEL_META_KEY_SAMPLING_MIN_P(3),
  LLAMA_MODEL_META_KEY_SAMPLING_XTC_PROBABILITY(4),
  LLAMA_MODEL_META_KEY_SAMPLING_XTC_THRESHOLD(5),
  LLAMA_MODEL_META_KEY_SAMPLING_TEMP(6),
  LLAMA_MODEL_META_KEY_SAMPLING_PENALTY_LAST_N(7),
  LLAMA_MODEL_META_KEY_SAMPLING_PENALTY_REPEAT(8),
  LLAMA_MODEL_META_KEY_SAMPLING_MIROSTAT(9),
  LLAMA_MODEL_META_KEY_SAMPLING_MIROSTAT_TAU(10),
  LLAMA_MODEL_META_KEY_SAMPLING_MIROSTAT_ETA(11);

  final int value;
  const llama_model_meta_key(this.value);

  static llama_model_meta_key fromValue(int value) => switch (value) {
    0 => LLAMA_MODEL_META_KEY_SAMPLING_SEQUENCE,
    1 => LLAMA_MODEL_META_KEY_SAMPLING_TOP_K,
    2 => LLAMA_MODEL_META_KEY_SAMPLING_TOP_P,
    3 => LLAMA_MODEL_META_KEY_SAMPLING_MIN_P,
    4 => LLAMA_MODEL_META_KEY_SAMPLING_XTC_PROBABILITY,
    5 => LLAMA_MODEL_META_KEY_SAMPLING_XTC_THRESHOLD,
    6 => LLAMA_MODEL_META_KEY_SAMPLING_TEMP,
    7 => LLAMA_MODEL_META_KEY_SAMPLING_PENALTY_LAST_N,
    8 => LLAMA_MODEL_META_KEY_SAMPLING_PENALTY_REPEAT,
    9 => LLAMA_MODEL_META_KEY_SAMPLING_MIROSTAT,
    10 => LLAMA_MODEL_META_KEY_SAMPLING_MIROSTAT_TAU,
    11 => LLAMA_MODEL_META_KEY_SAMPLING_MIROSTAT_ETA,
    _ => throw ArgumentError('Unknown value for llama_model_meta_key: $value'),
  };
}

final class UnnamedUnion extends ffi.Union {
  @ffi.Int64()
  external int val_i64;

  @ffi.Double()
  external double val_f64;

  @ffi.Bool()
  external bool val_bool;

  @ffi.Array.multi([128])
  external ffi.Array<ffi.Char> val_str;
}

final class llama_model_kv_override extends ffi.Struct {
  @ffi.UnsignedInt()
  external int tagAsInt;

  llama_model_kv_override_type get tag =>
      llama_model_kv_override_type.fromValue(tagAsInt);

  @ffi.Array.multi([128])
  external ffi.Array<ffi.Char> key;

  external UnnamedUnion unnamed;
}

final class llama_model_tensor_buft_override extends ffi.Struct {
  external ffi.Pointer<ffi.Char> pattern;

  external ggml_backend_buffer_type_t buft;
}

final class llama_model_params extends ffi.Struct {
  /// NULL-terminated list of devices to use for offloading (if NULL, all available devices are used)
  external ffi.Pointer<ggml_backend_dev_t> devices;

  /// NULL-terminated list of buffer types to use for tensors that match a pattern
  external ffi.Pointer<llama_model_tensor_buft_override> tensor_buft_overrides;

  /// number of layers to store in VRAM
  @ffi.Int32()
  external int n_gpu_layers;

  /// how to split the model across multiple GPUs
  @ffi.UnsignedInt()
  external int split_modeAsInt;

  llama_split_mode get split_mode =>
      llama_split_mode.fromValue(split_modeAsInt);

  /// the GPU that is used for the entire model when split_mode is LLAMA_SPLIT_MODE_NONE
  @ffi.Int32()
  external int main_gpu;

  /// proportion of the model (layers or rows) to offload to each GPU, size: llama_max_devices()
  external ffi.Pointer<ffi.Float> tensor_split;

  /// Called with a progress value between 0.0 and 1.0. Pass NULL to disable.
  /// If the provided progress_callback returns true, model loading continues.
  /// If it returns false, model loading is immediately aborted.
  external llama_progress_callback progress_callback;

  /// context pointer passed to the progress callback
  external ffi.Pointer<ffi.Void> progress_callback_user_data;

  /// override key-value pairs of the model meta data
  external ffi.Pointer<llama_model_kv_override> kv_overrides;

  /// only load the vocabulary, no weights
  @ffi.Bool()
  external bool vocab_only;

  /// use mmap if possible
  @ffi.Bool()
  external bool use_mmap;

  /// force system to keep model in RAM
  @ffi.Bool()
  external bool use_mlock;

  /// validate model tensor data
  @ffi.Bool()
  external bool check_tensors;

  /// use extra buffer types (used for weight repacking)
  @ffi.Bool()
  external bool use_extra_bufts;

  /// bypass host buffer allowing extra buffers to be used
  @ffi.Bool()
  external bool no_host;
}

/// NOTE: changing the default values of parameters marked as [EXPERIMENTAL] may cause crashes or incorrect results in certain configurations
/// https://github.com/ggml-org/llama.cpp/pull/7544
final class llama_context_params extends ffi.Struct {
  /// text context, 0 = from model
  @ffi.Uint32()
  external int n_ctx;

  /// logical maximum batch size that can be submitted to llama_decode
  @ffi.Uint32()
  external int n_batch;

  /// physical maximum batch size
  @ffi.Uint32()
  external int n_ubatch;

  /// max number of sequences (i.e. distinct states for recurrent models)
  @ffi.Uint32()
  external int n_seq_max;

  /// number of threads to use for generation
  @ffi.Int32()
  external int n_threads;

  /// number of threads to use for batch processing
  @ffi.Int32()
  external int n_threads_batch;

  /// RoPE scaling type, from `enum llama_rope_scaling_type`
  @ffi.Int()
  external int rope_scaling_typeAsInt;

  llama_rope_scaling_type get rope_scaling_type =>
      llama_rope_scaling_type.fromValue(rope_scaling_typeAsInt);

  /// whether to pool (sum) embedding results by sequence id
  @ffi.Int()
  external int pooling_typeAsInt;

  llama_pooling_type get pooling_type =>
      llama_pooling_type.fromValue(pooling_typeAsInt);

  /// attention type to use for embeddings
  @ffi.Int()
  external int attention_typeAsInt;

  llama_attention_type get attention_type =>
      llama_attention_type.fromValue(attention_typeAsInt);

  /// when to enable Flash Attention
  @ffi.Int()
  external int flash_attn_typeAsInt;

  llama_flash_attn_type get flash_attn_type =>
      llama_flash_attn_type.fromValue(flash_attn_typeAsInt);

  /// RoPE base frequency, 0 = from model
  @ffi.Float()
  external double rope_freq_base;

  /// RoPE frequency scaling factor, 0 = from model
  @ffi.Float()
  external double rope_freq_scale;

  /// YaRN extrapolation mix factor, negative = from model
  @ffi.Float()
  external double yarn_ext_factor;

  /// YaRN magnitude scaling factor
  @ffi.Float()
  external double yarn_attn_factor;

  /// YaRN low correction dim
  @ffi.Float()
  external double yarn_beta_fast;

  /// YaRN high correction dim
  @ffi.Float()
  external double yarn_beta_slow;

  /// YaRN original context size
  @ffi.Uint32()
  external int yarn_orig_ctx;

  /// [DEPRECATED] defragment the KV cache if holes/size > thold, <= 0 disabled (default)
  @ffi.Float()
  external double defrag_thold;

  external ggml_backend_sched_eval_callback cb_eval;

  external ffi.Pointer<ffi.Void> cb_eval_user_data;

  /// data type for K cache [EXPERIMENTAL]
  @ffi.UnsignedInt()
  external int type_kAsInt;

  ggml_type get type_k => ggml_type.fromValue(type_kAsInt);

  /// data type for V cache [EXPERIMENTAL]
  @ffi.UnsignedInt()
  external int type_vAsInt;

  ggml_type get type_v => ggml_type.fromValue(type_vAsInt);

  /// Abort callback
  /// if it returns true, execution of llama_decode() will be aborted
  /// currently works only with CPU execution
  external ggml_abort_callback abort_callback;

  external ffi.Pointer<ffi.Void> abort_callback_data;

  /// if true, extract embeddings (together with logits)
  @ffi.Bool()
  external bool embeddings;

  /// offload the KQV ops (including the KV cache) to GPU
  @ffi.Bool()
  external bool offload_kqv;

  /// measure performance timings
  @ffi.Bool()
  external bool no_perf;

  /// offload host tensor operations to device
  @ffi.Bool()
  external bool op_offload;

  /// use full-size SWA cache (https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
  /// NOTE: setting to false when n_seq_max > 1 can cause bad performance in some cases
  /// ref: https://github.com/ggml-org/llama.cpp/pull/13845#issuecomment-2924800573
  @ffi.Bool()
  external bool swa_full;

  /// use a unified buffer across the input sequences when computing the attention
  /// try to disable when n_seq_max > 1 for improved performance when the sequences do not share a large prefix
  /// ref: https://github.com/ggml-org/llama.cpp/pull/14363
  @ffi.Bool()
  external bool kv_unified;
}

/// model quantization parameters
final class llama_model_quantize_params extends ffi.Struct {
  /// number of threads to use for quantizing, if <=0 will use std::thread::hardware_concurrency()
  @ffi.Int32()
  external int nthread;

  /// quantize to this llama_ftype
  @ffi.UnsignedInt()
  external int ftypeAsInt;

  llama_ftype get ftype => llama_ftype.fromValue(ftypeAsInt);

  /// output tensor type
  @ffi.UnsignedInt()
  external int output_tensor_typeAsInt;

  ggml_type get output_tensor_type =>
      ggml_type.fromValue(output_tensor_typeAsInt);

  /// token embeddings tensor type
  @ffi.UnsignedInt()
  external int token_embedding_typeAsInt;

  ggml_type get token_embedding_type =>
      ggml_type.fromValue(token_embedding_typeAsInt);

  /// allow quantizing non-f32/f16 tensors
  @ffi.Bool()
  external bool allow_requantize;

  /// quantize output.weight
  @ffi.Bool()
  external bool quantize_output_tensor;

  /// only copy tensors - ftype, allow_requantize and quantize_output_tensor are ignored
  @ffi.Bool()
  external bool only_copy;

  /// quantize all tensors to the default type
  @ffi.Bool()
  external bool pure;

  /// quantize to the same number of shards
  @ffi.Bool()
  external bool keep_split;

  /// pointer to importance matrix data
  external ffi.Pointer<ffi.Void> imatrix;

  /// pointer to vector containing overrides
  external ffi.Pointer<ffi.Void> kv_overrides;

  /// pointer to vector containing tensor types
  external ffi.Pointer<ffi.Void> tensor_types;

  /// pointer to vector containing layer indices to prune
  external ffi.Pointer<ffi.Void> prune_layers;
}

final class llama_logit_bias extends ffi.Struct {
  @llama_token()
  external int token;

  @ffi.Float()
  external double bias;
}

final class llama_sampler_chain_params extends ffi.Struct {
  /// whether to measure performance timings
  @ffi.Bool()
  external bool no_perf;
}

/// used in chat template
final class llama_chat_message extends ffi.Struct {
  external ffi.Pointer<ffi.Char> role;

  external ffi.Pointer<ffi.Char> content;
}

/// lora adapter
final class llama_adapter_lora extends ffi.Opaque {}

typedef llama_state_seq_flags = ffi.Uint32;
typedef Dartllama_state_seq_flags = int;

/// Performance utils
///
/// NOTE: Used by llama.cpp examples/tools, avoid using in third-party apps. Instead, do your own performance measurements.
final class llama_perf_context_data extends ffi.Struct {
  /// absolute start time
  @ffi.Double()
  external double t_start_ms;

  /// time needed for loading the model
  @ffi.Double()
  external double t_load_ms;

  /// time needed for processing the prompt
  @ffi.Double()
  external double t_p_eval_ms;

  /// time needed for generating tokens
  @ffi.Double()
  external double t_eval_ms;

  /// number of prompt tokens
  @ffi.Int32()
  external int n_p_eval;

  /// number of generated tokens
  @ffi.Int32()
  external int n_eval;

  /// number of times a ggml compute graph had been reused
  @ffi.Int32()
  external int n_reused;
}

final class llama_perf_sampler_data extends ffi.Struct {
  /// time needed for sampling in ms
  @ffi.Double()
  external double t_sample_ms;

  /// number of sampled tokens
  @ffi.Int32()
  external int n_sample;
}

typedef llama_opt_param_filterFunction =
    ffi.Bool Function(
      ffi.Pointer<ggml_tensor> tensor,
      ffi.Pointer<ffi.Void> userdata,
    );
typedef Dartllama_opt_param_filterFunction =
    bool Function(
      ffi.Pointer<ggml_tensor> tensor,
      ffi.Pointer<ffi.Void> userdata,
    );

/// function that returns whether or not a given tensor contains trainable parameters
typedef llama_opt_param_filter =
    ffi.Pointer<ffi.NativeFunction<llama_opt_param_filterFunction>>;

final class llama_opt_params extends ffi.Struct {
  /// assumed context size post training, use context size specified in llama_context if 0
  @ffi.Uint32()
  external int n_ctx_train;

  /// callback for determining which tensors contain trainable parameters
  external llama_opt_param_filter param_filter;

  /// userdata for determining which tensors contain trainable parameters
  external ffi.Pointer<ffi.Void> param_filter_ud;

  /// callback for calculating optimizer parameters
  external ggml_opt_get_optimizer_params get_opt_pars;

  /// userdata for calculating optimizer parameters
  external ffi.Pointer<ffi.Void> get_opt_pars_ud;

  @ffi.UnsignedInt()
  external int optimizer_typeAsInt;

  ggml_opt_optimizer_type get optimizer_type =>
      ggml_opt_optimizer_type.fromValue(optimizer_typeAsInt);
}

/// types that can be stored as GGUF KV data
enum gguf_type {
  GGUF_TYPE_UINT8(0),
  GGUF_TYPE_INT8(1),
  GGUF_TYPE_UINT16(2),
  GGUF_TYPE_INT16(3),
  GGUF_TYPE_UINT32(4),
  GGUF_TYPE_INT32(5),
  GGUF_TYPE_FLOAT32(6),
  GGUF_TYPE_BOOL(7),
  GGUF_TYPE_STRING(8),
  GGUF_TYPE_ARRAY(9),
  GGUF_TYPE_UINT64(10),
  GGUF_TYPE_INT64(11),
  GGUF_TYPE_FLOAT64(12),

  /// marks the end of the enum
  GGUF_TYPE_COUNT(13);

  final int value;
  const gguf_type(this.value);

  static gguf_type fromValue(int value) => switch (value) {
    0 => GGUF_TYPE_UINT8,
    1 => GGUF_TYPE_INT8,
    2 => GGUF_TYPE_UINT16,
    3 => GGUF_TYPE_INT16,
    4 => GGUF_TYPE_UINT32,
    5 => GGUF_TYPE_INT32,
    6 => GGUF_TYPE_FLOAT32,
    7 => GGUF_TYPE_BOOL,
    8 => GGUF_TYPE_STRING,
    9 => GGUF_TYPE_ARRAY,
    10 => GGUF_TYPE_UINT64,
    11 => GGUF_TYPE_INT64,
    12 => GGUF_TYPE_FLOAT64,
    13 => GGUF_TYPE_COUNT,
    _ => throw ArgumentError('Unknown value for gguf_type: $value'),
  };
}

final class gguf_context extends ffi.Opaque {}

final class gguf_init_params extends ffi.Struct {
  @ffi.Bool()
  external bool no_alloc;

  /// if not NULL, create a ggml_context and allocate the tensor data in it
  external ffi.Pointer<ffi.Pointer<ggml_context>> ctx;
}

const int GGML_FILE_MAGIC = 1734831468;

const int GGML_FILE_VERSION = 2;

const int GGML_QNT_VERSION = 2;

const int GGML_QNT_VERSION_FACTOR = 1000;

const int GGML_MAX_DIMS = 4;

const int GGML_MAX_PARAMS = 2048;

const int GGML_MAX_SRC = 10;

const int GGML_MAX_N_THREADS = 512;

const int GGML_MAX_OP_PARAMS = 64;

const int GGML_MAX_NAME = 64;

const int GGML_DEFAULT_N_THREADS = 4;

const int GGML_DEFAULT_GRAPH_SIZE = 2048;

const int GGML_MEM_ALIGN = 16;

const int GGML_EXIT_SUCCESS = 0;

const int GGML_EXIT_ABORTED = 1;

const int GGML_ROPE_TYPE_NORMAL = 0;

const int GGML_ROPE_TYPE_NEOX = 2;

const int GGML_ROPE_TYPE_MROPE = 8;

const int GGML_ROPE_TYPE_VISION = 24;

const int GGML_ROPE_TYPE_IMROPE = 40;

const int GGML_MROPE_SECTIONS = 4;

const int GGML_KQ_MASK_PAD = 1;

const int GGML_N_TASKS_MAX = -1;

const int LLAMA_DEFAULT_SEED = 4294967295;

const int LLAMA_TOKEN_NULL = -1;

const int LLAMA_FILE_MAGIC_GGLA = 1734831201;

const int LLAMA_FILE_MAGIC_GGSN = 1734833006;

const int LLAMA_FILE_MAGIC_GGSQ = 1734833009;

const int LLAMA_SESSION_MAGIC = 1734833006;

const int LLAMA_SESSION_VERSION = 9;

const int LLAMA_STATE_SEQ_MAGIC = 1734833009;

const int LLAMA_STATE_SEQ_VERSION = 2;

const int LLAMA_STATE_SEQ_FLAGS_SWA_ONLY = 1;

const int LLAMA_STATE_SEQ_FLAGS_PARTIAL_ONLY = 1;

const String GGUF_MAGIC = 'GGUF';

const int GGUF_VERSION = 3;

const String GGUF_KEY_GENERAL_ALIGNMENT = 'general.alignment';

const int GGUF_DEFAULT_ALIGNMENT = 32;
